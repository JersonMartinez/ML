{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_excel('./data/train_clientes.xlsx', index_col='ID_CORRELATIVO')\n",
    "df_test = pd.read_excel('./data/test_clientes.xlsx', index_col='ID_CORRELATIVO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Clientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train = df_train.drop(['ATTRITION'], axis=1)\n",
    "y_train = df_train['ATTRITION']\n",
    "x_test = df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 51)\n"
     ]
    }
   ],
   "source": [
    "# Join train and test df to aplly feature engineering\n",
    "df = pd.concat([x_train, x_test])\n",
    "print df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'RANG_INGRESO',\n",
       " u'FLAG_LIMA_PROVINCIA',\n",
       " u'RANG_SDO_PASIVO_MENOS0',\n",
       " u'RANG_NRO_PRODUCTOS_MENOS0']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split columns by type\n",
    "num_cols = df.select_dtypes(exclude=['datetime', 'object']).columns.tolist()\n",
    "cat_cols = [col for col in df.columns if col not in num_cols]\n",
    "cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dm = pd.get_dummies(df[cat_cols[1]], prefix='FLAG')\n",
    "df_ = pd.concat([df, dm], axis=1)\n",
    "df_.drop(cat_cols[1], inplace=True, axis=1)\n",
    "cat_cols.remove('FLAG_LIMA_PROVINCIA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FLG_BANCARIZADO</th>\n",
       "      <th>RANG_INGRESO</th>\n",
       "      <th>EDAD</th>\n",
       "      <th>ANTIGUEDAD</th>\n",
       "      <th>RANG_SDO_PASIVO_MENOS0</th>\n",
       "      <th>SDO_ACTIVO_MENOS0</th>\n",
       "      <th>SDO_ACTIVO_MENOS1</th>\n",
       "      <th>SDO_ACTIVO_MENOS2</th>\n",
       "      <th>SDO_ACTIVO_MENOS3</th>\n",
       "      <th>SDO_ACTIVO_MENOS4</th>\n",
       "      <th>...</th>\n",
       "      <th>NRO_ENTID_SSFF_MENOS4</th>\n",
       "      <th>NRO_ENTID_SSFF_MENOS5</th>\n",
       "      <th>FLG_SDO_OTSSFF_MENOS0</th>\n",
       "      <th>FLG_SDO_OTSSFF_MENOS1</th>\n",
       "      <th>FLG_SDO_OTSSFF_MENOS2</th>\n",
       "      <th>FLG_SDO_OTSSFF_MENOS3</th>\n",
       "      <th>FLG_SDO_OTSSFF_MENOS4</th>\n",
       "      <th>FLG_SDO_OTSSFF_MENOS5</th>\n",
       "      <th>FLAG_Lima</th>\n",
       "      <th>FLAG_Provincia</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_CORRELATIVO</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35653</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>25.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66575</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56800</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8410</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>63.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6853</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                FLG_BANCARIZADO  RANG_INGRESO  EDAD  ANTIGUEDAD  \\\n",
       "ID_CORRELATIVO                                                    \n",
       "35653                         1             5  25.0         6.0   \n",
       "66575                         1             2  27.0         0.0   \n",
       "56800                         1             0  34.0         4.0   \n",
       "8410                          1             3  63.0         5.0   \n",
       "6853                          1            -1  25.0         0.0   \n",
       "\n",
       "                RANG_SDO_PASIVO_MENOS0  SDO_ACTIVO_MENOS0  SDO_ACTIVO_MENOS1  \\\n",
       "ID_CORRELATIVO                                                                 \n",
       "35653                                9                  0                  0   \n",
       "66575                                1                  0                  0   \n",
       "56800                                2                  0                  0   \n",
       "8410                                 3                  0                  0   \n",
       "6853                                 1                  0                  0   \n",
       "\n",
       "                SDO_ACTIVO_MENOS2  SDO_ACTIVO_MENOS3  SDO_ACTIVO_MENOS4  \\\n",
       "ID_CORRELATIVO                                                            \n",
       "35653                           0                  0                  0   \n",
       "66575                           0                  0                  0   \n",
       "56800                           0                  0                  0   \n",
       "8410                            0                  0                  0   \n",
       "6853                            0                  0                  0   \n",
       "\n",
       "                     ...        NRO_ENTID_SSFF_MENOS4  NRO_ENTID_SSFF_MENOS5  \\\n",
       "ID_CORRELATIVO       ...                                                       \n",
       "35653                ...                            1                      1   \n",
       "66575                ...                            1                      1   \n",
       "56800                ...                            0                      0   \n",
       "8410                 ...                            3                      3   \n",
       "6853                 ...                            0                      0   \n",
       "\n",
       "                FLG_SDO_OTSSFF_MENOS0  FLG_SDO_OTSSFF_MENOS1  \\\n",
       "ID_CORRELATIVO                                                 \n",
       "35653                               1                      0   \n",
       "66575                               0                      0   \n",
       "56800                               0                      0   \n",
       "8410                                1                      1   \n",
       "6853                                0                      0   \n",
       "\n",
       "                FLG_SDO_OTSSFF_MENOS2  FLG_SDO_OTSSFF_MENOS3  \\\n",
       "ID_CORRELATIVO                                                 \n",
       "35653                               0                      0   \n",
       "66575                               0                      0   \n",
       "56800                               0                      0   \n",
       "8410                                1                      1   \n",
       "6853                                0                      0   \n",
       "\n",
       "                FLG_SDO_OTSSFF_MENOS4  FLG_SDO_OTSSFF_MENOS5  FLAG_Lima  \\\n",
       "ID_CORRELATIVO                                                            \n",
       "35653                               0                      0          1   \n",
       "66575                               0                      0          0   \n",
       "56800                               0                      0          0   \n",
       "8410                                1                      1          0   \n",
       "6853                                0                      0          1   \n",
       "\n",
       "                FLAG_Provincia  \n",
       "ID_CORRELATIVO                  \n",
       "35653                        0  \n",
       "66575                        1  \n",
       "56800                        1  \n",
       "8410                         1  \n",
       "6853                         0  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for col in cat_cols:\n",
    "    df_[col] = df_[col].astype('category').cat.codes\n",
    "    \n",
    "df_.drop('CODMES', inplace=True, axis=1)\n",
    "df_.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAf0AAAFJCAYAAACRl/TrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x60dbf60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_.EDAD.plot.hist(bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAf0AAAFJCAYAAACRl/TrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12afa940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_.ANTIGUEDAD.plot.hist(bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_.EDAD.fillna(df_.EDAD.median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_.ANTIGUEDAD.fillna(df_.ANTIGUEDAD.median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "decomp = PCA(n_components=5)\n",
    "pca_result = decomp.fit_transform(StandardScaler().fit_transform(df_.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = ['PCA{}'.format(i+1) for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ = pd.DataFrame(data=pca_result, columns=cols)\n",
    "df_.index=df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelado clientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_ = df_.iloc[0:70000]\n",
    "x_test_ = df_.iloc[70000:]\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x_train_, y_train, test_size=.10, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tTest-logloss:0.564098\n",
      "Will train until Test-logloss hasn't improved in 10 rounds.\n",
      "[1]\tTest-logloss:0.494121\n",
      "[2]\tTest-logloss:0.453393\n",
      "[3]\tTest-logloss:0.430364\n",
      "[4]\tTest-logloss:0.415571\n",
      "[5]\tTest-logloss:0.405807\n",
      "[6]\tTest-logloss:0.39985\n",
      "[7]\tTest-logloss:0.395994\n",
      "[8]\tTest-logloss:0.393946\n",
      "[9]\tTest-logloss:0.391933\n",
      "[10]\tTest-logloss:0.39093\n",
      "[11]\tTest-logloss:0.390661\n",
      "[12]\tTest-logloss:0.389872\n",
      "[13]\tTest-logloss:0.38853\n",
      "[14]\tTest-logloss:0.388401\n",
      "[15]\tTest-logloss:0.38763\n",
      "[16]\tTest-logloss:0.387251\n",
      "[17]\tTest-logloss:0.387262\n",
      "[18]\tTest-logloss:0.386171\n",
      "[19]\tTest-logloss:0.386741\n",
      "[20]\tTest-logloss:0.386794\n",
      "[21]\tTest-logloss:0.386041\n",
      "[22]\tTest-logloss:0.385927\n",
      "[23]\tTest-logloss:0.385251\n",
      "[24]\tTest-logloss:0.384924\n",
      "[25]\tTest-logloss:0.384882\n",
      "[26]\tTest-logloss:0.384805\n",
      "[27]\tTest-logloss:0.384888\n",
      "[28]\tTest-logloss:0.384565\n",
      "[29]\tTest-logloss:0.38463\n",
      "[30]\tTest-logloss:0.384306\n",
      "[31]\tTest-logloss:0.384389\n",
      "[32]\tTest-logloss:0.384602\n",
      "[33]\tTest-logloss:0.38438\n",
      "[34]\tTest-logloss:0.384611\n",
      "[35]\tTest-logloss:0.384597\n",
      "[36]\tTest-logloss:0.384775\n",
      "[37]\tTest-logloss:0.384622\n",
      "[38]\tTest-logloss:0.384776\n",
      "[39]\tTest-logloss:0.384884\n",
      "[40]\tTest-logloss:0.38492\n",
      "Stopping. Best iteration:\n",
      "[30]\tTest-logloss:0.384306\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(xtrain, label=ytrain)\n",
    "dtest = xgb.DMatrix(xtest, label=ytest)\n",
    "\n",
    "params_xgb = {\n",
    "    'max_depth': 6,\n",
    "    'min_child_weight': 1,\n",
    "    'eta':.3,\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree': 1,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric':'logloss'\n",
    "}\n",
    "\n",
    "xgb_model = xgb.train(params_xgb, \n",
    "                      dtrain,\n",
    "                      num_boost_round=999, \n",
    "                      early_stopping_rounds=10,\n",
    "                      evals=[(dtest, \"Test\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test-logloss-mean</th>\n",
       "      <th>test-logloss-std</th>\n",
       "      <th>train-logloss-mean</th>\n",
       "      <th>train-logloss-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.540111</td>\n",
       "      <td>0.001392</td>\n",
       "      <td>0.539427</td>\n",
       "      <td>0.000601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.456337</td>\n",
       "      <td>0.002398</td>\n",
       "      <td>0.455070</td>\n",
       "      <td>0.000817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.406285</td>\n",
       "      <td>0.003232</td>\n",
       "      <td>0.404455</td>\n",
       "      <td>0.001024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.373830</td>\n",
       "      <td>0.003567</td>\n",
       "      <td>0.371323</td>\n",
       "      <td>0.001222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.353061</td>\n",
       "      <td>0.003998</td>\n",
       "      <td>0.349706</td>\n",
       "      <td>0.001395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.339352</td>\n",
       "      <td>0.004470</td>\n",
       "      <td>0.335181</td>\n",
       "      <td>0.001314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.330192</td>\n",
       "      <td>0.004983</td>\n",
       "      <td>0.324942</td>\n",
       "      <td>0.001378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.323989</td>\n",
       "      <td>0.004919</td>\n",
       "      <td>0.317734</td>\n",
       "      <td>0.001637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.319535</td>\n",
       "      <td>0.005360</td>\n",
       "      <td>0.312329</td>\n",
       "      <td>0.001381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.316619</td>\n",
       "      <td>0.005523</td>\n",
       "      <td>0.308433</td>\n",
       "      <td>0.001269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.313818</td>\n",
       "      <td>0.005143</td>\n",
       "      <td>0.304643</td>\n",
       "      <td>0.001498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.311789</td>\n",
       "      <td>0.005334</td>\n",
       "      <td>0.301615</td>\n",
       "      <td>0.001430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.310741</td>\n",
       "      <td>0.005301</td>\n",
       "      <td>0.299428</td>\n",
       "      <td>0.001475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.309877</td>\n",
       "      <td>0.005171</td>\n",
       "      <td>0.297596</td>\n",
       "      <td>0.001550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.309194</td>\n",
       "      <td>0.005449</td>\n",
       "      <td>0.295581</td>\n",
       "      <td>0.001406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.308536</td>\n",
       "      <td>0.005473</td>\n",
       "      <td>0.293925</td>\n",
       "      <td>0.001587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.308088</td>\n",
       "      <td>0.005488</td>\n",
       "      <td>0.292576</td>\n",
       "      <td>0.001369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.307694</td>\n",
       "      <td>0.005408</td>\n",
       "      <td>0.291112</td>\n",
       "      <td>0.001419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.307261</td>\n",
       "      <td>0.005291</td>\n",
       "      <td>0.289646</td>\n",
       "      <td>0.001739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.306896</td>\n",
       "      <td>0.005356</td>\n",
       "      <td>0.288319</td>\n",
       "      <td>0.001713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.306454</td>\n",
       "      <td>0.005512</td>\n",
       "      <td>0.287106</td>\n",
       "      <td>0.001513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.306116</td>\n",
       "      <td>0.005264</td>\n",
       "      <td>0.286034</td>\n",
       "      <td>0.001794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.305825</td>\n",
       "      <td>0.005221</td>\n",
       "      <td>0.284808</td>\n",
       "      <td>0.001869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.305468</td>\n",
       "      <td>0.005318</td>\n",
       "      <td>0.283405</td>\n",
       "      <td>0.001517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.305240</td>\n",
       "      <td>0.005336</td>\n",
       "      <td>0.282344</td>\n",
       "      <td>0.001658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.305192</td>\n",
       "      <td>0.005335</td>\n",
       "      <td>0.281430</td>\n",
       "      <td>0.001659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.304769</td>\n",
       "      <td>0.004752</td>\n",
       "      <td>0.280386</td>\n",
       "      <td>0.001938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.304718</td>\n",
       "      <td>0.004723</td>\n",
       "      <td>0.279701</td>\n",
       "      <td>0.002079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.304673</td>\n",
       "      <td>0.004629</td>\n",
       "      <td>0.278464</td>\n",
       "      <td>0.001945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.304625</td>\n",
       "      <td>0.004676</td>\n",
       "      <td>0.278027</td>\n",
       "      <td>0.001892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.304663</td>\n",
       "      <td>0.004657</td>\n",
       "      <td>0.277368</td>\n",
       "      <td>0.001869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.304636</td>\n",
       "      <td>0.004699</td>\n",
       "      <td>0.276577</td>\n",
       "      <td>0.001927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.304606</td>\n",
       "      <td>0.004722</td>\n",
       "      <td>0.275904</td>\n",
       "      <td>0.001767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.304667</td>\n",
       "      <td>0.004766</td>\n",
       "      <td>0.275487</td>\n",
       "      <td>0.001789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.304656</td>\n",
       "      <td>0.005028</td>\n",
       "      <td>0.275070</td>\n",
       "      <td>0.001680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.304536</td>\n",
       "      <td>0.005227</td>\n",
       "      <td>0.274301</td>\n",
       "      <td>0.001338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.304547</td>\n",
       "      <td>0.005233</td>\n",
       "      <td>0.273902</td>\n",
       "      <td>0.001496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.304484</td>\n",
       "      <td>0.005173</td>\n",
       "      <td>0.272903</td>\n",
       "      <td>0.001638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.304434</td>\n",
       "      <td>0.005113</td>\n",
       "      <td>0.272179</td>\n",
       "      <td>0.001976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.304428</td>\n",
       "      <td>0.005034</td>\n",
       "      <td>0.271282</td>\n",
       "      <td>0.001743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    test-logloss-mean  test-logloss-std  train-logloss-mean  train-logloss-std\n",
       "0            0.540111          0.001392            0.539427           0.000601\n",
       "1            0.456337          0.002398            0.455070           0.000817\n",
       "2            0.406285          0.003232            0.404455           0.001024\n",
       "3            0.373830          0.003567            0.371323           0.001222\n",
       "4            0.353061          0.003998            0.349706           0.001395\n",
       "5            0.339352          0.004470            0.335181           0.001314\n",
       "6            0.330192          0.004983            0.324942           0.001378\n",
       "7            0.323989          0.004919            0.317734           0.001637\n",
       "8            0.319535          0.005360            0.312329           0.001381\n",
       "9            0.316619          0.005523            0.308433           0.001269\n",
       "10           0.313818          0.005143            0.304643           0.001498\n",
       "11           0.311789          0.005334            0.301615           0.001430\n",
       "12           0.310741          0.005301            0.299428           0.001475\n",
       "13           0.309877          0.005171            0.297596           0.001550\n",
       "14           0.309194          0.005449            0.295581           0.001406\n",
       "15           0.308536          0.005473            0.293925           0.001587\n",
       "16           0.308088          0.005488            0.292576           0.001369\n",
       "17           0.307694          0.005408            0.291112           0.001419\n",
       "18           0.307261          0.005291            0.289646           0.001739\n",
       "19           0.306896          0.005356            0.288319           0.001713\n",
       "20           0.306454          0.005512            0.287106           0.001513\n",
       "21           0.306116          0.005264            0.286034           0.001794\n",
       "22           0.305825          0.005221            0.284808           0.001869\n",
       "23           0.305468          0.005318            0.283405           0.001517\n",
       "24           0.305240          0.005336            0.282344           0.001658\n",
       "25           0.305192          0.005335            0.281430           0.001659\n",
       "26           0.304769          0.004752            0.280386           0.001938\n",
       "27           0.304718          0.004723            0.279701           0.002079\n",
       "28           0.304673          0.004629            0.278464           0.001945\n",
       "29           0.304625          0.004676            0.278027           0.001892\n",
       "30           0.304663          0.004657            0.277368           0.001869\n",
       "31           0.304636          0.004699            0.276577           0.001927\n",
       "32           0.304606          0.004722            0.275904           0.001767\n",
       "33           0.304667          0.004766            0.275487           0.001789\n",
       "34           0.304656          0.005028            0.275070           0.001680\n",
       "35           0.304536          0.005227            0.274301           0.001338\n",
       "36           0.304547          0.005233            0.273902           0.001496\n",
       "37           0.304484          0.005173            0.272903           0.001638\n",
       "38           0.304434          0.005113            0.272179           0.001976\n",
       "39           0.304428          0.005034            0.271282           0.001743"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.cv(params, \n",
    "       xgb.DMatrix(x_train_, label=y_train), \n",
    "       num_boost_round=999,\n",
    "       early_stopping_rounds=10, \n",
    "       nfold=5, \n",
    "       seed=0,\n",
    "       metrics='logloss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with max_depth=9, min_child_weight=5\n",
      "\tLogloss 0.3019878 for 19 rounds\n",
      "CV with max_depth=9, min_child_weight=6\n",
      "\tLogloss 0.3019938 for 20 rounds\n",
      "CV with max_depth=9, min_child_weight=7\n",
      "\tLogloss 0.3020584 for 17 rounds\n",
      "CV with max_depth=10, min_child_weight=5\n",
      "\tLogloss 0.30276 for 17 rounds\n",
      "CV with max_depth=10, min_child_weight=6\n",
      "\tLogloss 0.3018538 for 16 rounds\n",
      "CV with max_depth=10, min_child_weight=7\n",
      "\tLogloss 0.3021882 for 18 rounds\n",
      "CV with max_depth=11, min_child_weight=5\n",
      "\tLogloss 0.302065 for 15 rounds\n",
      "CV with max_depth=11, min_child_weight=6\n",
      "\tLogloss 0.3016956 for 15 rounds\n",
      "CV with max_depth=11, min_child_weight=7\n",
      "\tLogloss 0.3024108 for 14 rounds\n",
      "Best params: 11, 6, Logloss: 0.3016956\n"
     ]
    }
   ],
   "source": [
    "gridsearch_params = [(max_depth, min_child_weight)\n",
    "                    for max_depth in range(9,12)\n",
    "                    for min_child_weight in range(5,8)]\n",
    "\n",
    "min_logloss = float(\"Inf\")\n",
    "best_params = None\n",
    "\n",
    "for max_depth, min_child_weight in gridsearch_params:\n",
    "    print(\"CV with max_depth={}, min_child_weight={}\".format(\n",
    "                             max_depth,\n",
    "                             min_child_weight))\n",
    "\n",
    "    # Update our parameters\n",
    "    params_xgb['max_depth'] = max_depth\n",
    "    params_xgb['min_child_weight'] = min_child_weight\n",
    "\n",
    "    # Run CV\n",
    "    cv_results = xgb.cv(\n",
    "        params_xgb,\n",
    "        xgb.DMatrix(x_train_, label=y_train),\n",
    "        num_boost_round=999,\n",
    "        seed=0,\n",
    "        nfold=5,\n",
    "        metrics='logloss',\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "\n",
    "    # Update best logloss\n",
    "    mean_logloss = cv_results['test-logloss-mean'].min()\n",
    "    boost_rounds = cv_results['test-logloss-mean'].argmin()\n",
    "    print(\"\\tLogloss {} for {} rounds\".format(mean_logloss, boost_rounds))\n",
    "    if mean_logloss < min_logloss:\n",
    "        min_logloss = mean_logloss\n",
    "        best_params = (max_depth, min_child_weight)\n",
    "\n",
    "print(\"Best params: {}, {}, Logloss: {}\".format(best_params[0], best_params[1], min_logloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params_xgb['max_depth'] = 11\n",
    "params_xgb['min_child_weight'] = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with subsample=1.0, colsample=1.0\n",
      "\tLogloss 0.3016956 for 15 rounds\n",
      "CV with subsample=1.0, colsample=0.9\n",
      "\tLogloss 0.301813 for 16 rounds\n",
      "CV with subsample=1.0, colsample=0.8\n",
      "\tLogloss 0.301169 for 17 rounds\n",
      "CV with subsample=1.0, colsample=0.7\n",
      "\tLogloss 0.3018682 for 16 rounds\n",
      "CV with subsample=0.9, colsample=1.0\n",
      "\tLogloss 0.3025508 for 13 rounds\n",
      "CV with subsample=0.9, colsample=0.9\n",
      "\tLogloss 0.3020234 for 15 rounds\n",
      "CV with subsample=0.9, colsample=0.8\n",
      "\tLogloss 0.3027602 for 17 rounds\n",
      "CV with subsample=0.9, colsample=0.7\n",
      "\tLogloss 0.302117 for 19 rounds\n",
      "CV with subsample=0.8, colsample=1.0\n",
      "\tLogloss 0.303744 for 14 rounds\n",
      "CV with subsample=0.8, colsample=0.9\n",
      "\tLogloss 0.3022242 for 14 rounds\n",
      "CV with subsample=0.8, colsample=0.8\n",
      "\tLogloss 0.3029602 for 19 rounds\n",
      "CV with subsample=0.8, colsample=0.7\n",
      "\tLogloss 0.303156 for 19 rounds\n",
      "CV with subsample=0.7, colsample=1.0\n",
      "\tLogloss 0.3036902 for 14 rounds\n",
      "CV with subsample=0.7, colsample=0.9\n",
      "\tLogloss 0.3032894 for 15 rounds\n",
      "CV with subsample=0.7, colsample=0.8\n",
      "\tLogloss 0.3040306 for 17 rounds\n",
      "CV with subsample=0.7, colsample=0.7\n",
      "\tLogloss 0.3035838 for 19 rounds\n",
      "Best params: 1.0, 0.8, Logloss: 0.301169\n"
     ]
    }
   ],
   "source": [
    "gridsearch_params = [\n",
    "    (subsample, colsample)\n",
    "    for subsample in [i/10. for i in range(7,11)]\n",
    "    for colsample in [i/10. for i in range(7,11)]\n",
    "]\n",
    "\n",
    "min_logloss = float(\"Inf\")\n",
    "best_params = None\n",
    "\n",
    "for subsample, colsample in reversed(gridsearch_params):\n",
    "    print(\"CV with subsample={}, colsample_bytree={}\".format(\n",
    "                             subsample,\n",
    "                             colsample))\n",
    "\n",
    "    # We update our parameters\n",
    "    params_xgb['subsample'] = subsample\n",
    "    params_xgb['colsample_bytree'] = colsample\n",
    "\n",
    "    # Run CV\n",
    "    cv_results = xgb.cv(\n",
    "        params_xgb,\n",
    "        xgb.DMatrix(x_train_, label=y_train),\n",
    "        num_boost_round=999,\n",
    "        seed=0,\n",
    "        nfold=5,\n",
    "        metrics='logloss',\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "\n",
    "    # Update best logloss\n",
    "    mean_logloss = cv_results['test-logloss-mean'].min()\n",
    "    boost_rounds = cv_results['test-logloss-mean'].argmin()\n",
    "    print(\"\\tLogloss {} for {} rounds\".format(mean_logloss, boost_rounds))\n",
    "    if mean_logloss < min_logloss:\n",
    "        min_logloss = mean_logloss\n",
    "        best_params = (subsample, colsample)\n",
    "\n",
    "print(\"Best params: {}, {}, Logloss: {}\".format(best_params[0], best_params[1], min_logloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params_xgb['subsample'] = 1.0\n",
    "params_xgb['colsample_bytree'] = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with eta=0.3\n",
      "\tLogloss 0.301169 for 17 rounds\n",
      "CV with eta=0.2\n",
      "\tLogloss 0.2999164 for 26 rounds\n",
      "CV with eta=0.1\n",
      "\tLogloss 0.2982548 for 54 rounds\n",
      "CV with eta=0.05\n",
      "\tLogloss 0.2977802 for 116 rounds\n",
      "CV with eta=0.01\n",
      "\tLogloss 0.2973146 for 604 rounds\n",
      "CV with eta=0.005\n",
      "\tLogloss 0.2978146 for 998 rounds\n",
      "Best params: 0.01, Logloss: 0.2973146\n"
     ]
    }
   ],
   "source": [
    "min_logloss = float(\"Inf\")\n",
    "best_params = None\n",
    "\n",
    "for eta in [.3, .2, .1, .05, .01, .005]:\n",
    "    print(\"CV with eta={}\".format(eta))\n",
    "\n",
    "    # We update our parameters\n",
    "    params_xgb['eta'] = eta\n",
    "    \n",
    "    cv_results = xgb.cv(\n",
    "        params_xgb,\n",
    "        xgb.DMatrix(x_train_, label=y_train),\n",
    "        num_boost_round=999,\n",
    "        seed=0,\n",
    "        nfold=5,\n",
    "        metrics='logloss',\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "\n",
    "    # Update best score\n",
    "    mean_logloss = cv_results['test-logloss-mean'].min()\n",
    "    boost_rounds = cv_results['test-logloss-mean'].argmin()\n",
    "    print(\"\\tLogloss {} for {} rounds\".format(mean_logloss, boost_rounds))\n",
    "    if mean_logloss < min_logloss:\n",
    "        min_logloss = mean_logloss\n",
    "        best_params = eta\n",
    "\n",
    "print(\"Best params: {}, Logloss: {}\".format(best_params, min_logloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params_xgb['eta'] = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.8,\n",
       " 'eta': 0.01,\n",
       " 'eval_metric': 'logloss',\n",
       " 'max_depth': 11,\n",
       " 'min_child_weight': 6,\n",
       " 'objective': 'binary:logistic',\n",
       " 'subsample': 1.0}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_xgb_model = xgb.train(\n",
    "    params_xgb,\n",
    "    xgb.DMatrix(x_train_, label=y_train),\n",
    "    num_boost_round=604\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_CORRELATIVO</th>\n",
       "      <th>ATTRITION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47411</td>\n",
       "      <td>0.374053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39861</td>\n",
       "      <td>0.278564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38898</td>\n",
       "      <td>0.023526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50927</td>\n",
       "      <td>0.014539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32969</td>\n",
       "      <td>0.363567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_CORRELATIVO  ATTRITION\n",
       "0           47411   0.374053\n",
       "1           39861   0.278564\n",
       "2           38898   0.023526\n",
       "3           50927   0.014539\n",
       "4           32969   0.363567"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = best_xgb_model.predict(xgb.DMatrix(x_test_))\n",
    "submission = pd.DataFrame()\n",
    "submission['ID_CORRELATIVO'] = x_test_.index\n",
    "submission['ATTRITION'] = y_pred\n",
    "submission.head()\n",
    "\n",
    "submission.to_csv('./data/submission14_XGB.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tTrain's binary_logloss: 0.539359\tTest's binary_logloss: 0.540194\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\tTrain's binary_logloss: 0.455084\tTest's binary_logloss: 0.456778\n",
      "[3]\tTrain's binary_logloss: 0.404938\tTest's binary_logloss: 0.407416\n",
      "[4]\tTrain's binary_logloss: 0.371791\tTest's binary_logloss: 0.374792\n",
      "[5]\tTrain's binary_logloss: 0.35137\tTest's binary_logloss: 0.354528\n",
      "[6]\tTrain's binary_logloss: 0.337076\tTest's binary_logloss: 0.340333\n",
      "[7]\tTrain's binary_logloss: 0.326848\tTest's binary_logloss: 0.330454\n",
      "[8]\tTrain's binary_logloss: 0.320195\tTest's binary_logloss: 0.324332\n",
      "[9]\tTrain's binary_logloss: 0.315489\tTest's binary_logloss: 0.320418\n",
      "[10]\tTrain's binary_logloss: 0.311734\tTest's binary_logloss: 0.316944\n",
      "[11]\tTrain's binary_logloss: 0.30874\tTest's binary_logloss: 0.314812\n",
      "[12]\tTrain's binary_logloss: 0.306662\tTest's binary_logloss: 0.313082\n",
      "[13]\tTrain's binary_logloss: 0.304714\tTest's binary_logloss: 0.311823\n",
      "[14]\tTrain's binary_logloss: 0.303184\tTest's binary_logloss: 0.310817\n",
      "[15]\tTrain's binary_logloss: 0.300815\tTest's binary_logloss: 0.309594\n",
      "[16]\tTrain's binary_logloss: 0.299808\tTest's binary_logloss: 0.309255\n",
      "[17]\tTrain's binary_logloss: 0.297926\tTest's binary_logloss: 0.308197\n",
      "[18]\tTrain's binary_logloss: 0.296335\tTest's binary_logloss: 0.308085\n",
      "[19]\tTrain's binary_logloss: 0.295171\tTest's binary_logloss: 0.307987\n",
      "[20]\tTrain's binary_logloss: 0.293526\tTest's binary_logloss: 0.306489\n",
      "[21]\tTrain's binary_logloss: 0.292422\tTest's binary_logloss: 0.306028\n",
      "[22]\tTrain's binary_logloss: 0.291347\tTest's binary_logloss: 0.305533\n",
      "[23]\tTrain's binary_logloss: 0.290543\tTest's binary_logloss: 0.305815\n",
      "[24]\tTrain's binary_logloss: 0.289852\tTest's binary_logloss: 0.305789\n",
      "[25]\tTrain's binary_logloss: 0.289223\tTest's binary_logloss: 0.305563\n",
      "[26]\tTrain's binary_logloss: 0.287674\tTest's binary_logloss: 0.305205\n",
      "[27]\tTrain's binary_logloss: 0.286984\tTest's binary_logloss: 0.304943\n",
      "[28]\tTrain's binary_logloss: 0.286144\tTest's binary_logloss: 0.304817\n",
      "[29]\tTrain's binary_logloss: 0.285379\tTest's binary_logloss: 0.304978\n",
      "[30]\tTrain's binary_logloss: 0.284748\tTest's binary_logloss: 0.305369\n",
      "[31]\tTrain's binary_logloss: 0.284499\tTest's binary_logloss: 0.305273\n",
      "[32]\tTrain's binary_logloss: 0.283606\tTest's binary_logloss: 0.305049\n",
      "[33]\tTrain's binary_logloss: 0.282931\tTest's binary_logloss: 0.30479\n",
      "[34]\tTrain's binary_logloss: 0.281762\tTest's binary_logloss: 0.30423\n",
      "[35]\tTrain's binary_logloss: 0.281418\tTest's binary_logloss: 0.30434\n",
      "[36]\tTrain's binary_logloss: 0.281094\tTest's binary_logloss: 0.30447\n",
      "[37]\tTrain's binary_logloss: 0.280389\tTest's binary_logloss: 0.304684\n",
      "[38]\tTrain's binary_logloss: 0.279882\tTest's binary_logloss: 0.304869\n",
      "[39]\tTrain's binary_logloss: 0.279453\tTest's binary_logloss: 0.305197\n",
      "[40]\tTrain's binary_logloss: 0.27868\tTest's binary_logloss: 0.305211\n",
      "[41]\tTrain's binary_logloss: 0.277889\tTest's binary_logloss: 0.304986\n",
      "[42]\tTrain's binary_logloss: 0.277532\tTest's binary_logloss: 0.304991\n",
      "[43]\tTrain's binary_logloss: 0.276855\tTest's binary_logloss: 0.305006\n",
      "[44]\tTrain's binary_logloss: 0.276624\tTest's binary_logloss: 0.305136\n",
      "Early stopping, best iteration is:\n",
      "[34]\tTrain's binary_logloss: 0.281762\tTest's binary_logloss: 0.30423\n"
     ]
    }
   ],
   "source": [
    "# Lightgbm\n",
    "\n",
    "dtrain =lgb.Dataset(xtrain,label=ytrain)\n",
    "dtest = lgb.Dataset(xtest,label=ytest)\n",
    "\n",
    "params_lgb = {'objective': 'binary',\n",
    "          'max_depth': 6,\n",
    "          'learning_rate':.3,\n",
    "          'max_bin': 200,\n",
    "          'metric': 'binary_logloss'}\n",
    "\n",
    "\n",
    "lgb_model = lgb.train(params_lgb,\n",
    "                      dtrain,\n",
    "                      num_boost_round=999,\n",
    "                      early_stopping_rounds=10,\n",
    "                      valid_sets=[dtest, dtrain],\n",
    "                      valid_names=[\"Test\", \"Train\"]\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with max_depth=9, min_child_weight=5\n",
      "\tLogloss 0.302007861425 for 31 rounds\n",
      "CV with max_depth=9, min_child_weight=6\n",
      "\tLogloss 0.302764372097 for 32 rounds\n",
      "CV with max_depth=9, min_child_weight=7\n",
      "\tLogloss 0.302109801105 for 37 rounds\n",
      "CV with max_depth=10, min_child_weight=5\n",
      "\tLogloss 0.302627212519 for 31 rounds\n",
      "CV with max_depth=10, min_child_weight=6\n",
      "\tLogloss 0.30259731669 for 36 rounds\n",
      "CV with max_depth=10, min_child_weight=7\n",
      "\tLogloss 0.302337873608 for 33 rounds\n",
      "CV with max_depth=11, min_child_weight=5\n",
      "\tLogloss 0.301839794585 for 28 rounds\n",
      "CV with max_depth=11, min_child_weight=6\n",
      "\tLogloss 0.301804794312 for 44 rounds\n",
      "CV with max_depth=11, min_child_weight=7\n",
      "\tLogloss 0.301315875596 for 36 rounds\n",
      "Best params: 11, 7, Logloss: 0.301315875596\n"
     ]
    }
   ],
   "source": [
    "gridsearch_params = [(max_depth, min_child_weight)\n",
    "                    for max_depth in range(9,12)\n",
    "                    for min_child_weight in range(5,8)]\n",
    "\n",
    "min_logloss = float(\"Inf\")\n",
    "best_params = None\n",
    "\n",
    "for max_depth, min_child_weight in gridsearch_params:\n",
    "    print(\"CV with max_depth={}, min_child_weight={}\".format(\n",
    "                             max_depth,\n",
    "                             min_child_weight))\n",
    "\n",
    "    # Update our parameters\n",
    "    params_lgb['max_depth'] = max_depth\n",
    "    params_lgb['min_child_weight'] = min_child_weight\n",
    "\n",
    "    # Run CV\n",
    "    cv_results = lgb.cv(\n",
    "        params_lgb,\n",
    "        lgb.Dataset(x_train_, label=y_train),\n",
    "        num_boost_round=999,\n",
    "        seed=0,\n",
    "        nfold=5,\n",
    "        metrics=['binary_logloss'],\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "\n",
    "    # Update best logloss\n",
    "    mean_logloss = np.min(cv_results['binary_logloss-mean'])\n",
    "    boost_rounds = len(cv_results['binary_logloss-mean'])\n",
    "    print(\"\\tLogloss {} for {} rounds\".format(mean_logloss, boost_rounds))\n",
    "    if mean_logloss < min_logloss:\n",
    "        min_logloss = mean_logloss\n",
    "        best_params = (max_depth, min_child_weight)\n",
    "\n",
    "print(\"Best params: {}, {}, Logloss: {}\".format(best_params[0], best_params[1], min_logloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params_lgb['max_depth'] = 11\n",
    "params_lgb['min_child_weight'] = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with subsample=1.0, colsample=1.0\n",
      "\tLogloss 0.301315875596 for 36 rounds\n",
      "CV with subsample=1.0, colsample=0.9\n",
      "\tLogloss 0.301605787112 for 33 rounds\n",
      "CV with subsample=1.0, colsample=0.8\n",
      "\tLogloss 0.301139242022 for 32 rounds\n",
      "CV with subsample=1.0, colsample=0.7\n",
      "\tLogloss 0.301382287763 for 36 rounds\n",
      "CV with subsample=0.9, colsample=1.0\n",
      "\tLogloss 0.301315875596 for 36 rounds\n",
      "CV with subsample=0.9, colsample=0.9\n",
      "\tLogloss 0.301605787112 for 33 rounds\n",
      "CV with subsample=0.9, colsample=0.8\n",
      "\tLogloss 0.301139242022 for 32 rounds\n",
      "CV with subsample=0.9, colsample=0.7\n",
      "\tLogloss 0.301382287763 for 36 rounds\n",
      "CV with subsample=0.8, colsample=1.0\n",
      "\tLogloss 0.301315875596 for 36 rounds\n",
      "CV with subsample=0.8, colsample=0.9\n",
      "\tLogloss 0.301605787112 for 33 rounds\n",
      "CV with subsample=0.8, colsample=0.8\n",
      "\tLogloss 0.301139242022 for 32 rounds\n",
      "CV with subsample=0.8, colsample=0.7\n",
      "\tLogloss 0.301382287763 for 36 rounds\n",
      "CV with subsample=0.7, colsample=1.0\n",
      "\tLogloss 0.301315875596 for 36 rounds\n",
      "CV with subsample=0.7, colsample=0.9\n",
      "\tLogloss 0.301605787112 for 33 rounds\n",
      "CV with subsample=0.7, colsample=0.8\n",
      "\tLogloss 0.301139242022 for 32 rounds\n",
      "CV with subsample=0.7, colsample=0.7\n",
      "\tLogloss 0.301382287763 for 36 rounds\n",
      "Best params: 1.0, 0.8, Logloss: 0.301139242022\n"
     ]
    }
   ],
   "source": [
    "gridsearch_params = [\n",
    "    (subsample, colsample)\n",
    "    for subsample in [i/10. for i in range(7,11)]\n",
    "    for colsample in [i/10. for i in range(7,11)]\n",
    "]\n",
    "\n",
    "min_logloss = float(\"Inf\")\n",
    "best_params = None\n",
    "\n",
    "for subsample, colsample in reversed(gridsearch_params):\n",
    "    print(\"CV with subsample={}, colsample_bytree={}\".format(\n",
    "                             subsample,\n",
    "                             colsample))\n",
    "\n",
    "    # We update our parameters\n",
    "    params_lgb['subsample'] = subsample\n",
    "    params_lgb['colsample_bytree'] = colsample\n",
    "\n",
    "    # Run CV\n",
    "    cv_results = lgb.cv(\n",
    "        params_lgb,\n",
    "        lgb.Dataset(x_train_, label=y_train),\n",
    "        num_boost_round=999,\n",
    "        seed=0,\n",
    "        nfold=5,\n",
    "        metrics=['binary_logloss'],\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "\n",
    "    # Update best logloss\n",
    "    mean_logloss = np.min(cv_results['binary_logloss-mean'])\n",
    "    boost_rounds = len(cv_results['binary_logloss-mean'])\n",
    "    print(\"\\tLogloss {} for {} rounds\".format(mean_logloss, boost_rounds))\n",
    "    if mean_logloss < min_logloss:\n",
    "        min_logloss = mean_logloss\n",
    "        best_params = (subsample, colsample)\n",
    "\n",
    "print(\"Best params: {}, {}, Logloss: {}\".format(best_params[0], best_params[1], min_logloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params_lgb['subsample'] = 1.0\n",
    "params_lgb['colsample_bytree'] = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with eta=0.3\n",
      "\tLogloss 0.301139242022 for 32 rounds\n",
      "CV with eta=0.2\n",
      "\tLogloss 0.300803432581 for 57 rounds\n",
      "CV with eta=0.1\n",
      "\tLogloss 0.298725762419 for 157 rounds\n",
      "CV with eta=0.05\n",
      "\tLogloss 0.299110335401 for 259 rounds\n",
      "CV with eta=0.01\n",
      "\tLogloss 0.298868533403 for 1289 rounds\n",
      "CV with eta=0.005\n",
      "\tLogloss 0.298945958294 for 2463 rounds\n",
      "Best params: 0.1, Logloss: 0.298725762419\n"
     ]
    }
   ],
   "source": [
    "min_logloss = float(\"Inf\")\n",
    "best_params = None\n",
    "\n",
    "for eta in [.3, .2, .1, .05, .01, .005]:\n",
    "    print(\"CV with eta={}\".format(eta))\n",
    "\n",
    "    # We update our parameters\n",
    "    params_lgb['learning_rate'] = eta\n",
    "    \n",
    "    # Run CV\n",
    "    cv_results = lgb.cv(\n",
    "        params_lgb,\n",
    "        lgb.Dataset(x_train_, label=y_train),\n",
    "        num_boost_round=5000,\n",
    "        seed=0,\n",
    "        nfold=5,\n",
    "        metrics=['binary_logloss'],\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "\n",
    "    # Update best logloss\n",
    "    mean_logloss = np.min(cv_results['binary_logloss-mean'])\n",
    "    boost_rounds = len(cv_results['binary_logloss-mean'])\n",
    "    print(\"\\tLogloss {} for {} rounds\".format(mean_logloss, boost_rounds))\n",
    "    if mean_logloss < min_logloss:\n",
    "        min_logloss = mean_logloss\n",
    "        best_params = eta\n",
    "\n",
    "print(\"Best params: {}, Logloss: {}\".format(best_params, min_logloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params_lgb['learning_rate'] = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_lgb_model = lgb.train(params_lgb,\n",
    "                      lgb.Dataset(x_train_, label=y_train),\n",
    "                      num_boost_round=157\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_CORRELATIVO</th>\n",
       "      <th>ATTRITION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47411</td>\n",
       "      <td>0.319690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39861</td>\n",
       "      <td>0.211634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38898</td>\n",
       "      <td>0.028088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50927</td>\n",
       "      <td>0.013631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32969</td>\n",
       "      <td>0.282040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_CORRELATIVO  ATTRITION\n",
       "0           47411   0.319690\n",
       "1           39861   0.211634\n",
       "2           38898   0.028088\n",
       "3           50927   0.013631\n",
       "4           32969   0.282040"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = best_lgb_model.predict(x_test_)\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission['ID_CORRELATIVO'] = x_test_.index\n",
    "submission['ATTRITION'] = y_pred\n",
    "submission.head()\n",
    "\n",
    "submission.to_csv('./data/submission14_LGB.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.4879995\ttest: 0.4870228\tbest: 0.4870228 (0)\ttotal: 186ms\tremaining: 22.1s\n",
      "1:\tlearn: 0.3996157\ttest: 0.3987906\tbest: 0.3987906 (1)\ttotal: 368ms\tremaining: 21.7s\n",
      "2:\tlearn: 0.3616182\ttest: 0.3604330\tbest: 0.3604330 (2)\ttotal: 569ms\tremaining: 22.2s\n",
      "3:\tlearn: 0.3438614\ttest: 0.3422769\tbest: 0.3422769 (3)\ttotal: 747ms\tremaining: 21.7s\n",
      "4:\tlearn: 0.3332636\ttest: 0.3318690\tbest: 0.3318690 (4)\ttotal: 943ms\tremaining: 21.7s\n",
      "5:\tlearn: 0.3278904\ttest: 0.3266996\tbest: 0.3266996 (5)\ttotal: 1.13s\tremaining: 21.5s\n",
      "6:\tlearn: 0.3234529\ttest: 0.3218799\tbest: 0.3218799 (6)\ttotal: 1.34s\tremaining: 21.7s\n",
      "7:\tlearn: 0.3205778\ttest: 0.3201654\tbest: 0.3201654 (7)\ttotal: 1.52s\tremaining: 21.3s\n",
      "8:\tlearn: 0.3173653\ttest: 0.3169261\tbest: 0.3169261 (8)\ttotal: 1.73s\tremaining: 21.3s\n",
      "9:\tlearn: 0.3167142\ttest: 0.3165499\tbest: 0.3165499 (9)\ttotal: 1.91s\tremaining: 21s\n",
      "10:\tlearn: 0.3155683\ttest: 0.3156029\tbest: 0.3156029 (10)\ttotal: 2.14s\tremaining: 21.2s\n",
      "11:\tlearn: 0.3147207\ttest: 0.3150824\tbest: 0.3150824 (11)\ttotal: 2.34s\tremaining: 21.1s\n",
      "12:\tlearn: 0.3137496\ttest: 0.3140350\tbest: 0.3140350 (12)\ttotal: 2.55s\tremaining: 21s\n",
      "13:\tlearn: 0.3128032\ttest: 0.3135057\tbest: 0.3135057 (13)\ttotal: 2.76s\tremaining: 20.9s\n",
      "14:\tlearn: 0.3115264\ttest: 0.3126470\tbest: 0.3126470 (14)\ttotal: 2.95s\tremaining: 20.7s\n",
      "15:\tlearn: 0.3106660\ttest: 0.3123436\tbest: 0.3123436 (15)\ttotal: 3.14s\tremaining: 20.4s\n",
      "16:\tlearn: 0.3096874\ttest: 0.3115586\tbest: 0.3115586 (16)\ttotal: 3.35s\tremaining: 20.3s\n",
      "17:\tlearn: 0.3092081\ttest: 0.3113040\tbest: 0.3113040 (17)\ttotal: 3.54s\tremaining: 20s\n",
      "18:\tlearn: 0.3080682\ttest: 0.3103885\tbest: 0.3103885 (18)\ttotal: 3.74s\tremaining: 19.9s\n",
      "19:\tlearn: 0.3069898\ttest: 0.3094293\tbest: 0.3094293 (19)\ttotal: 3.93s\tremaining: 19.6s\n",
      "20:\tlearn: 0.3068610\ttest: 0.3092453\tbest: 0.3092453 (20)\ttotal: 4.05s\tremaining: 19.1s\n",
      "21:\tlearn: 0.3067629\ttest: 0.3090858\tbest: 0.3090858 (21)\ttotal: 4.25s\tremaining: 18.9s\n",
      "22:\tlearn: 0.3057768\ttest: 0.3082316\tbest: 0.3082316 (22)\ttotal: 4.44s\tremaining: 18.7s\n",
      "23:\tlearn: 0.3052913\ttest: 0.3077369\tbest: 0.3077369 (23)\ttotal: 4.64s\tremaining: 18.6s\n",
      "24:\tlearn: 0.3039095\ttest: 0.3071962\tbest: 0.3071962 (24)\ttotal: 4.85s\tremaining: 18.4s\n",
      "25:\tlearn: 0.3035946\ttest: 0.3071373\tbest: 0.3071373 (25)\ttotal: 5.04s\tremaining: 18.2s\n",
      "26:\tlearn: 0.3027554\ttest: 0.3067945\tbest: 0.3067945 (26)\ttotal: 5.25s\tremaining: 18.1s\n",
      "27:\tlearn: 0.3025302\ttest: 0.3067894\tbest: 0.3067894 (27)\ttotal: 5.44s\tremaining: 17.9s\n",
      "28:\tlearn: 0.3016938\ttest: 0.3065345\tbest: 0.3065345 (28)\ttotal: 5.63s\tremaining: 17.7s\n",
      "29:\tlearn: 0.3012525\ttest: 0.3063722\tbest: 0.3063722 (29)\ttotal: 5.86s\tremaining: 17.6s\n",
      "30:\tlearn: 0.3006326\ttest: 0.3062407\tbest: 0.3062407 (30)\ttotal: 6.08s\tremaining: 17.5s\n",
      "31:\tlearn: 0.2999332\ttest: 0.3055014\tbest: 0.3055014 (31)\ttotal: 6.28s\tremaining: 17.3s\n",
      "32:\tlearn: 0.2996225\ttest: 0.3053610\tbest: 0.3053610 (32)\ttotal: 6.47s\tremaining: 17.1s\n",
      "33:\tlearn: 0.2986621\ttest: 0.3050518\tbest: 0.3050518 (33)\ttotal: 6.67s\tremaining: 16.9s\n",
      "34:\tlearn: 0.2983756\ttest: 0.3047786\tbest: 0.3047786 (34)\ttotal: 6.87s\tremaining: 16.7s\n",
      "35:\tlearn: 0.2978438\ttest: 0.3044977\tbest: 0.3044977 (35)\ttotal: 7.05s\tremaining: 16.5s\n",
      "36:\tlearn: 0.2976746\ttest: 0.3044615\tbest: 0.3044615 (36)\ttotal: 7.26s\tremaining: 16.3s\n",
      "37:\tlearn: 0.2972689\ttest: 0.3044140\tbest: 0.3044140 (37)\ttotal: 7.44s\tremaining: 16.1s\n",
      "38:\tlearn: 0.2968647\ttest: 0.3043233\tbest: 0.3043233 (38)\ttotal: 7.65s\tremaining: 15.9s\n",
      "39:\tlearn: 0.2964035\ttest: 0.3041514\tbest: 0.3041514 (39)\ttotal: 7.83s\tremaining: 15.7s\n",
      "40:\tlearn: 0.2960118\ttest: 0.3040648\tbest: 0.3040648 (40)\ttotal: 8.06s\tremaining: 15.5s\n",
      "41:\tlearn: 0.2958067\ttest: 0.3039315\tbest: 0.3039315 (41)\ttotal: 8.26s\tremaining: 15.3s\n",
      "42:\tlearn: 0.2950660\ttest: 0.3034288\tbest: 0.3034288 (42)\ttotal: 8.47s\tremaining: 15.2s\n",
      "43:\tlearn: 0.2948290\ttest: 0.3033442\tbest: 0.3033442 (43)\ttotal: 8.68s\tremaining: 15s\n",
      "44:\tlearn: 0.2944143\ttest: 0.3032289\tbest: 0.3032289 (44)\ttotal: 8.89s\tremaining: 14.8s\n",
      "45:\tlearn: 0.2939610\ttest: 0.3030492\tbest: 0.3030492 (45)\ttotal: 9.12s\tremaining: 14.7s\n",
      "46:\tlearn: 0.2936066\ttest: 0.3028610\tbest: 0.3028610 (46)\ttotal: 9.33s\tremaining: 14.5s\n",
      "47:\tlearn: 0.2931790\ttest: 0.3026904\tbest: 0.3026904 (47)\ttotal: 9.53s\tremaining: 14.3s\n",
      "48:\tlearn: 0.2926827\ttest: 0.3023180\tbest: 0.3023180 (48)\ttotal: 9.72s\tremaining: 14.1s\n",
      "49:\tlearn: 0.2923839\ttest: 0.3020688\tbest: 0.3020688 (49)\ttotal: 9.92s\tremaining: 13.9s\n",
      "50:\tlearn: 0.2922016\ttest: 0.3020505\tbest: 0.3020505 (50)\ttotal: 10.1s\tremaining: 13.7s\n",
      "51:\tlearn: 0.2918337\ttest: 0.3016353\tbest: 0.3016353 (51)\ttotal: 10.3s\tremaining: 13.5s\n",
      "52:\tlearn: 0.2912704\ttest: 0.3015299\tbest: 0.3015299 (52)\ttotal: 10.5s\tremaining: 13.3s\n",
      "53:\tlearn: 0.2908208\ttest: 0.3012779\tbest: 0.3012779 (53)\ttotal: 10.7s\tremaining: 13.1s\n",
      "54:\tlearn: 0.2902196\ttest: 0.3009719\tbest: 0.3009719 (54)\ttotal: 10.9s\tremaining: 12.9s\n",
      "55:\tlearn: 0.2901090\ttest: 0.3010576\tbest: 0.3009719 (54)\ttotal: 11.1s\tremaining: 12.7s\n",
      "56:\tlearn: 0.2894439\ttest: 0.3007651\tbest: 0.3007651 (56)\ttotal: 11.3s\tremaining: 12.5s\n",
      "57:\tlearn: 0.2892742\ttest: 0.3006416\tbest: 0.3006416 (57)\ttotal: 11.5s\tremaining: 12.3s\n",
      "58:\tlearn: 0.2891162\ttest: 0.3005451\tbest: 0.3005451 (58)\ttotal: 11.7s\tremaining: 12.1s\n",
      "59:\tlearn: 0.2888476\ttest: 0.3005641\tbest: 0.3005451 (58)\ttotal: 11.9s\tremaining: 11.9s\n",
      "60:\tlearn: 0.2886648\ttest: 0.3005306\tbest: 0.3005306 (60)\ttotal: 12s\tremaining: 11.6s\n",
      "61:\tlearn: 0.2884660\ttest: 0.3002774\tbest: 0.3002774 (61)\ttotal: 12.2s\tremaining: 11.4s\n",
      "62:\tlearn: 0.2882685\ttest: 0.3002994\tbest: 0.3002774 (61)\ttotal: 12.4s\tremaining: 11.2s\n",
      "63:\tlearn: 0.2880064\ttest: 0.3001137\tbest: 0.3001137 (63)\ttotal: 12.6s\tremaining: 11s\n",
      "64:\tlearn: 0.2878078\ttest: 0.3001307\tbest: 0.3001137 (63)\ttotal: 12.8s\tremaining: 10.8s\n",
      "65:\tlearn: 0.2876176\ttest: 0.3000540\tbest: 0.3000540 (65)\ttotal: 13s\tremaining: 10.6s\n",
      "66:\tlearn: 0.2873199\ttest: 0.3000750\tbest: 0.3000540 (65)\ttotal: 13.2s\tremaining: 10.4s\n",
      "67:\tlearn: 0.2870936\ttest: 0.3000338\tbest: 0.3000338 (67)\ttotal: 13.4s\tremaining: 10.2s\n",
      "68:\tlearn: 0.2867685\ttest: 0.2999103\tbest: 0.2999103 (68)\ttotal: 13.6s\tremaining: 10s\n",
      "69:\tlearn: 0.2866041\ttest: 0.2999631\tbest: 0.2999103 (68)\ttotal: 13.8s\tremaining: 9.85s\n",
      "70:\tlearn: 0.2865398\ttest: 0.2999655\tbest: 0.2999103 (68)\ttotal: 14s\tremaining: 9.64s\n",
      "71:\tlearn: 0.2864148\ttest: 0.3000422\tbest: 0.2999103 (68)\ttotal: 14.2s\tremaining: 9.45s\n",
      "72:\tlearn: 0.2858991\ttest: 0.2997126\tbest: 0.2997126 (72)\ttotal: 14.4s\tremaining: 9.26s\n",
      "73:\tlearn: 0.2856595\ttest: 0.2997518\tbest: 0.2997126 (72)\ttotal: 14.6s\tremaining: 9.08s\n",
      "74:\tlearn: 0.2855370\ttest: 0.2999069\tbest: 0.2997126 (72)\ttotal: 14.8s\tremaining: 8.88s\n",
      "75:\tlearn: 0.2853586\ttest: 0.2996979\tbest: 0.2996979 (75)\ttotal: 15s\tremaining: 8.69s\n",
      "76:\tlearn: 0.2851268\ttest: 0.2996875\tbest: 0.2996875 (76)\ttotal: 15.2s\tremaining: 8.49s\n",
      "77:\tlearn: 0.2849346\ttest: 0.2995376\tbest: 0.2995376 (77)\ttotal: 15.4s\tremaining: 8.29s\n",
      "78:\tlearn: 0.2847379\ttest: 0.2995646\tbest: 0.2995376 (77)\ttotal: 15.6s\tremaining: 8.09s\n",
      "79:\tlearn: 0.2845489\ttest: 0.2997560\tbest: 0.2995376 (77)\ttotal: 15.8s\tremaining: 7.89s\n",
      "80:\tlearn: 0.2842288\ttest: 0.2996912\tbest: 0.2995376 (77)\ttotal: 16s\tremaining: 7.69s\n",
      "81:\tlearn: 0.2840038\ttest: 0.2997022\tbest: 0.2995376 (77)\ttotal: 16.2s\tremaining: 7.5s\n",
      "82:\tlearn: 0.2837592\ttest: 0.2997597\tbest: 0.2995376 (77)\ttotal: 16.4s\tremaining: 7.3s\n",
      "83:\tlearn: 0.2835366\ttest: 0.2997193\tbest: 0.2995376 (77)\ttotal: 16.6s\tremaining: 7.13s\n",
      "84:\tlearn: 0.2833815\ttest: 0.2997028\tbest: 0.2995376 (77)\ttotal: 16.9s\tremaining: 6.94s\n",
      "85:\tlearn: 0.2832094\ttest: 0.2994967\tbest: 0.2994967 (85)\ttotal: 17.1s\tremaining: 6.76s\n",
      "86:\tlearn: 0.2829779\ttest: 0.2994692\tbest: 0.2994692 (86)\ttotal: 17.3s\tremaining: 6.55s\n",
      "87:\tlearn: 0.2828548\ttest: 0.2995420\tbest: 0.2994692 (86)\ttotal: 17.5s\tremaining: 6.37s\n",
      "88:\tlearn: 0.2825615\ttest: 0.2993842\tbest: 0.2993842 (88)\ttotal: 17.7s\tremaining: 6.17s\n",
      "89:\tlearn: 0.2823685\ttest: 0.2994464\tbest: 0.2993842 (88)\ttotal: 17.9s\tremaining: 5.98s\n",
      "90:\tlearn: 0.2821558\ttest: 0.2995487\tbest: 0.2993842 (88)\ttotal: 18.1s\tremaining: 5.78s\n",
      "91:\tlearn: 0.2818664\ttest: 0.2997929\tbest: 0.2993842 (88)\ttotal: 18.3s\tremaining: 5.58s\n",
      "92:\tlearn: 0.2817311\ttest: 0.2998128\tbest: 0.2993842 (88)\ttotal: 18.6s\tremaining: 5.39s\n",
      "93:\tlearn: 0.2814163\ttest: 0.2999109\tbest: 0.2993842 (88)\ttotal: 18.7s\tremaining: 5.18s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94:\tlearn: 0.2811150\ttest: 0.2999763\tbest: 0.2993842 (88)\ttotal: 18.9s\tremaining: 4.98s\n",
      "95:\tlearn: 0.2808835\ttest: 0.3000508\tbest: 0.2993842 (88)\ttotal: 19.1s\tremaining: 4.78s\n",
      "96:\tlearn: 0.2805344\ttest: 0.3003778\tbest: 0.2993842 (88)\ttotal: 19.3s\tremaining: 4.58s\n",
      "97:\tlearn: 0.2802377\ttest: 0.3003655\tbest: 0.2993842 (88)\ttotal: 19.5s\tremaining: 4.38s\n",
      "98:\tlearn: 0.2800196\ttest: 0.3003154\tbest: 0.2993842 (88)\ttotal: 19.7s\tremaining: 4.19s\n",
      "99:\tlearn: 0.2798289\ttest: 0.3003751\tbest: 0.2993842 (88)\ttotal: 19.9s\tremaining: 3.98s\n",
      "100:\tlearn: 0.2796466\ttest: 0.3004894\tbest: 0.2993842 (88)\ttotal: 20.1s\tremaining: 3.79s\n",
      "101:\tlearn: 0.2795575\ttest: 0.3004797\tbest: 0.2993842 (88)\ttotal: 20.3s\tremaining: 3.58s\n",
      "102:\tlearn: 0.2793674\ttest: 0.3006760\tbest: 0.2993842 (88)\ttotal: 20.5s\tremaining: 3.38s\n",
      "103:\tlearn: 0.2791936\ttest: 0.3007168\tbest: 0.2993842 (88)\ttotal: 20.7s\tremaining: 3.18s\n",
      "104:\tlearn: 0.2789215\ttest: 0.3003583\tbest: 0.2993842 (88)\ttotal: 20.9s\tremaining: 2.98s\n",
      "105:\tlearn: 0.2785636\ttest: 0.3001598\tbest: 0.2993842 (88)\ttotal: 21.1s\tremaining: 2.78s\n",
      "106:\tlearn: 0.2784118\ttest: 0.3001001\tbest: 0.2993842 (88)\ttotal: 21.3s\tremaining: 2.59s\n",
      "107:\tlearn: 0.2782067\ttest: 0.3001867\tbest: 0.2993842 (88)\ttotal: 21.5s\tremaining: 2.39s\n",
      "108:\tlearn: 0.2780598\ttest: 0.3001114\tbest: 0.2993842 (88)\ttotal: 21.7s\tremaining: 2.19s\n",
      "109:\tlearn: 0.2778455\ttest: 0.2999913\tbest: 0.2993842 (88)\ttotal: 21.9s\tremaining: 1.99s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.2993841966\n",
      "bestIteration = 88\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core._CatBoostBase at 0x2aad94e0>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_ = xtrain.fillna(-999)\n",
    "xtest_ = xtest.fillna(-999)\n",
    "\n",
    "cat = CatBoostClassifier(iterations=120, \n",
    "                         learning_rate=0.3, \n",
    "                         depth=6, \n",
    "                         eval_metric='Logloss',\n",
    "                         l2_leaf_reg=3,\n",
    "                         border_count=32,\n",
    "                         od_type='Iter')\n",
    "\n",
    "cat.fit(xtrain_, ytrain, eval_set=(xtest_, ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from catboost import cv, Pool\n",
    "\n",
    "x_train__ = x_train_.fillna(-999)\n",
    "\n",
    "params_cat = {\n",
    "    'depth': 6,\n",
    "    'learning_rate': .3,\n",
    "    'iterations': 1000,\n",
    "    'loss_function': 'Logloss',\n",
    "    'l2_leaf_reg': 3,\n",
    "    'border_count': 32,\n",
    "    'od_type': 'Iter'\n",
    "}\n",
    "\n",
    "#cv(params, Pool(x_train__, y_train),  partition_random_seed=0, fold_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with depth=4\n",
      "\tLogloss 0.30423980111 for 147 rounds\n",
      "CV with depth=5\n",
      "\tLogloss 0.302573060641 for 154 rounds\n",
      "CV with depth=6\n",
      "\tLogloss 0.302653517225 for 115 rounds\n",
      "CV with depth=7\n",
      "\tLogloss 0.301941417523 for 91 rounds\n",
      "CV with depth=8\n",
      "\tLogloss 0.302723864531 for 96 rounds\n",
      "CV with depth=9\n",
      "\tLogloss 0.302432334958 for 67 rounds\n",
      "CV with depth=10\n",
      "\tLogloss 0.302566840871 for 60 rounds\n",
      "CV with depth=11\n",
      "\tLogloss 0.302757829367 for 64 rounds\n",
      "Best params: 7, Logloss: 0.301941417523, Rounds: 91\n"
     ]
    }
   ],
   "source": [
    "gridsearch_params = [depth for depth in range(4,12)]\n",
    "\n",
    "min_logloss = float(\"Inf\")\n",
    "best_params = None\n",
    "\n",
    "for depth in gridsearch_params:\n",
    "    print(\"CV with depth={}\".format(depth))\n",
    "\n",
    "    # Update our parameters\n",
    "    params_cat['depth'] = depth\n",
    "\n",
    "    # Run CV\n",
    "    cv_results = cv(\n",
    "        params_cat,\n",
    "        Pool(x_train__, y_train),  \n",
    "        partition_random_seed=0, \n",
    "        fold_count=5\n",
    "    )\n",
    "\n",
    "    # Update best logloss\n",
    "    mean_logloss = np.min(cv_results['Logloss_test_avg'])\n",
    "    boost_rounds = len(cv_results['Logloss_test_avg'])\n",
    "    print(\"\\tLogloss {} for {} rounds\".format(mean_logloss, boost_rounds))\n",
    "    if mean_logloss < min_logloss:\n",
    "        min_logloss = mean_logloss\n",
    "        best_params = depth\n",
    "        best_rounds = boost_rounds\n",
    "\n",
    "print(\"Best params: {}, Logloss: {}, Rounds: {}\".format(best_params, min_logloss, best_rounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_cat['depth'] = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'border_count': 32,\n",
       " 'depth': 7,\n",
       " 'iterations': 533,\n",
       " 'l2_leaf_reg': 500,\n",
       " 'learning_rate': 0.1,\n",
       " 'loss_function': 'Logloss',\n",
       " 'od_type': 'Iter'}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with leaf_reg=10\n",
      "\tLogloss 0.297993372921 for 405 rounds\n",
      "CV with leaf_reg=100\n",
      "\tLogloss 0.296618841003 for 533 rounds\n",
      "CV with leaf_reg=200\n",
      "\tLogloss 0.296863228118 for 533 rounds\n",
      "CV with leaf_reg=500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-123-11261b083aaf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mPool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mpartition_random_seed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mfold_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     )\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JERSON\\Anaconda2\\lib\\site-packages\\catboost\\core.pyc\u001b[0m in \u001b[0;36mcv\u001b[1;34m(params, pool, fold_count, inverted, partition_random_seed, shuffle)\u001b[0m\n\u001b[0;32m   1674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1675\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mlog_fixup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_cv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfold_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minverted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartition_random_seed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._cv\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._cv\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gridsearch_params = [10,100,200,500]\n",
    "\n",
    "min_logloss = float(\"Inf\")\n",
    "best_params = None\n",
    "\n",
    "for leaf_reg in gridsearch_params:\n",
    "    print(\"CV with leaf_reg={}\".format(leaf_reg))\n",
    "\n",
    "    # Update our parameters\n",
    "    params_cat['l2_leaf_reg'] = leaf_reg\n",
    "\n",
    "    # Run CV\n",
    "    cv_results = cv(\n",
    "        params_cat,\n",
    "        Pool(x_train__, y_train),  \n",
    "        partition_random_seed=0, \n",
    "        fold_count=5\n",
    "    )\n",
    "\n",
    "    # Update best logloss\n",
    "    mean_logloss = np.min(cv_results['Logloss_test_avg'])\n",
    "    boost_rounds = len(cv_results['Logloss_test_avg'])\n",
    "    print(\"\\tLogloss {} for {} rounds\".format(mean_logloss, boost_rounds))\n",
    "    if mean_logloss < min_logloss:\n",
    "        min_logloss = mean_logloss\n",
    "        best_params = leaf_reg\n",
    "        best_rounds = boost_rounds\n",
    "\n",
    "print(\"Best params: {}, Logloss: {}, Rounds: {}\".format(best_params, min_logloss, best_rounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_cat['l2_leaf_reg'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with eta=0.05\n",
      "\tLogloss 0.296545588394 for 1020 rounds\n",
      "Best params: 0.05, Logloss: 0.296545588394, Rounds: 1020\n"
     ]
    }
   ],
   "source": [
    "min_logloss = float(\"Inf\")\n",
    "best_params = None\n",
    "\n",
    "params_cat['iterations'] = 5000\n",
    "\n",
    "for eta in [0.05]:\n",
    "    print(\"CV with eta={}\".format(eta))\n",
    "\n",
    "    # We update our parameters\n",
    "    params_cat['learning_rate'] = eta\n",
    "\n",
    "    # Run CV\n",
    "    cv_results = cv(\n",
    "        params_cat,\n",
    "        Pool(x_train__, y_train),  \n",
    "        partition_random_seed=0, \n",
    "        fold_count=5\n",
    "    )\n",
    "\n",
    "    # Update best logloss\n",
    "    mean_logloss = np.min(cv_results['Logloss_test_avg'])\n",
    "    boost_rounds = len(cv_results['Logloss_test_avg'])\n",
    "    print(\"\\tLogloss {} for {} rounds\".format(mean_logloss, boost_rounds))\n",
    "    if mean_logloss < min_logloss:\n",
    "        min_logloss = mean_logloss\n",
    "        best_params = eta\n",
    "        best_rounds = boost_rounds\n",
    "\n",
    "print(\"Best params: {}, Logloss: {}, Rounds: {}\".format(best_params, min_logloss, best_rounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_cat['iterations'] = 533\n",
    "params_cat['learning_rate'] = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6099229\ttotal: 221ms\tremaining: 1m 57s\n",
      "1:\tlearn: 0.5481480\ttotal: 445ms\tremaining: 1m 58s\n",
      "2:\tlearn: 0.4995824\ttotal: 696ms\tremaining: 2m 2s\n",
      "3:\tlearn: 0.4619350\ttotal: 947ms\tremaining: 2m 5s\n",
      "4:\tlearn: 0.4402092\ttotal: 1.19s\tremaining: 2m 5s\n",
      "5:\tlearn: 0.4168744\ttotal: 1.46s\tremaining: 2m 8s\n",
      "6:\tlearn: 0.3996531\ttotal: 1.69s\tremaining: 2m 7s\n",
      "7:\tlearn: 0.3855065\ttotal: 1.94s\tremaining: 2m 7s\n",
      "8:\tlearn: 0.3745852\ttotal: 2.18s\tremaining: 2m 6s\n",
      "9:\tlearn: 0.3639721\ttotal: 2.42s\tremaining: 2m 6s\n",
      "10:\tlearn: 0.3575227\ttotal: 2.68s\tremaining: 2m 7s\n",
      "11:\tlearn: 0.3522394\ttotal: 2.92s\tremaining: 2m 6s\n",
      "12:\tlearn: 0.3489108\ttotal: 3.16s\tremaining: 2m 6s\n",
      "13:\tlearn: 0.3451069\ttotal: 3.4s\tremaining: 2m 6s\n",
      "14:\tlearn: 0.3408515\ttotal: 3.66s\tremaining: 2m 6s\n",
      "15:\tlearn: 0.3383942\ttotal: 3.92s\tremaining: 2m 6s\n",
      "16:\tlearn: 0.3357892\ttotal: 4.17s\tremaining: 2m 6s\n",
      "17:\tlearn: 0.3336509\ttotal: 4.43s\tremaining: 2m 6s\n",
      "18:\tlearn: 0.3311787\ttotal: 4.74s\tremaining: 2m 8s\n",
      "19:\tlearn: 0.3293177\ttotal: 4.98s\tremaining: 2m 7s\n",
      "20:\tlearn: 0.3273580\ttotal: 5.24s\tremaining: 2m 7s\n",
      "21:\tlearn: 0.3261029\ttotal: 5.5s\tremaining: 2m 7s\n",
      "22:\tlearn: 0.3246034\ttotal: 5.75s\tremaining: 2m 7s\n",
      "23:\tlearn: 0.3238710\ttotal: 6.01s\tremaining: 2m 7s\n",
      "24:\tlearn: 0.3228594\ttotal: 6.25s\tremaining: 2m 6s\n",
      "25:\tlearn: 0.3210853\ttotal: 6.5s\tremaining: 2m 6s\n",
      "26:\tlearn: 0.3208232\ttotal: 6.74s\tremaining: 2m 6s\n",
      "27:\tlearn: 0.3195485\ttotal: 6.99s\tremaining: 2m 6s\n",
      "28:\tlearn: 0.3192466\ttotal: 7.25s\tremaining: 2m 6s\n",
      "29:\tlearn: 0.3181763\ttotal: 7.49s\tremaining: 2m 5s\n",
      "30:\tlearn: 0.3177215\ttotal: 7.74s\tremaining: 2m 5s\n",
      "31:\tlearn: 0.3172090\ttotal: 8.01s\tremaining: 2m 5s\n",
      "32:\tlearn: 0.3162342\ttotal: 8.27s\tremaining: 2m 5s\n",
      "33:\tlearn: 0.3155302\ttotal: 8.54s\tremaining: 2m 5s\n",
      "34:\tlearn: 0.3150906\ttotal: 8.82s\tremaining: 2m 5s\n",
      "35:\tlearn: 0.3145954\ttotal: 9.1s\tremaining: 2m 5s\n",
      "36:\tlearn: 0.3138606\ttotal: 9.4s\tremaining: 2m 5s\n",
      "37:\tlearn: 0.3132586\ttotal: 9.68s\tremaining: 2m 6s\n",
      "38:\tlearn: 0.3129110\ttotal: 9.96s\tremaining: 2m 6s\n",
      "39:\tlearn: 0.3123102\ttotal: 10.2s\tremaining: 2m 5s\n",
      "40:\tlearn: 0.3114963\ttotal: 10.5s\tremaining: 2m 6s\n",
      "41:\tlearn: 0.3110860\ttotal: 10.8s\tremaining: 2m 6s\n",
      "42:\tlearn: 0.3106450\ttotal: 11.1s\tremaining: 2m 6s\n",
      "43:\tlearn: 0.3104976\ttotal: 11.3s\tremaining: 2m 5s\n",
      "44:\tlearn: 0.3103531\ttotal: 11.5s\tremaining: 2m 5s\n",
      "45:\tlearn: 0.3101163\ttotal: 11.8s\tremaining: 2m 4s\n",
      "46:\tlearn: 0.3097558\ttotal: 12s\tremaining: 2m 3s\n",
      "47:\tlearn: 0.3095936\ttotal: 12.2s\tremaining: 2m 3s\n",
      "48:\tlearn: 0.3092550\ttotal: 12.5s\tremaining: 2m 3s\n",
      "49:\tlearn: 0.3087720\ttotal: 12.7s\tremaining: 2m 2s\n",
      "50:\tlearn: 0.3084743\ttotal: 12.9s\tremaining: 2m 2s\n",
      "51:\tlearn: 0.3083671\ttotal: 13.2s\tremaining: 2m 1s\n",
      "52:\tlearn: 0.3079680\ttotal: 13.4s\tremaining: 2m 1s\n",
      "53:\tlearn: 0.3074743\ttotal: 13.6s\tremaining: 2m\n",
      "54:\tlearn: 0.3072151\ttotal: 13.9s\tremaining: 2m\n",
      "55:\tlearn: 0.3069838\ttotal: 14.1s\tremaining: 2m\n",
      "56:\tlearn: 0.3069128\ttotal: 14.3s\tremaining: 1m 59s\n",
      "57:\tlearn: 0.3066564\ttotal: 14.6s\tremaining: 1m 59s\n",
      "58:\tlearn: 0.3064515\ttotal: 14.8s\tremaining: 1m 59s\n",
      "59:\tlearn: 0.3061048\ttotal: 15.1s\tremaining: 1m 58s\n",
      "60:\tlearn: 0.3060026\ttotal: 15.3s\tremaining: 1m 58s\n",
      "61:\tlearn: 0.3058322\ttotal: 15.5s\tremaining: 1m 58s\n",
      "62:\tlearn: 0.3056193\ttotal: 15.8s\tremaining: 1m 57s\n",
      "63:\tlearn: 0.3055237\ttotal: 16s\tremaining: 1m 57s\n",
      "64:\tlearn: 0.3052750\ttotal: 16.3s\tremaining: 1m 57s\n",
      "65:\tlearn: 0.3049751\ttotal: 16.5s\tremaining: 1m 56s\n",
      "66:\tlearn: 0.3046164\ttotal: 16.8s\tremaining: 1m 56s\n",
      "67:\tlearn: 0.3042131\ttotal: 17s\tremaining: 1m 56s\n",
      "68:\tlearn: 0.3041109\ttotal: 17.3s\tremaining: 1m 56s\n",
      "69:\tlearn: 0.3040448\ttotal: 17.5s\tremaining: 1m 55s\n",
      "70:\tlearn: 0.3037559\ttotal: 17.7s\tremaining: 1m 55s\n",
      "71:\tlearn: 0.3035225\ttotal: 18s\tremaining: 1m 55s\n",
      "72:\tlearn: 0.3033488\ttotal: 18.2s\tremaining: 1m 54s\n",
      "73:\tlearn: 0.3031323\ttotal: 18.4s\tremaining: 1m 54s\n",
      "74:\tlearn: 0.3028331\ttotal: 18.7s\tremaining: 1m 54s\n",
      "75:\tlearn: 0.3026604\ttotal: 19s\tremaining: 1m 54s\n",
      "76:\tlearn: 0.3023263\ttotal: 19.2s\tremaining: 1m 53s\n",
      "77:\tlearn: 0.3022326\ttotal: 19.4s\tremaining: 1m 53s\n",
      "78:\tlearn: 0.3020328\ttotal: 19.7s\tremaining: 1m 53s\n",
      "79:\tlearn: 0.3017689\ttotal: 19.9s\tremaining: 1m 52s\n",
      "80:\tlearn: 0.3016395\ttotal: 20.2s\tremaining: 1m 52s\n",
      "81:\tlearn: 0.3015253\ttotal: 20.4s\tremaining: 1m 52s\n",
      "82:\tlearn: 0.3014229\ttotal: 20.7s\tremaining: 1m 52s\n",
      "83:\tlearn: 0.3013544\ttotal: 20.9s\tremaining: 1m 51s\n",
      "84:\tlearn: 0.3012048\ttotal: 21.2s\tremaining: 1m 51s\n",
      "85:\tlearn: 0.3011781\ttotal: 21.3s\tremaining: 1m 50s\n",
      "86:\tlearn: 0.3011272\ttotal: 21.5s\tremaining: 1m 50s\n",
      "87:\tlearn: 0.3009389\ttotal: 21.8s\tremaining: 1m 50s\n",
      "88:\tlearn: 0.3006646\ttotal: 22s\tremaining: 1m 49s\n",
      "89:\tlearn: 0.3004663\ttotal: 22.2s\tremaining: 1m 49s\n",
      "90:\tlearn: 0.3003081\ttotal: 22.5s\tremaining: 1m 49s\n",
      "91:\tlearn: 0.3000818\ttotal: 22.7s\tremaining: 1m 48s\n",
      "92:\tlearn: 0.3000125\ttotal: 22.9s\tremaining: 1m 48s\n",
      "93:\tlearn: 0.2998651\ttotal: 23.2s\tremaining: 1m 48s\n",
      "94:\tlearn: 0.2997280\ttotal: 23.4s\tremaining: 1m 47s\n",
      "95:\tlearn: 0.2996271\ttotal: 23.6s\tremaining: 1m 47s\n",
      "96:\tlearn: 0.2995284\ttotal: 23.8s\tremaining: 1m 47s\n",
      "97:\tlearn: 0.2994735\ttotal: 24.1s\tremaining: 1m 46s\n",
      "98:\tlearn: 0.2993401\ttotal: 24.3s\tremaining: 1m 46s\n",
      "99:\tlearn: 0.2992385\ttotal: 24.5s\tremaining: 1m 46s\n",
      "100:\tlearn: 0.2991877\ttotal: 24.8s\tremaining: 1m 45s\n",
      "101:\tlearn: 0.2991114\ttotal: 25s\tremaining: 1m 45s\n",
      "102:\tlearn: 0.2986683\ttotal: 25.2s\tremaining: 1m 45s\n",
      "103:\tlearn: 0.2985118\ttotal: 25.5s\tremaining: 1m 45s\n",
      "104:\tlearn: 0.2983432\ttotal: 25.7s\tremaining: 1m 44s\n",
      "105:\tlearn: 0.2983123\ttotal: 25.9s\tremaining: 1m 44s\n",
      "106:\tlearn: 0.2982463\ttotal: 26.2s\tremaining: 1m 44s\n",
      "107:\tlearn: 0.2981709\ttotal: 26.4s\tremaining: 1m 43s\n",
      "108:\tlearn: 0.2979951\ttotal: 26.7s\tremaining: 1m 43s\n",
      "109:\tlearn: 0.2979155\ttotal: 26.9s\tremaining: 1m 43s\n",
      "110:\tlearn: 0.2977091\ttotal: 27.1s\tremaining: 1m 43s\n",
      "111:\tlearn: 0.2974681\ttotal: 27.4s\tremaining: 1m 42s\n",
      "112:\tlearn: 0.2974032\ttotal: 27.6s\tremaining: 1m 42s\n",
      "113:\tlearn: 0.2973800\ttotal: 27.8s\tremaining: 1m 42s\n",
      "114:\tlearn: 0.2971684\ttotal: 28.1s\tremaining: 1m 42s\n",
      "115:\tlearn: 0.2969987\ttotal: 28.3s\tremaining: 1m 41s\n",
      "116:\tlearn: 0.2969473\ttotal: 28.6s\tremaining: 1m 41s\n",
      "117:\tlearn: 0.2967720\ttotal: 28.8s\tremaining: 1m 41s\n",
      "118:\tlearn: 0.2965534\ttotal: 29.1s\tremaining: 1m 41s\n",
      "119:\tlearn: 0.2965263\ttotal: 29.3s\tremaining: 1m 40s\n",
      "120:\tlearn: 0.2964292\ttotal: 29.6s\tremaining: 1m 40s\n",
      "121:\tlearn: 0.2962345\ttotal: 29.8s\tremaining: 1m 40s\n",
      "122:\tlearn: 0.2961972\ttotal: 30s\tremaining: 1m 40s\n",
      "123:\tlearn: 0.2961051\ttotal: 30.3s\tremaining: 1m 39s\n",
      "124:\tlearn: 0.2960780\ttotal: 30.5s\tremaining: 1m 39s\n",
      "125:\tlearn: 0.2960393\ttotal: 30.7s\tremaining: 1m 39s\n",
      "126:\tlearn: 0.2959226\ttotal: 31s\tremaining: 1m 39s\n",
      "127:\tlearn: 0.2957926\ttotal: 31.3s\tremaining: 1m 38s\n",
      "128:\tlearn: 0.2956455\ttotal: 31.5s\tremaining: 1m 38s\n",
      "129:\tlearn: 0.2955232\ttotal: 31.8s\tremaining: 1m 38s\n",
      "130:\tlearn: 0.2954088\ttotal: 32s\tremaining: 1m 38s\n",
      "131:\tlearn: 0.2952288\ttotal: 32.3s\tremaining: 1m 38s\n",
      "132:\tlearn: 0.2951111\ttotal: 32.5s\tremaining: 1m 37s\n",
      "133:\tlearn: 0.2950371\ttotal: 32.8s\tremaining: 1m 37s\n",
      "134:\tlearn: 0.2948890\ttotal: 33s\tremaining: 1m 37s\n",
      "135:\tlearn: 0.2948009\ttotal: 33.3s\tremaining: 1m 37s\n",
      "136:\tlearn: 0.2947265\ttotal: 33.5s\tremaining: 1m 36s\n",
      "137:\tlearn: 0.2946668\ttotal: 33.8s\tremaining: 1m 36s\n",
      "138:\tlearn: 0.2945436\ttotal: 34s\tremaining: 1m 36s\n",
      "139:\tlearn: 0.2944139\ttotal: 34.3s\tremaining: 1m 36s\n",
      "140:\tlearn: 0.2942985\ttotal: 34.5s\tremaining: 1m 35s\n",
      "141:\tlearn: 0.2941384\ttotal: 34.8s\tremaining: 1m 35s\n",
      "142:\tlearn: 0.2940103\ttotal: 35s\tremaining: 1m 35s\n",
      "143:\tlearn: 0.2938627\ttotal: 35.2s\tremaining: 1m 35s\n",
      "144:\tlearn: 0.2937781\ttotal: 35.5s\tremaining: 1m 34s\n",
      "145:\tlearn: 0.2936180\ttotal: 35.7s\tremaining: 1m 34s\n",
      "146:\tlearn: 0.2934754\ttotal: 36s\tremaining: 1m 34s\n",
      "147:\tlearn: 0.2933880\ttotal: 36.2s\tremaining: 1m 34s\n",
      "148:\tlearn: 0.2933197\ttotal: 36.4s\tremaining: 1m 33s\n",
      "149:\tlearn: 0.2931969\ttotal: 36.7s\tremaining: 1m 33s\n",
      "150:\tlearn: 0.2931824\ttotal: 36.9s\tremaining: 1m 33s\n",
      "151:\tlearn: 0.2931201\ttotal: 37.2s\tremaining: 1m 33s\n",
      "152:\tlearn: 0.2929588\ttotal: 37.5s\tremaining: 1m 33s\n",
      "153:\tlearn: 0.2929035\ttotal: 37.7s\tremaining: 1m 32s\n",
      "154:\tlearn: 0.2928899\ttotal: 37.9s\tremaining: 1m 32s\n",
      "155:\tlearn: 0.2928132\ttotal: 38.2s\tremaining: 1m 32s\n",
      "156:\tlearn: 0.2926614\ttotal: 38.4s\tremaining: 1m 32s\n",
      "157:\tlearn: 0.2926173\ttotal: 38.7s\tremaining: 1m 31s\n",
      "158:\tlearn: 0.2924769\ttotal: 38.9s\tremaining: 1m 31s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159:\tlearn: 0.2924095\ttotal: 39.2s\tremaining: 1m 31s\n",
      "160:\tlearn: 0.2923125\ttotal: 39.4s\tremaining: 1m 31s\n",
      "161:\tlearn: 0.2922154\ttotal: 39.6s\tremaining: 1m 30s\n",
      "162:\tlearn: 0.2921130\ttotal: 39.9s\tremaining: 1m 30s\n",
      "163:\tlearn: 0.2920779\ttotal: 40.1s\tremaining: 1m 30s\n",
      "164:\tlearn: 0.2920612\ttotal: 40.4s\tremaining: 1m 30s\n",
      "165:\tlearn: 0.2920009\ttotal: 40.6s\tremaining: 1m 29s\n",
      "166:\tlearn: 0.2919348\ttotal: 40.8s\tremaining: 1m 29s\n",
      "167:\tlearn: 0.2919149\ttotal: 41.1s\tremaining: 1m 29s\n",
      "168:\tlearn: 0.2918546\ttotal: 41.3s\tremaining: 1m 29s\n",
      "169:\tlearn: 0.2916809\ttotal: 41.6s\tremaining: 1m 28s\n",
      "170:\tlearn: 0.2916629\ttotal: 41.8s\tremaining: 1m 28s\n",
      "171:\tlearn: 0.2916233\ttotal: 42s\tremaining: 1m 28s\n",
      "172:\tlearn: 0.2916024\ttotal: 42.3s\tremaining: 1m 27s\n",
      "173:\tlearn: 0.2915481\ttotal: 42.5s\tremaining: 1m 27s\n",
      "174:\tlearn: 0.2914932\ttotal: 42.8s\tremaining: 1m 27s\n",
      "175:\tlearn: 0.2913347\ttotal: 43s\tremaining: 1m 27s\n",
      "176:\tlearn: 0.2911900\ttotal: 43.2s\tremaining: 1m 26s\n",
      "177:\tlearn: 0.2911103\ttotal: 43.5s\tremaining: 1m 26s\n",
      "178:\tlearn: 0.2910642\ttotal: 43.7s\tremaining: 1m 26s\n",
      "179:\tlearn: 0.2909533\ttotal: 44s\tremaining: 1m 26s\n",
      "180:\tlearn: 0.2909013\ttotal: 44.2s\tremaining: 1m 25s\n",
      "181:\tlearn: 0.2907328\ttotal: 44.4s\tremaining: 1m 25s\n",
      "182:\tlearn: 0.2906366\ttotal: 44.7s\tremaining: 1m 25s\n",
      "183:\tlearn: 0.2905812\ttotal: 44.9s\tremaining: 1m 25s\n",
      "184:\tlearn: 0.2905135\ttotal: 45.1s\tremaining: 1m 24s\n",
      "185:\tlearn: 0.2904359\ttotal: 45.4s\tremaining: 1m 24s\n",
      "186:\tlearn: 0.2903790\ttotal: 45.6s\tremaining: 1m 24s\n",
      "187:\tlearn: 0.2903359\ttotal: 45.8s\tremaining: 1m 24s\n",
      "188:\tlearn: 0.2902483\ttotal: 46.1s\tremaining: 1m 23s\n",
      "189:\tlearn: 0.2901613\ttotal: 46.3s\tremaining: 1m 23s\n",
      "190:\tlearn: 0.2901339\ttotal: 46.5s\tremaining: 1m 23s\n",
      "191:\tlearn: 0.2900512\ttotal: 46.8s\tremaining: 1m 23s\n",
      "192:\tlearn: 0.2899915\ttotal: 47s\tremaining: 1m 22s\n",
      "193:\tlearn: 0.2899245\ttotal: 47.2s\tremaining: 1m 22s\n",
      "194:\tlearn: 0.2899115\ttotal: 47.4s\tremaining: 1m 22s\n",
      "195:\tlearn: 0.2898628\ttotal: 47.7s\tremaining: 1m 21s\n",
      "196:\tlearn: 0.2897971\ttotal: 47.9s\tremaining: 1m 21s\n",
      "197:\tlearn: 0.2897220\ttotal: 48.1s\tremaining: 1m 21s\n",
      "198:\tlearn: 0.2896334\ttotal: 48.4s\tremaining: 1m 21s\n",
      "199:\tlearn: 0.2895597\ttotal: 48.6s\tremaining: 1m 20s\n",
      "200:\tlearn: 0.2894451\ttotal: 48.9s\tremaining: 1m 20s\n",
      "201:\tlearn: 0.2893274\ttotal: 49.1s\tremaining: 1m 20s\n",
      "202:\tlearn: 0.2892345\ttotal: 49.3s\tremaining: 1m 20s\n",
      "203:\tlearn: 0.2891215\ttotal: 49.6s\tremaining: 1m 19s\n",
      "204:\tlearn: 0.2890565\ttotal: 49.8s\tremaining: 1m 19s\n",
      "205:\tlearn: 0.2890147\ttotal: 50s\tremaining: 1m 19s\n",
      "206:\tlearn: 0.2890026\ttotal: 50.3s\tremaining: 1m 19s\n",
      "207:\tlearn: 0.2889069\ttotal: 50.5s\tremaining: 1m 18s\n",
      "208:\tlearn: 0.2888238\ttotal: 50.7s\tremaining: 1m 18s\n",
      "209:\tlearn: 0.2887572\ttotal: 51s\tremaining: 1m 18s\n",
      "210:\tlearn: 0.2886776\ttotal: 51.2s\tremaining: 1m 18s\n",
      "211:\tlearn: 0.2886272\ttotal: 51.4s\tremaining: 1m 17s\n",
      "212:\tlearn: 0.2884752\ttotal: 51.7s\tremaining: 1m 17s\n",
      "213:\tlearn: 0.2884353\ttotal: 51.9s\tremaining: 1m 17s\n",
      "214:\tlearn: 0.2883375\ttotal: 52.2s\tremaining: 1m 17s\n",
      "215:\tlearn: 0.2882544\ttotal: 52.4s\tremaining: 1m 16s\n",
      "216:\tlearn: 0.2881894\ttotal: 52.6s\tremaining: 1m 16s\n",
      "217:\tlearn: 0.2881145\ttotal: 52.9s\tremaining: 1m 16s\n",
      "218:\tlearn: 0.2880225\ttotal: 53.1s\tremaining: 1m 16s\n",
      "219:\tlearn: 0.2879755\ttotal: 53.3s\tremaining: 1m 15s\n",
      "220:\tlearn: 0.2878732\ttotal: 53.6s\tremaining: 1m 15s\n",
      "221:\tlearn: 0.2878356\ttotal: 53.8s\tremaining: 1m 15s\n",
      "222:\tlearn: 0.2877654\ttotal: 54s\tremaining: 1m 15s\n",
      "223:\tlearn: 0.2877073\ttotal: 54.3s\tremaining: 1m 14s\n",
      "224:\tlearn: 0.2875738\ttotal: 54.5s\tremaining: 1m 14s\n",
      "225:\tlearn: 0.2874905\ttotal: 54.7s\tremaining: 1m 14s\n",
      "226:\tlearn: 0.2874514\ttotal: 55s\tremaining: 1m 14s\n",
      "227:\tlearn: 0.2873809\ttotal: 55.2s\tremaining: 1m 13s\n",
      "228:\tlearn: 0.2873196\ttotal: 55.4s\tremaining: 1m 13s\n",
      "229:\tlearn: 0.2872157\ttotal: 55.7s\tremaining: 1m 13s\n",
      "230:\tlearn: 0.2871222\ttotal: 55.9s\tremaining: 1m 13s\n",
      "231:\tlearn: 0.2870666\ttotal: 56.1s\tremaining: 1m 12s\n",
      "232:\tlearn: 0.2869829\ttotal: 56.4s\tremaining: 1m 12s\n",
      "233:\tlearn: 0.2867925\ttotal: 56.7s\tremaining: 1m 12s\n",
      "234:\tlearn: 0.2867392\ttotal: 56.9s\tremaining: 1m 12s\n",
      "235:\tlearn: 0.2866266\ttotal: 57.2s\tremaining: 1m 11s\n",
      "236:\tlearn: 0.2865851\ttotal: 57.4s\tremaining: 1m 11s\n",
      "237:\tlearn: 0.2864676\ttotal: 57.7s\tremaining: 1m 11s\n",
      "238:\tlearn: 0.2864050\ttotal: 57.9s\tremaining: 1m 11s\n",
      "239:\tlearn: 0.2863334\ttotal: 58.2s\tremaining: 1m 11s\n",
      "240:\tlearn: 0.2862511\ttotal: 58.4s\tremaining: 1m 10s\n",
      "241:\tlearn: 0.2861564\ttotal: 58.7s\tremaining: 1m 10s\n",
      "242:\tlearn: 0.2861045\ttotal: 58.9s\tremaining: 1m 10s\n",
      "243:\tlearn: 0.2860333\ttotal: 59.2s\tremaining: 1m 10s\n",
      "244:\tlearn: 0.2859863\ttotal: 59.4s\tremaining: 1m 9s\n",
      "245:\tlearn: 0.2859112\ttotal: 59.6s\tremaining: 1m 9s\n",
      "246:\tlearn: 0.2858781\ttotal: 59.8s\tremaining: 1m 9s\n",
      "247:\tlearn: 0.2857691\ttotal: 1m\tremaining: 1m 9s\n",
      "248:\tlearn: 0.2857060\ttotal: 1m\tremaining: 1m 8s\n",
      "249:\tlearn: 0.2856126\ttotal: 1m\tremaining: 1m 8s\n",
      "250:\tlearn: 0.2855893\ttotal: 1m\tremaining: 1m 8s\n",
      "251:\tlearn: 0.2855408\ttotal: 1m 1s\tremaining: 1m 8s\n",
      "252:\tlearn: 0.2855020\ttotal: 1m 1s\tremaining: 1m 7s\n",
      "253:\tlearn: 0.2853851\ttotal: 1m 1s\tremaining: 1m 7s\n",
      "254:\tlearn: 0.2853499\ttotal: 1m 1s\tremaining: 1m 7s\n",
      "255:\tlearn: 0.2852591\ttotal: 1m 1s\tremaining: 1m 7s\n",
      "256:\tlearn: 0.2851973\ttotal: 1m 2s\tremaining: 1m 6s\n",
      "257:\tlearn: 0.2851407\ttotal: 1m 2s\tremaining: 1m 6s\n",
      "258:\tlearn: 0.2851001\ttotal: 1m 2s\tremaining: 1m 6s\n",
      "259:\tlearn: 0.2850559\ttotal: 1m 2s\tremaining: 1m 6s\n",
      "260:\tlearn: 0.2850086\ttotal: 1m 3s\tremaining: 1m 5s\n",
      "261:\tlearn: 0.2848862\ttotal: 1m 3s\tremaining: 1m 5s\n",
      "262:\tlearn: 0.2848185\ttotal: 1m 3s\tremaining: 1m 5s\n",
      "263:\tlearn: 0.2847664\ttotal: 1m 3s\tremaining: 1m 5s\n",
      "264:\tlearn: 0.2847131\ttotal: 1m 4s\tremaining: 1m 4s\n",
      "265:\tlearn: 0.2846440\ttotal: 1m 4s\tremaining: 1m 4s\n",
      "266:\tlearn: 0.2845796\ttotal: 1m 4s\tremaining: 1m 4s\n",
      "267:\tlearn: 0.2844381\ttotal: 1m 4s\tremaining: 1m 4s\n",
      "268:\tlearn: 0.2844061\ttotal: 1m 5s\tremaining: 1m 3s\n",
      "269:\tlearn: 0.2843328\ttotal: 1m 5s\tremaining: 1m 3s\n",
      "270:\tlearn: 0.2842717\ttotal: 1m 5s\tremaining: 1m 3s\n",
      "271:\tlearn: 0.2841972\ttotal: 1m 5s\tremaining: 1m 3s\n",
      "272:\tlearn: 0.2841282\ttotal: 1m 6s\tremaining: 1m 2s\n",
      "273:\tlearn: 0.2840414\ttotal: 1m 6s\tremaining: 1m 2s\n",
      "274:\tlearn: 0.2839355\ttotal: 1m 6s\tremaining: 1m 2s\n",
      "275:\tlearn: 0.2838248\ttotal: 1m 6s\tremaining: 1m 2s\n",
      "276:\tlearn: 0.2837809\ttotal: 1m 7s\tremaining: 1m 1s\n",
      "277:\tlearn: 0.2836986\ttotal: 1m 7s\tremaining: 1m 1s\n",
      "278:\tlearn: 0.2836353\ttotal: 1m 7s\tremaining: 1m 1s\n",
      "279:\tlearn: 0.2835563\ttotal: 1m 7s\tremaining: 1m 1s\n",
      "280:\tlearn: 0.2835177\ttotal: 1m 8s\tremaining: 1m 1s\n",
      "281:\tlearn: 0.2834989\ttotal: 1m 8s\tremaining: 1m\n",
      "282:\tlearn: 0.2834525\ttotal: 1m 8s\tremaining: 1m\n",
      "283:\tlearn: 0.2833591\ttotal: 1m 8s\tremaining: 1m\n",
      "284:\tlearn: 0.2833370\ttotal: 1m 8s\tremaining: 1m\n",
      "285:\tlearn: 0.2832617\ttotal: 1m 9s\tremaining: 59.8s\n",
      "286:\tlearn: 0.2831874\ttotal: 1m 9s\tremaining: 59.6s\n",
      "287:\tlearn: 0.2831618\ttotal: 1m 9s\tremaining: 59.3s\n",
      "288:\tlearn: 0.2830533\ttotal: 1m 9s\tremaining: 59.1s\n",
      "289:\tlearn: 0.2829903\ttotal: 1m 10s\tremaining: 58.9s\n",
      "290:\tlearn: 0.2829380\ttotal: 1m 10s\tremaining: 58.6s\n",
      "291:\tlearn: 0.2828659\ttotal: 1m 10s\tremaining: 58.4s\n",
      "292:\tlearn: 0.2828444\ttotal: 1m 11s\tremaining: 58.2s\n",
      "293:\tlearn: 0.2827951\ttotal: 1m 11s\tremaining: 57.9s\n",
      "294:\tlearn: 0.2827051\ttotal: 1m 11s\tremaining: 57.7s\n",
      "295:\tlearn: 0.2826364\ttotal: 1m 11s\tremaining: 57.4s\n",
      "296:\tlearn: 0.2826030\ttotal: 1m 11s\tremaining: 57.2s\n",
      "297:\tlearn: 0.2825715\ttotal: 1m 12s\tremaining: 56.9s\n",
      "298:\tlearn: 0.2824777\ttotal: 1m 12s\tremaining: 56.7s\n",
      "299:\tlearn: 0.2824496\ttotal: 1m 12s\tremaining: 56.4s\n",
      "300:\tlearn: 0.2824176\ttotal: 1m 12s\tremaining: 56.2s\n",
      "301:\tlearn: 0.2823810\ttotal: 1m 13s\tremaining: 55.9s\n",
      "302:\tlearn: 0.2823469\ttotal: 1m 13s\tremaining: 55.7s\n",
      "303:\tlearn: 0.2822747\ttotal: 1m 13s\tremaining: 55.4s\n",
      "304:\tlearn: 0.2822318\ttotal: 1m 13s\tremaining: 55.2s\n",
      "305:\tlearn: 0.2821177\ttotal: 1m 14s\tremaining: 54.9s\n",
      "306:\tlearn: 0.2820474\ttotal: 1m 14s\tremaining: 54.7s\n",
      "307:\tlearn: 0.2819154\ttotal: 1m 14s\tremaining: 54.4s\n",
      "308:\tlearn: 0.2818273\ttotal: 1m 14s\tremaining: 54.2s\n",
      "309:\tlearn: 0.2817972\ttotal: 1m 14s\tremaining: 53.9s\n",
      "310:\tlearn: 0.2816846\ttotal: 1m 15s\tremaining: 53.7s\n",
      "311:\tlearn: 0.2816322\ttotal: 1m 15s\tremaining: 53.4s\n",
      "312:\tlearn: 0.2815584\ttotal: 1m 15s\tremaining: 53.2s\n",
      "313:\tlearn: 0.2815195\ttotal: 1m 15s\tremaining: 52.9s\n",
      "314:\tlearn: 0.2814512\ttotal: 1m 16s\tremaining: 52.7s\n",
      "315:\tlearn: 0.2813475\ttotal: 1m 16s\tremaining: 52.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316:\tlearn: 0.2812741\ttotal: 1m 16s\tremaining: 52.2s\n",
      "317:\tlearn: 0.2812404\ttotal: 1m 16s\tremaining: 51.9s\n",
      "318:\tlearn: 0.2811746\ttotal: 1m 17s\tremaining: 51.7s\n",
      "319:\tlearn: 0.2811584\ttotal: 1m 17s\tremaining: 51.4s\n",
      "320:\tlearn: 0.2810840\ttotal: 1m 17s\tremaining: 51.2s\n",
      "321:\tlearn: 0.2809341\ttotal: 1m 17s\tremaining: 50.9s\n",
      "322:\tlearn: 0.2809024\ttotal: 1m 17s\tremaining: 50.7s\n",
      "323:\tlearn: 0.2808517\ttotal: 1m 18s\tremaining: 50.4s\n",
      "324:\tlearn: 0.2807612\ttotal: 1m 18s\tremaining: 50.2s\n",
      "325:\tlearn: 0.2807143\ttotal: 1m 18s\tremaining: 50s\n",
      "326:\tlearn: 0.2806006\ttotal: 1m 18s\tremaining: 49.7s\n",
      "327:\tlearn: 0.2805619\ttotal: 1m 19s\tremaining: 49.5s\n",
      "328:\tlearn: 0.2804991\ttotal: 1m 19s\tremaining: 49.2s\n",
      "329:\tlearn: 0.2804079\ttotal: 1m 19s\tremaining: 49s\n",
      "330:\tlearn: 0.2803788\ttotal: 1m 19s\tremaining: 48.8s\n",
      "331:\tlearn: 0.2803352\ttotal: 1m 20s\tremaining: 48.5s\n",
      "332:\tlearn: 0.2803033\ttotal: 1m 20s\tremaining: 48.3s\n",
      "333:\tlearn: 0.2802144\ttotal: 1m 20s\tremaining: 48.1s\n",
      "334:\tlearn: 0.2801893\ttotal: 1m 20s\tremaining: 47.8s\n",
      "335:\tlearn: 0.2801181\ttotal: 1m 21s\tremaining: 47.6s\n",
      "336:\tlearn: 0.2800948\ttotal: 1m 21s\tremaining: 47.3s\n",
      "337:\tlearn: 0.2800624\ttotal: 1m 21s\tremaining: 47.1s\n",
      "338:\tlearn: 0.2799462\ttotal: 1m 21s\tremaining: 46.9s\n",
      "339:\tlearn: 0.2799037\ttotal: 1m 22s\tremaining: 46.6s\n",
      "340:\tlearn: 0.2798497\ttotal: 1m 22s\tremaining: 46.4s\n",
      "341:\tlearn: 0.2797462\ttotal: 1m 22s\tremaining: 46.2s\n",
      "342:\tlearn: 0.2796812\ttotal: 1m 22s\tremaining: 45.9s\n",
      "343:\tlearn: 0.2796288\ttotal: 1m 23s\tremaining: 45.7s\n",
      "344:\tlearn: 0.2795786\ttotal: 1m 23s\tremaining: 45.4s\n",
      "345:\tlearn: 0.2795263\ttotal: 1m 23s\tremaining: 45.2s\n",
      "346:\tlearn: 0.2794310\ttotal: 1m 23s\tremaining: 45s\n",
      "347:\tlearn: 0.2793762\ttotal: 1m 24s\tremaining: 44.7s\n",
      "348:\tlearn: 0.2792919\ttotal: 1m 24s\tremaining: 44.5s\n",
      "349:\tlearn: 0.2792756\ttotal: 1m 24s\tremaining: 44.3s\n",
      "350:\tlearn: 0.2792379\ttotal: 1m 24s\tremaining: 44s\n",
      "351:\tlearn: 0.2791711\ttotal: 1m 25s\tremaining: 43.8s\n",
      "352:\tlearn: 0.2790996\ttotal: 1m 25s\tremaining: 43.6s\n",
      "353:\tlearn: 0.2790318\ttotal: 1m 25s\tremaining: 43.4s\n",
      "354:\tlearn: 0.2789788\ttotal: 1m 25s\tremaining: 43.1s\n",
      "355:\tlearn: 0.2789433\ttotal: 1m 26s\tremaining: 42.9s\n",
      "356:\tlearn: 0.2789270\ttotal: 1m 26s\tremaining: 42.6s\n",
      "357:\tlearn: 0.2788889\ttotal: 1m 26s\tremaining: 42.4s\n",
      "358:\tlearn: 0.2788750\ttotal: 1m 26s\tremaining: 42.1s\n",
      "359:\tlearn: 0.2788191\ttotal: 1m 27s\tremaining: 41.9s\n",
      "360:\tlearn: 0.2787747\ttotal: 1m 27s\tremaining: 41.7s\n",
      "361:\tlearn: 0.2787260\ttotal: 1m 27s\tremaining: 41.4s\n",
      "362:\tlearn: 0.2786860\ttotal: 1m 27s\tremaining: 41.2s\n",
      "363:\tlearn: 0.2785754\ttotal: 1m 28s\tremaining: 40.9s\n",
      "364:\tlearn: 0.2784754\ttotal: 1m 28s\tremaining: 40.7s\n",
      "365:\tlearn: 0.2784218\ttotal: 1m 28s\tremaining: 40.4s\n",
      "366:\tlearn: 0.2783432\ttotal: 1m 28s\tremaining: 40.2s\n",
      "367:\tlearn: 0.2783146\ttotal: 1m 29s\tremaining: 39.9s\n",
      "368:\tlearn: 0.2782373\ttotal: 1m 29s\tremaining: 39.7s\n",
      "369:\tlearn: 0.2782036\ttotal: 1m 29s\tremaining: 39.4s\n",
      "370:\tlearn: 0.2781340\ttotal: 1m 29s\tremaining: 39.2s\n",
      "371:\tlearn: 0.2780677\ttotal: 1m 29s\tremaining: 38.9s\n",
      "372:\tlearn: 0.2780445\ttotal: 1m 30s\tremaining: 38.7s\n",
      "373:\tlearn: 0.2779953\ttotal: 1m 30s\tremaining: 38.5s\n",
      "374:\tlearn: 0.2779593\ttotal: 1m 30s\tremaining: 38.2s\n",
      "375:\tlearn: 0.2779170\ttotal: 1m 30s\tremaining: 38s\n",
      "376:\tlearn: 0.2778792\ttotal: 1m 31s\tremaining: 37.7s\n",
      "377:\tlearn: 0.2778489\ttotal: 1m 31s\tremaining: 37.5s\n",
      "378:\tlearn: 0.2777785\ttotal: 1m 31s\tremaining: 37.2s\n",
      "379:\tlearn: 0.2777268\ttotal: 1m 31s\tremaining: 37s\n",
      "380:\tlearn: 0.2776710\ttotal: 1m 32s\tremaining: 36.7s\n",
      "381:\tlearn: 0.2776296\ttotal: 1m 32s\tremaining: 36.5s\n",
      "382:\tlearn: 0.2775493\ttotal: 1m 32s\tremaining: 36.2s\n",
      "383:\tlearn: 0.2775016\ttotal: 1m 32s\tremaining: 36s\n",
      "384:\tlearn: 0.2774350\ttotal: 1m 33s\tremaining: 35.8s\n",
      "385:\tlearn: 0.2773504\ttotal: 1m 33s\tremaining: 35.5s\n",
      "386:\tlearn: 0.2773039\ttotal: 1m 33s\tremaining: 35.3s\n",
      "387:\tlearn: 0.2772466\ttotal: 1m 33s\tremaining: 35s\n",
      "388:\tlearn: 0.2772201\ttotal: 1m 33s\tremaining: 34.8s\n",
      "389:\tlearn: 0.2771708\ttotal: 1m 34s\tremaining: 34.5s\n",
      "390:\tlearn: 0.2770916\ttotal: 1m 34s\tremaining: 34.3s\n",
      "391:\tlearn: 0.2769513\ttotal: 1m 34s\tremaining: 34s\n",
      "392:\tlearn: 0.2768968\ttotal: 1m 34s\tremaining: 33.8s\n",
      "393:\tlearn: 0.2767945\ttotal: 1m 35s\tremaining: 33.6s\n",
      "394:\tlearn: 0.2767605\ttotal: 1m 35s\tremaining: 33.3s\n",
      "395:\tlearn: 0.2766671\ttotal: 1m 35s\tremaining: 33.1s\n",
      "396:\tlearn: 0.2766268\ttotal: 1m 35s\tremaining: 32.8s\n",
      "397:\tlearn: 0.2765120\ttotal: 1m 36s\tremaining: 32.6s\n",
      "398:\tlearn: 0.2764706\ttotal: 1m 36s\tremaining: 32.3s\n",
      "399:\tlearn: 0.2763877\ttotal: 1m 36s\tremaining: 32.1s\n",
      "400:\tlearn: 0.2763424\ttotal: 1m 36s\tremaining: 31.9s\n",
      "401:\tlearn: 0.2762851\ttotal: 1m 37s\tremaining: 31.6s\n",
      "402:\tlearn: 0.2762223\ttotal: 1m 37s\tremaining: 31.4s\n",
      "403:\tlearn: 0.2761722\ttotal: 1m 37s\tremaining: 31.1s\n",
      "404:\tlearn: 0.2760851\ttotal: 1m 37s\tremaining: 30.9s\n",
      "405:\tlearn: 0.2760537\ttotal: 1m 38s\tremaining: 30.7s\n",
      "406:\tlearn: 0.2760184\ttotal: 1m 38s\tremaining: 30.4s\n",
      "407:\tlearn: 0.2759748\ttotal: 1m 38s\tremaining: 30.2s\n",
      "408:\tlearn: 0.2758852\ttotal: 1m 38s\tremaining: 30s\n",
      "409:\tlearn: 0.2758251\ttotal: 1m 39s\tremaining: 29.7s\n",
      "410:\tlearn: 0.2757866\ttotal: 1m 39s\tremaining: 29.5s\n",
      "411:\tlearn: 0.2757478\ttotal: 1m 39s\tremaining: 29.2s\n",
      "412:\tlearn: 0.2756992\ttotal: 1m 39s\tremaining: 29s\n",
      "413:\tlearn: 0.2756701\ttotal: 1m 39s\tremaining: 28.7s\n",
      "414:\tlearn: 0.2755899\ttotal: 1m 40s\tremaining: 28.5s\n",
      "415:\tlearn: 0.2755305\ttotal: 1m 40s\tremaining: 28.3s\n",
      "416:\tlearn: 0.2754899\ttotal: 1m 40s\tremaining: 28s\n",
      "417:\tlearn: 0.2754519\ttotal: 1m 40s\tremaining: 27.8s\n",
      "418:\tlearn: 0.2754043\ttotal: 1m 41s\tremaining: 27.5s\n",
      "419:\tlearn: 0.2753412\ttotal: 1m 41s\tremaining: 27.3s\n",
      "420:\tlearn: 0.2752455\ttotal: 1m 41s\tremaining: 27s\n",
      "421:\tlearn: 0.2752259\ttotal: 1m 41s\tremaining: 26.8s\n",
      "422:\tlearn: 0.2751813\ttotal: 1m 42s\tremaining: 26.6s\n",
      "423:\tlearn: 0.2751557\ttotal: 1m 42s\tremaining: 26.3s\n",
      "424:\tlearn: 0.2751182\ttotal: 1m 42s\tremaining: 26.1s\n",
      "425:\tlearn: 0.2750790\ttotal: 1m 42s\tremaining: 25.9s\n",
      "426:\tlearn: 0.2750539\ttotal: 1m 43s\tremaining: 25.6s\n",
      "427:\tlearn: 0.2750076\ttotal: 1m 43s\tremaining: 25.4s\n",
      "428:\tlearn: 0.2749739\ttotal: 1m 43s\tremaining: 25.1s\n",
      "429:\tlearn: 0.2749195\ttotal: 1m 43s\tremaining: 24.9s\n",
      "430:\tlearn: 0.2748847\ttotal: 1m 44s\tremaining: 24.7s\n",
      "431:\tlearn: 0.2748185\ttotal: 1m 44s\tremaining: 24.4s\n",
      "432:\tlearn: 0.2747510\ttotal: 1m 44s\tremaining: 24.2s\n",
      "433:\tlearn: 0.2746765\ttotal: 1m 44s\tremaining: 23.9s\n",
      "434:\tlearn: 0.2746532\ttotal: 1m 45s\tremaining: 23.7s\n",
      "435:\tlearn: 0.2746154\ttotal: 1m 45s\tremaining: 23.4s\n",
      "436:\tlearn: 0.2745795\ttotal: 1m 45s\tremaining: 23.2s\n",
      "437:\tlearn: 0.2745557\ttotal: 1m 45s\tremaining: 23s\n",
      "438:\tlearn: 0.2745245\ttotal: 1m 46s\tremaining: 22.7s\n",
      "439:\tlearn: 0.2744788\ttotal: 1m 46s\tremaining: 22.5s\n",
      "440:\tlearn: 0.2744163\ttotal: 1m 46s\tremaining: 22.2s\n",
      "441:\tlearn: 0.2743760\ttotal: 1m 46s\tremaining: 22s\n",
      "442:\tlearn: 0.2743309\ttotal: 1m 47s\tremaining: 21.8s\n",
      "443:\tlearn: 0.2742982\ttotal: 1m 47s\tremaining: 21.5s\n",
      "444:\tlearn: 0.2742605\ttotal: 1m 47s\tremaining: 21.3s\n",
      "445:\tlearn: 0.2742037\ttotal: 1m 47s\tremaining: 21s\n",
      "446:\tlearn: 0.2741479\ttotal: 1m 48s\tremaining: 20.8s\n",
      "447:\tlearn: 0.2741204\ttotal: 1m 48s\tremaining: 20.6s\n",
      "448:\tlearn: 0.2740978\ttotal: 1m 48s\tremaining: 20.3s\n",
      "449:\tlearn: 0.2739998\ttotal: 1m 48s\tremaining: 20.1s\n",
      "450:\tlearn: 0.2739433\ttotal: 1m 49s\tremaining: 19.8s\n",
      "451:\tlearn: 0.2738607\ttotal: 1m 49s\tremaining: 19.6s\n",
      "452:\tlearn: 0.2738363\ttotal: 1m 49s\tremaining: 19.3s\n",
      "453:\tlearn: 0.2738114\ttotal: 1m 49s\tremaining: 19.1s\n",
      "454:\tlearn: 0.2737423\ttotal: 1m 50s\tremaining: 18.9s\n",
      "455:\tlearn: 0.2736649\ttotal: 1m 50s\tremaining: 18.6s\n",
      "456:\tlearn: 0.2736078\ttotal: 1m 50s\tremaining: 18.4s\n",
      "457:\tlearn: 0.2735797\ttotal: 1m 50s\tremaining: 18.1s\n",
      "458:\tlearn: 0.2735418\ttotal: 1m 51s\tremaining: 17.9s\n",
      "459:\tlearn: 0.2734829\ttotal: 1m 51s\tremaining: 17.7s\n",
      "460:\tlearn: 0.2734444\ttotal: 1m 51s\tremaining: 17.4s\n",
      "461:\tlearn: 0.2734148\ttotal: 1m 51s\tremaining: 17.2s\n",
      "462:\tlearn: 0.2733904\ttotal: 1m 52s\tremaining: 16.9s\n",
      "463:\tlearn: 0.2733430\ttotal: 1m 52s\tremaining: 16.7s\n",
      "464:\tlearn: 0.2733065\ttotal: 1m 52s\tremaining: 16.4s\n",
      "465:\tlearn: 0.2732694\ttotal: 1m 52s\tremaining: 16.2s\n",
      "466:\tlearn: 0.2731752\ttotal: 1m 52s\tremaining: 16s\n",
      "467:\tlearn: 0.2731533\ttotal: 1m 53s\tremaining: 15.7s\n",
      "468:\tlearn: 0.2731297\ttotal: 1m 53s\tremaining: 15.5s\n",
      "469:\tlearn: 0.2730909\ttotal: 1m 53s\tremaining: 15.2s\n",
      "470:\tlearn: 0.2730777\ttotal: 1m 53s\tremaining: 15s\n",
      "471:\tlearn: 0.2730007\ttotal: 1m 54s\tremaining: 14.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472:\tlearn: 0.2729631\ttotal: 1m 54s\tremaining: 14.5s\n",
      "473:\tlearn: 0.2729234\ttotal: 1m 54s\tremaining: 14.3s\n",
      "474:\tlearn: 0.2728889\ttotal: 1m 55s\tremaining: 14s\n",
      "475:\tlearn: 0.2728495\ttotal: 1m 55s\tremaining: 13.8s\n",
      "476:\tlearn: 0.2727749\ttotal: 1m 55s\tremaining: 13.6s\n",
      "477:\tlearn: 0.2727496\ttotal: 1m 55s\tremaining: 13.3s\n",
      "478:\tlearn: 0.2727095\ttotal: 1m 56s\tremaining: 13.1s\n",
      "479:\tlearn: 0.2726900\ttotal: 1m 56s\tremaining: 12.8s\n",
      "480:\tlearn: 0.2726505\ttotal: 1m 56s\tremaining: 12.6s\n",
      "481:\tlearn: 0.2726201\ttotal: 1m 56s\tremaining: 12.4s\n",
      "482:\tlearn: 0.2725686\ttotal: 1m 57s\tremaining: 12.1s\n",
      "483:\tlearn: 0.2725375\ttotal: 1m 57s\tremaining: 11.9s\n",
      "484:\tlearn: 0.2725018\ttotal: 1m 57s\tremaining: 11.6s\n",
      "485:\tlearn: 0.2724447\ttotal: 1m 57s\tremaining: 11.4s\n",
      "486:\tlearn: 0.2724039\ttotal: 1m 58s\tremaining: 11.1s\n",
      "487:\tlearn: 0.2723599\ttotal: 1m 58s\tremaining: 10.9s\n",
      "488:\tlearn: 0.2723226\ttotal: 1m 58s\tremaining: 10.7s\n",
      "489:\tlearn: 0.2722352\ttotal: 1m 58s\tremaining: 10.4s\n",
      "490:\tlearn: 0.2721589\ttotal: 1m 59s\tremaining: 10.2s\n",
      "491:\tlearn: 0.2721166\ttotal: 1m 59s\tremaining: 9.94s\n",
      "492:\tlearn: 0.2720815\ttotal: 1m 59s\tremaining: 9.7s\n",
      "493:\tlearn: 0.2720243\ttotal: 1m 59s\tremaining: 9.46s\n",
      "494:\tlearn: 0.2719350\ttotal: 2m\tremaining: 9.21s\n",
      "495:\tlearn: 0.2719033\ttotal: 2m\tremaining: 8.97s\n",
      "496:\tlearn: 0.2718519\ttotal: 2m\tremaining: 8.73s\n",
      "497:\tlearn: 0.2718160\ttotal: 2m\tremaining: 8.49s\n",
      "498:\tlearn: 0.2716915\ttotal: 2m 1s\tremaining: 8.25s\n",
      "499:\tlearn: 0.2716348\ttotal: 2m 1s\tremaining: 8.01s\n",
      "500:\tlearn: 0.2715766\ttotal: 2m 1s\tremaining: 7.76s\n",
      "501:\tlearn: 0.2715428\ttotal: 2m 1s\tremaining: 7.52s\n",
      "502:\tlearn: 0.2715119\ttotal: 2m 2s\tremaining: 7.28s\n",
      "503:\tlearn: 0.2714732\ttotal: 2m 2s\tremaining: 7.04s\n",
      "504:\tlearn: 0.2714512\ttotal: 2m 2s\tremaining: 6.79s\n",
      "505:\tlearn: 0.2714260\ttotal: 2m 2s\tremaining: 6.55s\n",
      "506:\tlearn: 0.2713728\ttotal: 2m 2s\tremaining: 6.31s\n",
      "507:\tlearn: 0.2712985\ttotal: 2m 3s\tremaining: 6.06s\n",
      "508:\tlearn: 0.2712629\ttotal: 2m 3s\tremaining: 5.82s\n",
      "509:\tlearn: 0.2711385\ttotal: 2m 3s\tremaining: 5.58s\n",
      "510:\tlearn: 0.2711172\ttotal: 2m 3s\tremaining: 5.34s\n",
      "511:\tlearn: 0.2710787\ttotal: 2m 4s\tremaining: 5.09s\n",
      "512:\tlearn: 0.2710251\ttotal: 2m 4s\tremaining: 4.85s\n",
      "513:\tlearn: 0.2709747\ttotal: 2m 4s\tremaining: 4.61s\n",
      "514:\tlearn: 0.2709386\ttotal: 2m 4s\tremaining: 4.37s\n",
      "515:\tlearn: 0.2709053\ttotal: 2m 5s\tremaining: 4.12s\n",
      "516:\tlearn: 0.2708818\ttotal: 2m 5s\tremaining: 3.88s\n",
      "517:\tlearn: 0.2708089\ttotal: 2m 5s\tremaining: 3.64s\n",
      "518:\tlearn: 0.2707347\ttotal: 2m 5s\tremaining: 3.4s\n",
      "519:\tlearn: 0.2706886\ttotal: 2m 6s\tremaining: 3.15s\n",
      "520:\tlearn: 0.2706320\ttotal: 2m 6s\tremaining: 2.91s\n",
      "521:\tlearn: 0.2705958\ttotal: 2m 6s\tremaining: 2.67s\n",
      "522:\tlearn: 0.2705690\ttotal: 2m 6s\tremaining: 2.43s\n",
      "523:\tlearn: 0.2705047\ttotal: 2m 7s\tremaining: 2.18s\n",
      "524:\tlearn: 0.2704233\ttotal: 2m 7s\tremaining: 1.94s\n",
      "525:\tlearn: 0.2703879\ttotal: 2m 7s\tremaining: 1.7s\n",
      "526:\tlearn: 0.2703387\ttotal: 2m 7s\tremaining: 1.46s\n",
      "527:\tlearn: 0.2703179\ttotal: 2m 8s\tremaining: 1.21s\n",
      "528:\tlearn: 0.2702681\ttotal: 2m 8s\tremaining: 971ms\n",
      "529:\tlearn: 0.2702095\ttotal: 2m 8s\tremaining: 728ms\n",
      "530:\tlearn: 0.2701731\ttotal: 2m 8s\tremaining: 485ms\n",
      "531:\tlearn: 0.2701069\ttotal: 2m 9s\tremaining: 243ms\n",
      "532:\tlearn: 0.2700104\ttotal: 2m 9s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core._CatBoostBase at 0x2ab059e8>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat = CatBoostClassifier(**params_cat)\n",
    "\n",
    "cat.fit(x_train__, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'border_count': 32,\n",
       " 'depth': 7,\n",
       " 'iterations': 533,\n",
       " 'l2_leaf_reg': 100,\n",
       " 'learning_rate': 0.1,\n",
       " 'loss_function': 'Logloss',\n",
       " 'od_type': 'Iter'}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_test__ = x_test_.fillna(-999)\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission['ID_CORRELATIVO'] = x_test__.index\n",
    "submission['ATTRITION'] = cat.predict_proba(x_test__)[:,1]\n",
    "submission.to_csv('./data/submission14_CAT.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelado requerimientos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_req = df_train_req.join(y_train).drop('ATTRITION', axis=1)\n",
    "y_req = df_train_req.join(y_train)['ATTRITION']\n",
    "x_req.fillna(-999, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_req, x_test_req, y_train_req, y_test_req = train_test_split(x_req, y_req, test_size=.10, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6044600\ttest: 0.6031466\tbest: 0.6031466 (0)\ttotal: 113ms\tremaining: 11.2s\n",
      "1:\tlearn: 0.5360142\ttest: 0.5335257\tbest: 0.5335257 (1)\ttotal: 154ms\tremaining: 7.53s\n",
      "2:\tlearn: 0.4833144\ttest: 0.4797636\tbest: 0.4797636 (2)\ttotal: 211ms\tremaining: 6.82s\n",
      "3:\tlearn: 0.4425946\ttest: 0.4381694\tbest: 0.4381694 (3)\ttotal: 287ms\tremaining: 6.89s\n",
      "4:\tlearn: 0.4113655\ttest: 0.4060800\tbest: 0.4060800 (4)\ttotal: 328ms\tremaining: 6.22s\n",
      "5:\tlearn: 0.3870200\ttest: 0.3810510\tbest: 0.3810510 (5)\ttotal: 455ms\tremaining: 7.12s\n",
      "6:\tlearn: 0.3683349\ttest: 0.3616980\tbest: 0.3616980 (6)\ttotal: 545ms\tremaining: 7.24s\n",
      "7:\tlearn: 0.3538443\ttest: 0.3465183\tbest: 0.3465183 (7)\ttotal: 621ms\tremaining: 7.14s\n",
      "8:\tlearn: 0.3425319\ttest: 0.3346711\tbest: 0.3346711 (8)\ttotal: 746ms\tremaining: 7.54s\n",
      "9:\tlearn: 0.3337013\ttest: 0.3253680\tbest: 0.3253680 (9)\ttotal: 811ms\tremaining: 7.3s\n",
      "10:\tlearn: 0.3265408\ttest: 0.3178328\tbest: 0.3178328 (10)\ttotal: 898ms\tremaining: 7.26s\n",
      "11:\tlearn: 0.3211377\ttest: 0.3120240\tbest: 0.3120240 (11)\ttotal: 940ms\tremaining: 6.89s\n",
      "12:\tlearn: 0.3168793\ttest: 0.3074006\tbest: 0.3074006 (12)\ttotal: 1s\tremaining: 6.73s\n",
      "13:\tlearn: 0.3135073\ttest: 0.3036986\tbest: 0.3036986 (13)\ttotal: 1.06s\tremaining: 6.53s\n",
      "14:\tlearn: 0.3108425\ttest: 0.3007379\tbest: 0.3007379 (14)\ttotal: 1.1s\tremaining: 6.25s\n",
      "15:\tlearn: 0.3087274\ttest: 0.2983564\tbest: 0.2983564 (15)\ttotal: 1.15s\tremaining: 6.02s\n",
      "16:\tlearn: 0.3069971\ttest: 0.2964117\tbest: 0.2964117 (16)\ttotal: 1.23s\tremaining: 5.99s\n",
      "17:\tlearn: 0.3055970\ttest: 0.2948086\tbest: 0.2948086 (17)\ttotal: 1.36s\tremaining: 6.21s\n",
      "18:\tlearn: 0.3045175\ttest: 0.2935270\tbest: 0.2935270 (18)\ttotal: 1.45s\tremaining: 6.2s\n",
      "19:\tlearn: 0.3035652\ttest: 0.2924651\tbest: 0.2924651 (19)\ttotal: 1.54s\tremaining: 6.16s\n",
      "20:\tlearn: 0.3026691\ttest: 0.2915082\tbest: 0.2915082 (20)\ttotal: 1.63s\tremaining: 6.15s\n",
      "21:\tlearn: 0.3020542\ttest: 0.2907527\tbest: 0.2907527 (21)\ttotal: 1.68s\tremaining: 5.97s\n",
      "22:\tlearn: 0.3014800\ttest: 0.2901059\tbest: 0.2901059 (22)\ttotal: 1.75s\tremaining: 5.84s\n",
      "23:\tlearn: 0.3009963\ttest: 0.2895892\tbest: 0.2895892 (23)\ttotal: 1.86s\tremaining: 5.88s\n",
      "24:\tlearn: 0.3007164\ttest: 0.2891881\tbest: 0.2891881 (24)\ttotal: 1.94s\tremaining: 5.81s\n",
      "25:\tlearn: 0.3003997\ttest: 0.2888390\tbest: 0.2888390 (25)\ttotal: 2.04s\tremaining: 5.82s\n",
      "26:\tlearn: 0.3002221\ttest: 0.2885744\tbest: 0.2885744 (26)\ttotal: 2.13s\tremaining: 5.76s\n",
      "27:\tlearn: 0.2999729\ttest: 0.2883119\tbest: 0.2883119 (27)\ttotal: 2.24s\tremaining: 5.76s\n",
      "28:\tlearn: 0.2998590\ttest: 0.2881292\tbest: 0.2881292 (28)\ttotal: 2.28s\tremaining: 5.59s\n",
      "29:\tlearn: 0.2996869\ttest: 0.2879386\tbest: 0.2879386 (29)\ttotal: 2.37s\tremaining: 5.52s\n",
      "30:\tlearn: 0.2995809\ttest: 0.2877867\tbest: 0.2877867 (30)\ttotal: 2.44s\tremaining: 5.42s\n",
      "31:\tlearn: 0.2994967\ttest: 0.2876583\tbest: 0.2876583 (31)\ttotal: 2.49s\tremaining: 5.29s\n",
      "32:\tlearn: 0.2993930\ttest: 0.2875513\tbest: 0.2875513 (32)\ttotal: 2.61s\tremaining: 5.3s\n",
      "33:\tlearn: 0.2993402\ttest: 0.2874597\tbest: 0.2874597 (33)\ttotal: 2.67s\tremaining: 5.17s\n",
      "34:\tlearn: 0.2993095\ttest: 0.2873935\tbest: 0.2873935 (34)\ttotal: 2.71s\tremaining: 5.03s\n",
      "35:\tlearn: 0.2992847\ttest: 0.2873367\tbest: 0.2873367 (35)\ttotal: 2.75s\tremaining: 4.89s\n",
      "36:\tlearn: 0.2991330\ttest: 0.2872043\tbest: 0.2872043 (36)\ttotal: 2.87s\tremaining: 4.89s\n",
      "37:\tlearn: 0.2990649\ttest: 0.2871168\tbest: 0.2871168 (37)\ttotal: 2.94s\tremaining: 4.8s\n",
      "38:\tlearn: 0.2989767\ttest: 0.2870185\tbest: 0.2870185 (38)\ttotal: 3.02s\tremaining: 4.72s\n",
      "39:\tlearn: 0.2988655\ttest: 0.2869326\tbest: 0.2869326 (39)\ttotal: 3.14s\tremaining: 4.71s\n",
      "40:\tlearn: 0.2988280\ttest: 0.2869043\tbest: 0.2869043 (40)\ttotal: 3.21s\tremaining: 4.62s\n",
      "41:\tlearn: 0.2987653\ttest: 0.2868726\tbest: 0.2868726 (41)\ttotal: 3.3s\tremaining: 4.55s\n",
      "42:\tlearn: 0.2987342\ttest: 0.2868340\tbest: 0.2868340 (42)\ttotal: 3.36s\tremaining: 4.45s\n",
      "43:\tlearn: 0.2986831\ttest: 0.2867934\tbest: 0.2867934 (43)\ttotal: 3.48s\tremaining: 4.43s\n",
      "44:\tlearn: 0.2986401\ttest: 0.2867725\tbest: 0.2867725 (44)\ttotal: 3.58s\tremaining: 4.37s\n",
      "45:\tlearn: 0.2984960\ttest: 0.2866086\tbest: 0.2866086 (45)\ttotal: 3.73s\tremaining: 4.38s\n",
      "46:\tlearn: 0.2984678\ttest: 0.2865880\tbest: 0.2865880 (46)\ttotal: 3.82s\tremaining: 4.31s\n",
      "47:\tlearn: 0.2984174\ttest: 0.2865563\tbest: 0.2865563 (47)\ttotal: 3.91s\tremaining: 4.24s\n",
      "48:\tlearn: 0.2983790\ttest: 0.2865104\tbest: 0.2865104 (48)\ttotal: 4.03s\tremaining: 4.19s\n",
      "49:\tlearn: 0.2983004\ttest: 0.2864583\tbest: 0.2864583 (49)\ttotal: 4.14s\tremaining: 4.14s\n",
      "50:\tlearn: 0.2982773\ttest: 0.2864504\tbest: 0.2864504 (50)\ttotal: 4.24s\tremaining: 4.08s\n",
      "51:\tlearn: 0.2982282\ttest: 0.2863891\tbest: 0.2863891 (51)\ttotal: 4.33s\tremaining: 4s\n",
      "52:\tlearn: 0.2982175\ttest: 0.2863897\tbest: 0.2863891 (51)\ttotal: 4.38s\tremaining: 3.88s\n",
      "53:\tlearn: 0.2982154\ttest: 0.2863855\tbest: 0.2863855 (53)\ttotal: 4.43s\tremaining: 3.78s\n",
      "54:\tlearn: 0.2981542\ttest: 0.2863684\tbest: 0.2863684 (54)\ttotal: 4.56s\tremaining: 3.73s\n",
      "55:\tlearn: 0.2980913\ttest: 0.2863178\tbest: 0.2863178 (55)\ttotal: 4.68s\tremaining: 3.68s\n",
      "56:\tlearn: 0.2980344\ttest: 0.2863206\tbest: 0.2863178 (55)\ttotal: 4.79s\tremaining: 3.61s\n",
      "57:\tlearn: 0.2980343\ttest: 0.2863191\tbest: 0.2863178 (55)\ttotal: 4.85s\tremaining: 3.51s\n",
      "58:\tlearn: 0.2980342\ttest: 0.2863171\tbest: 0.2863171 (58)\ttotal: 4.9s\tremaining: 3.41s\n",
      "59:\tlearn: 0.2980296\ttest: 0.2863124\tbest: 0.2863124 (59)\ttotal: 4.99s\tremaining: 3.33s\n",
      "60:\tlearn: 0.2980295\ttest: 0.2863115\tbest: 0.2863115 (60)\ttotal: 5.05s\tremaining: 3.23s\n",
      "61:\tlearn: 0.2980215\ttest: 0.2862934\tbest: 0.2862934 (61)\ttotal: 5.13s\tremaining: 3.14s\n",
      "62:\tlearn: 0.2979809\ttest: 0.2862655\tbest: 0.2862655 (62)\ttotal: 5.24s\tremaining: 3.08s\n",
      "63:\tlearn: 0.2979110\ttest: 0.2862398\tbest: 0.2862398 (63)\ttotal: 5.36s\tremaining: 3.02s\n",
      "64:\tlearn: 0.2979023\ttest: 0.2862384\tbest: 0.2862384 (64)\ttotal: 5.49s\tremaining: 2.96s\n",
      "65:\tlearn: 0.2978627\ttest: 0.2862413\tbest: 0.2862384 (64)\ttotal: 5.6s\tremaining: 2.88s\n",
      "66:\tlearn: 0.2978601\ttest: 0.2862382\tbest: 0.2862382 (66)\ttotal: 5.65s\tremaining: 2.78s\n",
      "67:\tlearn: 0.2978600\ttest: 0.2862377\tbest: 0.2862377 (67)\ttotal: 5.7s\tremaining: 2.68s\n",
      "68:\tlearn: 0.2978522\ttest: 0.2862283\tbest: 0.2862283 (68)\ttotal: 5.8s\tremaining: 2.61s\n",
      "69:\tlearn: 0.2978323\ttest: 0.2862416\tbest: 0.2862283 (68)\ttotal: 5.97s\tremaining: 2.56s\n",
      "70:\tlearn: 0.2978314\ttest: 0.2862411\tbest: 0.2862283 (68)\ttotal: 6.05s\tremaining: 2.47s\n",
      "71:\tlearn: 0.2978036\ttest: 0.2862380\tbest: 0.2862283 (68)\ttotal: 6.18s\tremaining: 2.4s\n",
      "72:\tlearn: 0.2977970\ttest: 0.2862299\tbest: 0.2862283 (68)\ttotal: 6.25s\tremaining: 2.31s\n",
      "73:\tlearn: 0.2977605\ttest: 0.2862185\tbest: 0.2862185 (73)\ttotal: 6.37s\tremaining: 2.24s\n",
      "74:\tlearn: 0.2977605\ttest: 0.2862185\tbest: 0.2862185 (74)\ttotal: 6.42s\tremaining: 2.14s\n",
      "75:\tlearn: 0.2977145\ttest: 0.2861931\tbest: 0.2861931 (75)\ttotal: 6.55s\tremaining: 2.07s\n",
      "76:\tlearn: 0.2976996\ttest: 0.2861991\tbest: 0.2861931 (75)\ttotal: 6.67s\tremaining: 1.99s\n",
      "77:\tlearn: 0.2976633\ttest: 0.2861673\tbest: 0.2861673 (77)\ttotal: 6.78s\tremaining: 1.91s\n",
      "78:\tlearn: 0.2976272\ttest: 0.2861437\tbest: 0.2861437 (78)\ttotal: 6.89s\tremaining: 1.83s\n",
      "79:\tlearn: 0.2976235\ttest: 0.2861500\tbest: 0.2861437 (78)\ttotal: 6.95s\tremaining: 1.74s\n",
      "80:\tlearn: 0.2976203\ttest: 0.2861564\tbest: 0.2861437 (78)\ttotal: 7.03s\tremaining: 1.65s\n",
      "81:\tlearn: 0.2975897\ttest: 0.2861639\tbest: 0.2861437 (78)\ttotal: 7.14s\tremaining: 1.57s\n",
      "82:\tlearn: 0.2975544\ttest: 0.2861293\tbest: 0.2861293 (82)\ttotal: 7.26s\tremaining: 1.49s\n",
      "83:\tlearn: 0.2975207\ttest: 0.2861495\tbest: 0.2861293 (82)\ttotal: 7.35s\tremaining: 1.4s\n",
      "84:\tlearn: 0.2975207\ttest: 0.2861499\tbest: 0.2861293 (82)\ttotal: 7.4s\tremaining: 1.31s\n",
      "85:\tlearn: 0.2975151\ttest: 0.2861460\tbest: 0.2861293 (82)\ttotal: 7.49s\tremaining: 1.22s\n",
      "86:\tlearn: 0.2974535\ttest: 0.2861362\tbest: 0.2861293 (82)\ttotal: 7.64s\tremaining: 1.14s\n",
      "87:\tlearn: 0.2974535\ttest: 0.2861367\tbest: 0.2861293 (82)\ttotal: 7.68s\tremaining: 1.05s\n",
      "88:\tlearn: 0.2974535\ttest: 0.2861371\tbest: 0.2861293 (82)\ttotal: 7.72s\tremaining: 954ms\n",
      "89:\tlearn: 0.2974517\ttest: 0.2861427\tbest: 0.2861293 (82)\ttotal: 7.77s\tremaining: 864ms\n",
      "90:\tlearn: 0.2973502\ttest: 0.2860664\tbest: 0.2860664 (90)\ttotal: 7.9s\tremaining: 782ms\n",
      "91:\tlearn: 0.2973276\ttest: 0.2860237\tbest: 0.2860237 (91)\ttotal: 7.99s\tremaining: 694ms\n",
      "92:\tlearn: 0.2973236\ttest: 0.2860256\tbest: 0.2860237 (91)\ttotal: 8.05s\tremaining: 606ms\n",
      "93:\tlearn: 0.2972953\ttest: 0.2860152\tbest: 0.2860152 (93)\ttotal: 8.18s\tremaining: 522ms\n",
      "94:\tlearn: 0.2972537\ttest: 0.2859765\tbest: 0.2859765 (94)\ttotal: 8.33s\tremaining: 439ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95:\tlearn: 0.2972434\ttest: 0.2859774\tbest: 0.2859765 (94)\ttotal: 8.42s\tremaining: 351ms\n",
      "96:\tlearn: 0.2971881\ttest: 0.2859623\tbest: 0.2859623 (96)\ttotal: 8.54s\tremaining: 264ms\n",
      "97:\tlearn: 0.2971674\ttest: 0.2859644\tbest: 0.2859623 (96)\ttotal: 8.66s\tremaining: 177ms\n",
      "98:\tlearn: 0.2971250\ttest: 0.2859578\tbest: 0.2859578 (98)\ttotal: 8.8s\tremaining: 88.8ms\n",
      "99:\tlearn: 0.2970371\ttest: 0.2859411\tbest: 0.2859411 (99)\ttotal: 8.99s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.2859410566\n",
      "bestIteration = 99\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core._CatBoostBase at 0xb492b70>"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_feat_index = np.where(x_req.dtypes != np.int64)[0]\n",
    "\n",
    "cat = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=6)\n",
    "cat.fit(x_train_req, y_train_req, cat_features=cat_feat_index, eval_set=(x_test_req, y_test_req))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'Logloss_test_avg': [0.6044590655482499,\n",
       "              0.5358769341370746,\n",
       "              0.4831631361206874,\n",
       "              0.44266836665059667,\n",
       "              0.41130337126945904,\n",
       "              0.387199392487251,\n",
       "              0.36854173334588264,\n",
       "              0.35395704295215674,\n",
       "              0.34267024847766736,\n",
       "              0.33383455044077237,\n",
       "              0.3268189790347938,\n",
       "              0.32134512608824817,\n",
       "              0.3170148265141174,\n",
       "              0.31358654291561605,\n",
       "              0.3108858404954843,\n",
       "              0.3087493540895614,\n",
       "              0.30703190890868537,\n",
       "              0.3056071108638281,\n",
       "              0.3044784752974292,\n",
       "              0.30361880765614996,\n",
       "              0.30287648987621496,\n",
       "              0.30227719167405254,\n",
       "              0.30180587338820675,\n",
       "              0.30143048209180734,\n",
       "              0.3011080926921161,\n",
       "              0.30076702870558575,\n",
       "              0.30057768507970434,\n",
       "              0.3003539898528995,\n",
       "              0.30013740848490567,\n",
       "              0.29998441518110547,\n",
       "              0.2998357550579544,\n",
       "              0.29970791028018445,\n",
       "              0.299592455581155,\n",
       "              0.2994793051072507,\n",
       "              0.29943097687793213,\n",
       "              0.2993677936185415,\n",
       "              0.29928015734905683,\n",
       "              0.299222242561448,\n",
       "              0.2991752985581511,\n",
       "              0.29911290636458154,\n",
       "              0.2990719905096405,\n",
       "              0.2990395415988582,\n",
       "              0.29901092380712674,\n",
       "              0.2989728229349152,\n",
       "              0.2989349204481081,\n",
       "              0.2989098055941685,\n",
       "              0.29890893327692036,\n",
       "              0.2988984007468958,\n",
       "              0.2988744165186078,\n",
       "              0.2988390216339997,\n",
       "              0.2988169570520611,\n",
       "              0.29880086173182807,\n",
       "              0.2987872386782083,\n",
       "              0.29877375401527106,\n",
       "              0.2987654070090594,\n",
       "              0.29874081971405325,\n",
       "              0.29871530773785976,\n",
       "              0.2986833038924919,\n",
       "              0.298670772203811,\n",
       "              0.2986581003919335,\n",
       "              0.29864106987252914,\n",
       "              0.29862472525842704,\n",
       "              0.29861391421792927,\n",
       "              0.29859464883276593,\n",
       "              0.29858701169356383,\n",
       "              0.29856894153608016,\n",
       "              0.2985328715862442,\n",
       "              0.2985217985184493,\n",
       "              0.2985097585418705,\n",
       "              0.29851009167886383,\n",
       "              0.2985048794697234,\n",
       "              0.2984942992399261,\n",
       "              0.29849027983616944,\n",
       "              0.2984832916130662,\n",
       "              0.2984790267218383,\n",
       "              0.29847781047271227,\n",
       "              0.29847812593202044,\n",
       "              0.29846932244832625,\n",
       "              0.2984606805922132,\n",
       "              0.29845906394431315,\n",
       "              0.29845616719079077,\n",
       "              0.2984518377971164,\n",
       "              0.2984432088146489,\n",
       "              0.2984427970927467,\n",
       "              0.29843574453316546,\n",
       "              0.2984345194878634,\n",
       "              0.29842057932057375,\n",
       "              0.29841145820236437,\n",
       "              0.2984066556134876,\n",
       "              0.29840614559528306,\n",
       "              0.29839704857969346,\n",
       "              0.2983998655234941,\n",
       "              0.29837979591324004,\n",
       "              0.2983611838902634,\n",
       "              0.2983543006820771,\n",
       "              0.2983559682767874,\n",
       "              0.2983673464267911,\n",
       "              0.29836618662111847,\n",
       "              0.2983631710069754,\n",
       "              0.2983540282459697],\n",
       "             'Logloss_test_stddev': [0.0005302050655482113,\n",
       "              0.0008027996968810062,\n",
       "              0.0012501554322457066,\n",
       "              0.0017044927282799417,\n",
       "              0.002188541030553298,\n",
       "              0.002599548840434625,\n",
       "              0.002996971416666937,\n",
       "              0.003450168330827939,\n",
       "              0.0037769490260006955,\n",
       "              0.004077913727747972,\n",
       "              0.004455317491192571,\n",
       "              0.004662540774977728,\n",
       "              0.004903317151723187,\n",
       "              0.005160726269618181,\n",
       "              0.005367765111183356,\n",
       "              0.005542277959299176,\n",
       "              0.005646204949992743,\n",
       "              0.0057061550909081335,\n",
       "              0.005875578186342371,\n",
       "              0.005994428729222684,\n",
       "              0.006105136833629706,\n",
       "              0.0061573828365720585,\n",
       "              0.006229532655314151,\n",
       "              0.006320123620611304,\n",
       "              0.0063942935233799285,\n",
       "              0.0063963155760151675,\n",
       "              0.006448959221363585,\n",
       "              0.006471156497506218,\n",
       "              0.006534513126844361,\n",
       "              0.006576343917004817,\n",
       "              0.006663152594243766,\n",
       "              0.006709368772963875,\n",
       "              0.006751011208441008,\n",
       "              0.0067784201900404125,\n",
       "              0.006799666167304942,\n",
       "              0.006800166340075271,\n",
       "              0.006800697483757739,\n",
       "              0.006812882113251861,\n",
       "              0.006828734962474072,\n",
       "              0.006872514494898817,\n",
       "              0.006891391340597478,\n",
       "              0.00691316021415298,\n",
       "              0.00692535131358381,\n",
       "              0.006913193907657615,\n",
       "              0.006904946091558276,\n",
       "              0.00691017798098321,\n",
       "              0.0069100101210384885,\n",
       "              0.006925915178919674,\n",
       "              0.006910704898092721,\n",
       "              0.006943256513525515,\n",
       "              0.006961458576571121,\n",
       "              0.006971609634633492,\n",
       "              0.0069685642588975475,\n",
       "              0.006980903319880433,\n",
       "              0.0069823281692871116,\n",
       "              0.006975882316231234,\n",
       "              0.006961055816053447,\n",
       "              0.006978349951134445,\n",
       "              0.006967356886416826,\n",
       "              0.006971947309992971,\n",
       "              0.006996925897789951,\n",
       "              0.007014967460346978,\n",
       "              0.00701900066334403,\n",
       "              0.007014313934950003,\n",
       "              0.007023263847073047,\n",
       "              0.007013158153734274,\n",
       "              0.007014593149820373,\n",
       "              0.00703194629128407,\n",
       "              0.007025643716956628,\n",
       "              0.007027269085064396,\n",
       "              0.0070234634108756215,\n",
       "              0.0070324896662203595,\n",
       "              0.007028067829995632,\n",
       "              0.007037599431983604,\n",
       "              0.007030762906557176,\n",
       "              0.007028925151159392,\n",
       "              0.007028733034567221,\n",
       "              0.007023509645566279,\n",
       "              0.007022845493153102,\n",
       "              0.007027219922946196,\n",
       "              0.007030131137317569,\n",
       "              0.0070184799541852,\n",
       "              0.007020027810440625,\n",
       "              0.007018103663314115,\n",
       "              0.007011439353988954,\n",
       "              0.007012343930922773,\n",
       "              0.00700901381688833,\n",
       "              0.0070094065305061634,\n",
       "              0.007012071758683345,\n",
       "              0.007001812469527092,\n",
       "              0.006996817985712948,\n",
       "              0.006993653989749598,\n",
       "              0.006957780854313694,\n",
       "              0.006960594360837947,\n",
       "              0.006960775853615596,\n",
       "              0.006964897669496203,\n",
       "              0.006970205352013158,\n",
       "              0.006968844226525232,\n",
       "              0.006972525163113022,\n",
       "              0.00696780388967226],\n",
       "             'Logloss_train_avg': [0.604349241240369,\n",
       "              0.5358020680729753,\n",
       "              0.48308152364118895,\n",
       "              0.442505170697938,\n",
       "              0.41118078130039615,\n",
       "              0.38706234785023924,\n",
       "              0.3684075733331451,\n",
       "              0.3538389461994355,\n",
       "              0.3425480848450321,\n",
       "              0.3337253433122166,\n",
       "              0.3267062994382293,\n",
       "              0.32123297867833395,\n",
       "              0.3168497759150529,\n",
       "              0.3134374414191308,\n",
       "              0.31071284430257456,\n",
       "              0.3085836915088448,\n",
       "              0.3068802691347593,\n",
       "              0.3054687724346419,\n",
       "              0.3043236314358002,\n",
       "              0.30345949081643253,\n",
       "              0.3026856582606139,\n",
       "              0.30209735176719654,\n",
       "              0.3016176640844645,\n",
       "              0.3012288557197367,\n",
       "              0.30089851009448554,\n",
       "              0.3005606073386863,\n",
       "              0.30037558461362146,\n",
       "              0.30013161672204347,\n",
       "              0.2999152353139227,\n",
       "              0.299767407505126,\n",
       "              0.29960606015227553,\n",
       "              0.2994759691813611,\n",
       "              0.299360471515352,\n",
       "              0.29923537718865256,\n",
       "              0.29917809228779213,\n",
       "              0.2991115423921114,\n",
       "              0.2990184998926565,\n",
       "              0.2989514440273137,\n",
       "              0.2988971874122312,\n",
       "              0.29882894974751234,\n",
       "              0.29876022685209264,\n",
       "              0.2987174622178017,\n",
       "              0.2986430754730842,\n",
       "              0.2985839106363633,\n",
       "              0.2985418770217043,\n",
       "              0.29849572444632566,\n",
       "              0.2984644697004989,\n",
       "              0.29843465196324404,\n",
       "              0.2983912709862846,\n",
       "              0.2983403226007285,\n",
       "              0.2982975347419277,\n",
       "              0.2982590585513014,\n",
       "              0.29821645763103916,\n",
       "              0.29817197617816704,\n",
       "              0.2981661695034217,\n",
       "              0.2981294237591265,\n",
       "              0.2980921593130735,\n",
       "              0.29803033073816076,\n",
       "              0.297996321786519,\n",
       "              0.2979641733403778,\n",
       "              0.2979243046818036,\n",
       "              0.29790430489010467,\n",
       "              0.297884682004878,\n",
       "              0.2978466731203276,\n",
       "              0.2978171220967707,\n",
       "              0.2977806326404369,\n",
       "              0.29772757559456753,\n",
       "              0.29769696381128585,\n",
       "              0.2976634359207823,\n",
       "              0.29764186952156085,\n",
       "              0.29762776141031155,\n",
       "              0.2976162248108983,\n",
       "              0.2976038325977609,\n",
       "              0.29758762882284934,\n",
       "              0.2975657568347533,\n",
       "              0.29754613325884105,\n",
       "              0.29754520185728184,\n",
       "              0.2975235176310056,\n",
       "              0.29749196793330085,\n",
       "              0.297479992499622,\n",
       "              0.29746845905440955,\n",
       "              0.2974460272530872,\n",
       "              0.29740648594493413,\n",
       "              0.29738492897031255,\n",
       "              0.2973430975479801,\n",
       "              0.297333837208435,\n",
       "              0.2973013688598046,\n",
       "              0.29724211480450136,\n",
       "              0.2972149063609052,\n",
       "              0.29717731358303195,\n",
       "              0.29715235473378304,\n",
       "              0.29712897271253047,\n",
       "              0.2970511432437738,\n",
       "              0.2970278144896764,\n",
       "              0.2969969173264362,\n",
       "              0.29697453172191796,\n",
       "              0.2969475382394431,\n",
       "              0.29689732747420583,\n",
       "              0.29686837472135463,\n",
       "              0.29684146533857925],\n",
       "             'Logloss_train_stddev': [0.00047276868938167223,\n",
       "              0.0009225340458520675,\n",
       "              0.0011406337808597723,\n",
       "              0.001313434496156618,\n",
       "              0.001364360010532723,\n",
       "              0.0014503794403463752,\n",
       "              0.0015164442307847342,\n",
       "              0.0015125768088979068,\n",
       "              0.0015536110308879882,\n",
       "              0.0015856111663879882,\n",
       "              0.0015069361916836136,\n",
       "              0.0015514022610086796,\n",
       "              0.0015704435727601801,\n",
       "              0.001530913419012218,\n",
       "              0.0014964369254185307,\n",
       "              0.0015058687665562148,\n",
       "              0.0015572819528380307,\n",
       "              0.001648642749380095,\n",
       "              0.0016254077445704454,\n",
       "              0.0016179583531942478,\n",
       "              0.0016081231131138633,\n",
       "              0.001650887310460214,\n",
       "              0.0016646980572122647,\n",
       "              0.0016407922768598285,\n",
       "              0.001631845772384906,\n",
       "              0.0016957270844960037,\n",
       "              0.0017003203520567058,\n",
       "              0.0017077615869507173,\n",
       "              0.0016887168632956282,\n",
       "              0.0016790709948551889,\n",
       "              0.00162205429579181,\n",
       "              0.0016100114988281944,\n",
       "              0.0016075056318674357,\n",
       "              0.0016219735985163188,\n",
       "              0.0016295563650066225,\n",
       "              0.0016575076383165753,\n",
       "              0.001683405210225322,\n",
       "              0.0016755815234488353,\n",
       "              0.0016688862469853114,\n",
       "              0.0016535460591816694,\n",
       "              0.0016017766861104383,\n",
       "              0.0015962267889620821,\n",
       "              0.0016216522704537095,\n",
       "              0.001663393246122245,\n",
       "              0.0016853812139615008,\n",
       "              0.0016736603899131907,\n",
       "              0.0016584007049762171,\n",
       "              0.001644956939285656,\n",
       "              0.00167876837960247,\n",
       "              0.0016435748154535704,\n",
       "              0.0016096277131620947,\n",
       "              0.0016158663974876087,\n",
       "              0.0016250696321150977,\n",
       "              0.0016319677509172487,\n",
       "              0.0016345203392000242,\n",
       "              0.0016437328500113516,\n",
       "              0.0016565267818363808,\n",
       "              0.0016632103295203277,\n",
       "              0.0016737608940007559,\n",
       "              0.00166017027542984,\n",
       "              0.0016372936432030145,\n",
       "              0.001622736688144356,\n",
       "              0.0016174803285589196,\n",
       "              0.0016329836740067172,\n",
       "              0.0016196583042911482,\n",
       "              0.0016271696366330132,\n",
       "              0.0016245381252933922,\n",
       "              0.0016065636836740938,\n",
       "              0.001623325144019267,\n",
       "              0.0016120264054728874,\n",
       "              0.001618180899642977,\n",
       "              0.0016105802564980237,\n",
       "              0.0016218540856086556,\n",
       "              0.0015980740834773883,\n",
       "              0.0016055140905891158,\n",
       "              0.0016016556643391436,\n",
       "              0.0016011503513134158,\n",
       "              0.0015869636888822273,\n",
       "              0.001589734839845426,\n",
       "              0.0015770962884687726,\n",
       "              0.0015718892798656805,\n",
       "              0.0015784658733142398,\n",
       "              0.0015943917447062608,\n",
       "              0.0016036650026530089,\n",
       "              0.001605389376816444,\n",
       "              0.001602139208433136,\n",
       "              0.0016057456557305614,\n",
       "              0.001564697824581389,\n",
       "              0.0015632914294796427,\n",
       "              0.001585022709300779,\n",
       "              0.0015934704532821057,\n",
       "              0.0015817111626003012,\n",
       "              0.0016187153035830666,\n",
       "              0.0016116141686608545,\n",
       "              0.0016223876691456346,\n",
       "              0.0016046580791354676,\n",
       "              0.0016106491326443682,\n",
       "              0.0016497659486925014,\n",
       "              0.0016501846866844876,\n",
       "              0.0016530147144865177]})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from catboost import cv, Pool\n",
    "\n",
    "params ={\n",
    "    'depth': 6,\n",
    "    'learning_rate': .1,\n",
    "    'iterations': 100,\n",
    "    'loss_function': 'Logloss'\n",
    "}\n",
    "\n",
    "cv(params, Pool(x_train_req, y_train_req, cat_features=cat_feat_index),  partition_random_seed=1234, fold_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ATT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_CORRELATIVO</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47411</th>\n",
       "      <td>0.291465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39861</th>\n",
       "      <td>0.269891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38898</th>\n",
       "      <td>0.028191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50927</th>\n",
       "      <td>0.019253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32969</th>\n",
       "      <td>0.470753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     ATT\n",
       "ID_CORRELATIVO          \n",
       "47411           0.291465\n",
       "39861           0.269891\n",
       "38898           0.028191\n",
       "50927           0.019253\n",
       "32969           0.470753"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_req['ATTRITION_REQ'] = cat.predict_proba(df_test_req)[:, 1]\n",
    "\n",
    "df_test_req_ = df_test_req.groupby(by=df_test_req.index)['ATTRITION_REQ'].mean()\n",
    "\n",
    "new_submission = pd.DataFrame(index=x_test_.index)\n",
    "new_submission['ATT'] = y_pred\n",
    "new_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test_req_ = new_submission.join(df_test_req_)\n",
    "df_test_req_['ATTRITION'] = df_test_req_.apply(lambda x: np.mean(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test_req_.reset_index(inplace=True)\n",
    "df_test_req_[['ID_CORRELATIVO', 'ATTRITION']].to_csv('./data/submission11.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

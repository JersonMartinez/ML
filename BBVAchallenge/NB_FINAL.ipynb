{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_excel('./data/train_clientes.xlsx', index_col='ID_CORRELATIVO')\n",
    "df_test = pd.read_excel('./data/test_clientes.xlsx', index_col='ID_CORRELATIVO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Clientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train = df_train.drop(['ATTRITION'], axis=1)\n",
    "y_train = df_train['ATTRITION']\n",
    "x_test = df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 51)\n"
     ]
    }
   ],
   "source": [
    "# Join train and test df to aplly feature engineering\n",
    "df = pd.concat([x_train, x_test])\n",
    "print df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'RANG_INGRESO',\n",
       " u'FLAG_LIMA_PROVINCIA',\n",
       " u'RANG_SDO_PASIVO_MENOS0',\n",
       " u'RANG_NRO_PRODUCTOS_MENOS0']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split columns by type\n",
    "num_cols = df.select_dtypes(exclude=['datetime', 'object']).columns.tolist()\n",
    "cat_cols = [col for col in df.columns if col not in num_cols]\n",
    "cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID_CORRELATIVO\n",
       "35653    0.626166\n",
       "66575    0.373834\n",
       "56800    0.373834\n",
       "8410     0.373834\n",
       "6853     0.626166\n",
       "Name: FLAG_LIMA_PROVINCIA, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[cat_cols[1]] = df[cat_cols[1]].map(df[cat_cols[1]].value_counts()/df[cat_cols[1]].value_counts().sum())\n",
    "df[cat_cols[1]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_cols.remove('FLAG_LIMA_PROVINCIA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in cat_cols:\n",
    "    df[col] = df[col].astype('category').cat.codes\n",
    "\n",
    "df.drop('CODMES', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-706063283e48>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdecomp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdf_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecomp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\JERSON\\Anaconda2\\lib\\site-packages\\sklearn\\decomposition\\pca.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m         \"\"\"\n\u001b[1;32m--> 344\u001b[1;33m         \u001b[0mU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mV\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    345\u001b[0m         \u001b[0mU\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mU\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_components_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JERSON\\Anaconda2\\lib\\site-packages\\sklearn\\decomposition\\pca.pyc\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m         X = check_array(X, dtype=[np.float64, np.float32], ensure_2d=True,\n\u001b[1;32m--> 366\u001b[1;33m                         copy=self.copy)\n\u001b[0m\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[1;31m# Handle n_components==None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JERSON\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    420\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[0;32m    421\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m             \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JERSON\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\validation.pyc\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     41\u001b[0m             and not np.isfinite(X).all()):\n\u001b[0;32m     42\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[1;32m---> 43\u001b[1;33m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "decomp = PCA(n_components=10)\n",
    "df_ = decomp.fit_transform(df.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelado clientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JERSON\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_ = df.iloc[0:70000]\n",
    "x_test_ = df.iloc[70000:]\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x_train_, y_train, test_size=.10, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tTest-logloss:0.540315\n",
      "Will train until Test-logloss hasn't improved in 10 rounds.\n",
      "[1]\tTest-logloss:0.456852\n",
      "[2]\tTest-logloss:0.407596\n",
      "[3]\tTest-logloss:0.374685\n",
      "[4]\tTest-logloss:0.354341\n",
      "[5]\tTest-logloss:0.340005\n",
      "[6]\tTest-logloss:0.329807\n",
      "[7]\tTest-logloss:0.324418\n",
      "[8]\tTest-logloss:0.320152\n",
      "[9]\tTest-logloss:0.316743\n",
      "[10]\tTest-logloss:0.313036\n",
      "[11]\tTest-logloss:0.311438\n",
      "[12]\tTest-logloss:0.310112\n",
      "[13]\tTest-logloss:0.30955\n",
      "[14]\tTest-logloss:0.308657\n",
      "[15]\tTest-logloss:0.307889\n",
      "[16]\tTest-logloss:0.307728\n",
      "[17]\tTest-logloss:0.306558\n",
      "[18]\tTest-logloss:0.306367\n",
      "[19]\tTest-logloss:0.306281\n",
      "[20]\tTest-logloss:0.305077\n",
      "[21]\tTest-logloss:0.30452\n",
      "[22]\tTest-logloss:0.30462\n",
      "[23]\tTest-logloss:0.304122\n",
      "[24]\tTest-logloss:0.303944\n",
      "[25]\tTest-logloss:0.303644\n",
      "[26]\tTest-logloss:0.303615\n",
      "[27]\tTest-logloss:0.303854\n",
      "[28]\tTest-logloss:0.304092\n",
      "[29]\tTest-logloss:0.30428\n",
      "[30]\tTest-logloss:0.30363\n",
      "[31]\tTest-logloss:0.303794\n",
      "[32]\tTest-logloss:0.303731\n",
      "[33]\tTest-logloss:0.303704\n",
      "[34]\tTest-logloss:0.303787\n",
      "[35]\tTest-logloss:0.303685\n",
      "[36]\tTest-logloss:0.303641\n",
      "Stopping. Best iteration:\n",
      "[26]\tTest-logloss:0.303615\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(xtrain, label=ytrain)\n",
    "dtest = xgb.DMatrix(xtest, label=ytest)\n",
    "\n",
    "params_xgb = {\n",
    "    'max_depth': 6,\n",
    "    'min_child_weight': 1,\n",
    "    'eta':.3,\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree': 1,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric':'logloss'\n",
    "}\n",
    "\n",
    "xgb_model = xgb.train(params_xgb, \n",
    "                      dtrain,\n",
    "                      num_boost_round=999, \n",
    "                      early_stopping_rounds=10,\n",
    "                      evals=[(dtest, \"Test\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test-logloss-mean</th>\n",
       "      <th>test-logloss-std</th>\n",
       "      <th>train-logloss-mean</th>\n",
       "      <th>train-logloss-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.540111</td>\n",
       "      <td>0.001387</td>\n",
       "      <td>0.539419</td>\n",
       "      <td>0.000608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.456349</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.455057</td>\n",
       "      <td>0.000829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.406292</td>\n",
       "      <td>0.003238</td>\n",
       "      <td>0.404449</td>\n",
       "      <td>0.001026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.373840</td>\n",
       "      <td>0.003555</td>\n",
       "      <td>0.371301</td>\n",
       "      <td>0.001229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.353053</td>\n",
       "      <td>0.004043</td>\n",
       "      <td>0.349708</td>\n",
       "      <td>0.001401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.339353</td>\n",
       "      <td>0.004531</td>\n",
       "      <td>0.335195</td>\n",
       "      <td>0.001321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.330301</td>\n",
       "      <td>0.004938</td>\n",
       "      <td>0.325050</td>\n",
       "      <td>0.001465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.324064</td>\n",
       "      <td>0.004859</td>\n",
       "      <td>0.317890</td>\n",
       "      <td>0.001765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.319504</td>\n",
       "      <td>0.005440</td>\n",
       "      <td>0.312271</td>\n",
       "      <td>0.001434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.316669</td>\n",
       "      <td>0.005591</td>\n",
       "      <td>0.308396</td>\n",
       "      <td>0.001271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.314081</td>\n",
       "      <td>0.005437</td>\n",
       "      <td>0.304722</td>\n",
       "      <td>0.001297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.312411</td>\n",
       "      <td>0.005669</td>\n",
       "      <td>0.301956</td>\n",
       "      <td>0.001224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.311351</td>\n",
       "      <td>0.005732</td>\n",
       "      <td>0.299872</td>\n",
       "      <td>0.001233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.310095</td>\n",
       "      <td>0.005776</td>\n",
       "      <td>0.297539</td>\n",
       "      <td>0.001311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.309300</td>\n",
       "      <td>0.005912</td>\n",
       "      <td>0.295559</td>\n",
       "      <td>0.001132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.308645</td>\n",
       "      <td>0.006022</td>\n",
       "      <td>0.293882</td>\n",
       "      <td>0.001179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.308127</td>\n",
       "      <td>0.005951</td>\n",
       "      <td>0.292211</td>\n",
       "      <td>0.001325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.307308</td>\n",
       "      <td>0.005340</td>\n",
       "      <td>0.290531</td>\n",
       "      <td>0.001799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.306876</td>\n",
       "      <td>0.005183</td>\n",
       "      <td>0.289158</td>\n",
       "      <td>0.002125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.306450</td>\n",
       "      <td>0.005505</td>\n",
       "      <td>0.287695</td>\n",
       "      <td>0.001966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.306119</td>\n",
       "      <td>0.005709</td>\n",
       "      <td>0.286388</td>\n",
       "      <td>0.001947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.306066</td>\n",
       "      <td>0.005630</td>\n",
       "      <td>0.285366</td>\n",
       "      <td>0.002211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.305632</td>\n",
       "      <td>0.005272</td>\n",
       "      <td>0.284035</td>\n",
       "      <td>0.002342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.305415</td>\n",
       "      <td>0.005263</td>\n",
       "      <td>0.282980</td>\n",
       "      <td>0.001904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.305166</td>\n",
       "      <td>0.005153</td>\n",
       "      <td>0.281612</td>\n",
       "      <td>0.001913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.305186</td>\n",
       "      <td>0.005130</td>\n",
       "      <td>0.280909</td>\n",
       "      <td>0.001870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.305090</td>\n",
       "      <td>0.004989</td>\n",
       "      <td>0.280320</td>\n",
       "      <td>0.002013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.305004</td>\n",
       "      <td>0.004937</td>\n",
       "      <td>0.279403</td>\n",
       "      <td>0.002251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.304829</td>\n",
       "      <td>0.004848</td>\n",
       "      <td>0.278500</td>\n",
       "      <td>0.002212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.304809</td>\n",
       "      <td>0.004911</td>\n",
       "      <td>0.277769</td>\n",
       "      <td>0.002290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.304851</td>\n",
       "      <td>0.004892</td>\n",
       "      <td>0.276929</td>\n",
       "      <td>0.002273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.304708</td>\n",
       "      <td>0.005133</td>\n",
       "      <td>0.276058</td>\n",
       "      <td>0.002054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.304601</td>\n",
       "      <td>0.005155</td>\n",
       "      <td>0.275215</td>\n",
       "      <td>0.001997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    test-logloss-mean  test-logloss-std  train-logloss-mean  train-logloss-std\n",
       "0            0.540111          0.001387            0.539419           0.000608\n",
       "1            0.456349          0.002412            0.455057           0.000829\n",
       "2            0.406292          0.003238            0.404449           0.001026\n",
       "3            0.373840          0.003555            0.371301           0.001229\n",
       "4            0.353053          0.004043            0.349708           0.001401\n",
       "5            0.339353          0.004531            0.335195           0.001321\n",
       "6            0.330301          0.004938            0.325050           0.001465\n",
       "7            0.324064          0.004859            0.317890           0.001765\n",
       "8            0.319504          0.005440            0.312271           0.001434\n",
       "9            0.316669          0.005591            0.308396           0.001271\n",
       "10           0.314081          0.005437            0.304722           0.001297\n",
       "11           0.312411          0.005669            0.301956           0.001224\n",
       "12           0.311351          0.005732            0.299872           0.001233\n",
       "13           0.310095          0.005776            0.297539           0.001311\n",
       "14           0.309300          0.005912            0.295559           0.001132\n",
       "15           0.308645          0.006022            0.293882           0.001179\n",
       "16           0.308127          0.005951            0.292211           0.001325\n",
       "17           0.307308          0.005340            0.290531           0.001799\n",
       "18           0.306876          0.005183            0.289158           0.002125\n",
       "19           0.306450          0.005505            0.287695           0.001966\n",
       "20           0.306119          0.005709            0.286388           0.001947\n",
       "21           0.306066          0.005630            0.285366           0.002211\n",
       "22           0.305632          0.005272            0.284035           0.002342\n",
       "23           0.305415          0.005263            0.282980           0.001904\n",
       "24           0.305166          0.005153            0.281612           0.001913\n",
       "25           0.305186          0.005130            0.280909           0.001870\n",
       "26           0.305090          0.004989            0.280320           0.002013\n",
       "27           0.305004          0.004937            0.279403           0.002251\n",
       "28           0.304829          0.004848            0.278500           0.002212\n",
       "29           0.304809          0.004911            0.277769           0.002290\n",
       "30           0.304851          0.004892            0.276929           0.002273\n",
       "31           0.304708          0.005133            0.276058           0.002054\n",
       "32           0.304601          0.005155            0.275215           0.001997"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.cv(params_xgb, \n",
    "       xgb.DMatrix(x_train_, label=y_train), \n",
    "       num_boost_round=999,\n",
    "       early_stopping_rounds=10, \n",
    "       nfold=5, \n",
    "       seed=0,\n",
    "       metrics='logloss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with max_depth=9, min_child_weight=5\n",
      "\tLogloss 0.3025916 for 24 rounds\n",
      "CV with max_depth=9, min_child_weight=6\n",
      "\tLogloss 0.3024176 for 17 rounds\n",
      "CV with max_depth=9, min_child_weight=7\n",
      "\tLogloss 0.3015254 for 17 rounds\n",
      "CV with max_depth=10, min_child_weight=5\n",
      "\tLogloss 0.3026122 for 15 rounds\n",
      "CV with max_depth=10, min_child_weight=6\n",
      "\tLogloss 0.3022632 for 15 rounds\n",
      "CV with max_depth=10, min_child_weight=7\n",
      "\tLogloss 0.3016802 for 18 rounds\n",
      "CV with max_depth=11, min_child_weight=5\n",
      "\tLogloss 0.3025726 for 15 rounds\n",
      "CV with max_depth=11, min_child_weight=6\n",
      "\tLogloss 0.3018722 for 18 rounds\n",
      "CV with max_depth=11, min_child_weight=7\n",
      "\tLogloss 0.3019368 for 14 rounds\n",
      "Best params: 9, 7, Logloss: 0.3015254\n"
     ]
    }
   ],
   "source": [
    "gridsearch_params = [(max_depth, min_child_weight)\n",
    "                    for max_depth in range(9,12)\n",
    "                    for min_child_weight in range(5,8)]\n",
    "\n",
    "min_logloss = float(\"Inf\")\n",
    "best_params = None\n",
    "\n",
    "for max_depth, min_child_weight in gridsearch_params:\n",
    "    print(\"CV with max_depth={}, min_child_weight={}\".format(\n",
    "                             max_depth,\n",
    "                             min_child_weight))\n",
    "\n",
    "    # Update our parameters\n",
    "    params_xgb['max_depth'] = max_depth\n",
    "    params_xgb['min_child_weight'] = min_child_weight\n",
    "\n",
    "    # Run CV\n",
    "    cv_results = xgb.cv(\n",
    "        params_xgb,\n",
    "        xgb.DMatrix(x_train_, label=y_train),\n",
    "        num_boost_round=999,\n",
    "        seed=0,\n",
    "        nfold=5,\n",
    "        metrics='logloss',\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "\n",
    "    # Update best logloss\n",
    "    mean_logloss = cv_results['test-logloss-mean'].min()\n",
    "    boost_rounds = cv_results['test-logloss-mean'].argmin()\n",
    "    print(\"\\tLogloss {} for {} rounds\".format(mean_logloss, boost_rounds))\n",
    "    if mean_logloss < min_logloss:\n",
    "        min_logloss = mean_logloss\n",
    "        best_params = (max_depth, min_child_weight)\n",
    "\n",
    "print(\"Best params: {}, {}, Logloss: {}\".format(best_params[0], best_params[1], min_logloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params_xgb['max_depth'] = 9\n",
    "params_xgb['min_child_weight'] = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with subsample=1.0, colsample_bytree=1.0\n",
      "\tLogloss 0.3015254 for 17 rounds\n",
      "CV with subsample=1.0, colsample_bytree=0.9\n",
      "\tLogloss 0.3023684 for 22 rounds\n",
      "CV with subsample=1.0, colsample_bytree=0.8\n",
      "\tLogloss 0.3014086 for 18 rounds\n",
      "CV with subsample=1.0, colsample_bytree=0.7\n",
      "\tLogloss 0.3013444 for 19 rounds\n",
      "CV with subsample=0.9, colsample_bytree=1.0\n",
      "\tLogloss 0.302638 for 17 rounds\n",
      "CV with subsample=0.9, colsample_bytree=0.9\n",
      "\tLogloss 0.3020914 for 22 rounds\n",
      "CV with subsample=0.9, colsample_bytree=0.8\n",
      "\tLogloss 0.3029778 for 17 rounds\n",
      "CV with subsample=0.9, colsample_bytree=0.7\n",
      "\tLogloss 0.3015262 for 22 rounds\n",
      "CV with subsample=0.8, colsample_bytree=1.0\n",
      "\tLogloss 0.302576 for 24 rounds\n",
      "CV with subsample=0.8, colsample_bytree=0.9\n",
      "\tLogloss 0.3024208 for 17 rounds\n",
      "CV with subsample=0.8, colsample_bytree=0.8\n",
      "\tLogloss 0.3036434 for 18 rounds\n",
      "CV with subsample=0.8, colsample_bytree=0.7\n",
      "\tLogloss 0.3020036 for 19 rounds\n",
      "CV with subsample=0.7, colsample_bytree=1.0\n",
      "\tLogloss 0.3032336 for 15 rounds\n",
      "CV with subsample=0.7, colsample_bytree=0.9\n",
      "\tLogloss 0.3036846 for 16 rounds\n",
      "CV with subsample=0.7, colsample_bytree=0.8\n",
      "\tLogloss 0.3033424 for 21 rounds\n",
      "CV with subsample=0.7, colsample_bytree=0.7\n",
      "\tLogloss 0.3033986 for 21 rounds\n",
      "Best params: 1.0, 0.7, Logloss: 0.3013444\n"
     ]
    }
   ],
   "source": [
    "gridsearch_params = [\n",
    "    (subsample, colsample)\n",
    "    for subsample in [i/10. for i in range(7,11)]\n",
    "    for colsample in [i/10. for i in range(7,11)]\n",
    "]\n",
    "\n",
    "min_logloss = float(\"Inf\")\n",
    "best_params = None\n",
    "\n",
    "for subsample, colsample in reversed(gridsearch_params):\n",
    "    print(\"CV with subsample={}, colsample_bytree={}\".format(\n",
    "                             subsample,\n",
    "                             colsample))\n",
    "\n",
    "    # We update our parameters\n",
    "    params_xgb['subsample'] = subsample\n",
    "    params_xgb['colsample_bytree'] = colsample\n",
    "\n",
    "    # Run CV\n",
    "    cv_results = xgb.cv(\n",
    "        params_xgb,\n",
    "        xgb.DMatrix(x_train_, label=y_train),\n",
    "        num_boost_round=999,\n",
    "        seed=0,\n",
    "        nfold=5,\n",
    "        metrics='logloss',\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "\n",
    "    # Update best logloss\n",
    "    mean_logloss = cv_results['test-logloss-mean'].min()\n",
    "    boost_rounds = cv_results['test-logloss-mean'].argmin()\n",
    "    print(\"\\tLogloss {} for {} rounds\".format(mean_logloss, boost_rounds))\n",
    "    if mean_logloss < min_logloss:\n",
    "        min_logloss = mean_logloss\n",
    "        best_params = (subsample, colsample)\n",
    "\n",
    "print(\"Best params: {}, {}, Logloss: {}\".format(best_params[0], best_params[1], min_logloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params_xgb['subsample'] = 1.0\n",
    "params_xgb['colsample_bytree'] = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with eta=0.3\n",
      "\tLogloss 0.3013444 for 19 rounds\n",
      "CV with eta=0.2\n",
      "\tLogloss 0.3001992 for 35 rounds\n",
      "CV with eta=0.1\n",
      "\tLogloss 0.2986254 for 65 rounds\n",
      "CV with eta=0.05\n",
      "\tLogloss 0.2983642 for 146 rounds\n",
      "CV with eta=0.01\n",
      "\tLogloss 0.2979122 for 737 rounds\n",
      "CV with eta=0.005\n",
      "\tLogloss 0.299106 for 998 rounds\n",
      "Best params: 0.01, Logloss: 0.2979122\n"
     ]
    }
   ],
   "source": [
    "min_logloss = float(\"Inf\")\n",
    "best_params = None\n",
    "\n",
    "for eta in [.3, .2, .1, .05, .01, .005]:\n",
    "    print(\"CV with eta={}\".format(eta))\n",
    "\n",
    "    # We update our parameters\n",
    "    params_xgb['eta'] = eta\n",
    "    \n",
    "    cv_results = xgb.cv(\n",
    "        params_xgb,\n",
    "        xgb.DMatrix(x_train_, label=y_train),\n",
    "        num_boost_round=999,\n",
    "        seed=0,\n",
    "        nfold=5,\n",
    "        metrics='logloss',\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "\n",
    "    # Update best score\n",
    "    mean_logloss = cv_results['test-logloss-mean'].min()\n",
    "    boost_rounds = cv_results['test-logloss-mean'].argmin()\n",
    "    print(\"\\tLogloss {} for {} rounds\".format(mean_logloss, boost_rounds))\n",
    "    if mean_logloss < min_logloss:\n",
    "        min_logloss = mean_logloss\n",
    "        best_params = eta\n",
    "\n",
    "print(\"Best params: {}, Logloss: {}\".format(best_params, min_logloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params_xgb['eta'] = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.7,\n",
       " 'eta': 0.01,\n",
       " 'eval_metric': 'logloss',\n",
       " 'max_depth': 9,\n",
       " 'min_child_weight': 7,\n",
       " 'objective': 'binary:logistic',\n",
       " 'subsample': 1.0}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_xgb_model = xgb.train(\n",
    "    params_xgb,\n",
    "    xgb.DMatrix(x_train_, label=y_train),\n",
    "    num_boost_round=737\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred = best_xgb_model.predict(xgb.DMatrix(x_test_))\n",
    "submission = pd.DataFrame()\n",
    "submission['ID_CORRELATIVO'] = x_test_.index\n",
    "submission['ATTRITION'] = y_pred\n",
    "\n",
    "submission.to_csv('./data/submission15_XGB.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tTrain's binary_logloss: 0.539359\tTest's binary_logloss: 0.540194\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\tTrain's binary_logloss: 0.455093\tTest's binary_logloss: 0.456731\n",
      "[3]\tTrain's binary_logloss: 0.404951\tTest's binary_logloss: 0.407433\n",
      "[4]\tTrain's binary_logloss: 0.3718\tTest's binary_logloss: 0.374736\n",
      "[5]\tTrain's binary_logloss: 0.351384\tTest's binary_logloss: 0.354544\n",
      "[6]\tTrain's binary_logloss: 0.337081\tTest's binary_logloss: 0.340343\n",
      "[7]\tTrain's binary_logloss: 0.326852\tTest's binary_logloss: 0.330458\n",
      "[8]\tTrain's binary_logloss: 0.320217\tTest's binary_logloss: 0.324401\n",
      "[9]\tTrain's binary_logloss: 0.315515\tTest's binary_logloss: 0.320492\n",
      "[10]\tTrain's binary_logloss: 0.311794\tTest's binary_logloss: 0.317005\n",
      "[11]\tTrain's binary_logloss: 0.308954\tTest's binary_logloss: 0.314752\n",
      "[12]\tTrain's binary_logloss: 0.306905\tTest's binary_logloss: 0.313345\n",
      "[13]\tTrain's binary_logloss: 0.304834\tTest's binary_logloss: 0.312147\n",
      "[14]\tTrain's binary_logloss: 0.302411\tTest's binary_logloss: 0.311067\n",
      "[15]\tTrain's binary_logloss: 0.300854\tTest's binary_logloss: 0.310041\n",
      "[16]\tTrain's binary_logloss: 0.29948\tTest's binary_logloss: 0.309735\n",
      "[17]\tTrain's binary_logloss: 0.298369\tTest's binary_logloss: 0.309436\n",
      "[18]\tTrain's binary_logloss: 0.297207\tTest's binary_logloss: 0.309252\n",
      "[19]\tTrain's binary_logloss: 0.296101\tTest's binary_logloss: 0.308486\n",
      "[20]\tTrain's binary_logloss: 0.295075\tTest's binary_logloss: 0.308242\n",
      "[21]\tTrain's binary_logloss: 0.293953\tTest's binary_logloss: 0.308132\n",
      "[22]\tTrain's binary_logloss: 0.292809\tTest's binary_logloss: 0.307675\n",
      "[23]\tTrain's binary_logloss: 0.291636\tTest's binary_logloss: 0.307708\n",
      "[24]\tTrain's binary_logloss: 0.29062\tTest's binary_logloss: 0.307929\n",
      "[25]\tTrain's binary_logloss: 0.289726\tTest's binary_logloss: 0.307824\n",
      "[26]\tTrain's binary_logloss: 0.289251\tTest's binary_logloss: 0.307664\n",
      "[27]\tTrain's binary_logloss: 0.288009\tTest's binary_logloss: 0.307252\n",
      "[28]\tTrain's binary_logloss: 0.286807\tTest's binary_logloss: 0.306942\n",
      "[29]\tTrain's binary_logloss: 0.286004\tTest's binary_logloss: 0.306961\n",
      "[30]\tTrain's binary_logloss: 0.285381\tTest's binary_logloss: 0.306746\n",
      "[31]\tTrain's binary_logloss: 0.284803\tTest's binary_logloss: 0.306594\n",
      "[32]\tTrain's binary_logloss: 0.284136\tTest's binary_logloss: 0.306786\n",
      "[33]\tTrain's binary_logloss: 0.283472\tTest's binary_logloss: 0.306745\n",
      "[34]\tTrain's binary_logloss: 0.283264\tTest's binary_logloss: 0.306753\n",
      "[35]\tTrain's binary_logloss: 0.282069\tTest's binary_logloss: 0.306418\n",
      "[36]\tTrain's binary_logloss: 0.280983\tTest's binary_logloss: 0.306292\n",
      "[37]\tTrain's binary_logloss: 0.280108\tTest's binary_logloss: 0.306387\n",
      "[38]\tTrain's binary_logloss: 0.279555\tTest's binary_logloss: 0.3066\n",
      "[39]\tTrain's binary_logloss: 0.278928\tTest's binary_logloss: 0.306952\n",
      "[40]\tTrain's binary_logloss: 0.27847\tTest's binary_logloss: 0.30725\n",
      "[41]\tTrain's binary_logloss: 0.277693\tTest's binary_logloss: 0.307141\n",
      "[42]\tTrain's binary_logloss: 0.277011\tTest's binary_logloss: 0.307185\n",
      "[43]\tTrain's binary_logloss: 0.276294\tTest's binary_logloss: 0.307251\n",
      "[44]\tTrain's binary_logloss: 0.275794\tTest's binary_logloss: 0.30733\n",
      "[45]\tTrain's binary_logloss: 0.275508\tTest's binary_logloss: 0.307385\n",
      "[46]\tTrain's binary_logloss: 0.274753\tTest's binary_logloss: 0.307328\n",
      "Early stopping, best iteration is:\n",
      "[36]\tTrain's binary_logloss: 0.280983\tTest's binary_logloss: 0.306292\n"
     ]
    }
   ],
   "source": [
    "# Lightgbm\n",
    "\n",
    "dtrain =lgb.Dataset(xtrain,label=ytrain)\n",
    "dtest = lgb.Dataset(xtest,label=ytest)\n",
    "\n",
    "params_lgb = {'objective': 'binary',\n",
    "          'max_depth': 6,\n",
    "          'learning_rate':.3,\n",
    "          'max_bin': 200,\n",
    "          'metric': 'binary_logloss'}\n",
    "\n",
    "\n",
    "lgb_model = lgb.train(params_lgb,\n",
    "                      dtrain,\n",
    "                      num_boost_round=999,\n",
    "                      early_stopping_rounds=10,\n",
    "                      valid_sets=[dtest, dtrain],\n",
    "                      valid_names=[\"Test\", \"Train\"]\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with max_depth=9, min_child_weight=5\n",
      "\tLogloss 0.302213708203 for 34 rounds\n",
      "CV with max_depth=9, min_child_weight=6\n",
      "\tLogloss 0.30242043626 for 41 rounds\n",
      "CV with max_depth=9, min_child_weight=7\n",
      "\tLogloss 0.302346182934 for 37 rounds\n",
      "CV with max_depth=10, min_child_weight=5\n",
      "\tLogloss 0.302547089373 for 31 rounds\n",
      "CV with max_depth=10, min_child_weight=6\n",
      "\tLogloss 0.302485790015 for 27 rounds\n",
      "CV with max_depth=10, min_child_weight=7\n",
      "\tLogloss 0.302089679894 for 37 rounds\n",
      "CV with max_depth=11, min_child_weight=5\n",
      "\tLogloss 0.302185575685 for 37 rounds\n",
      "CV with max_depth=11, min_child_weight=6\n",
      "\tLogloss 0.302545624019 for 28 rounds\n",
      "CV with max_depth=11, min_child_weight=7\n",
      "\tLogloss 0.301524958434 for 35 rounds\n",
      "Best params: 11, 7, Logloss: 0.301524958434\n"
     ]
    }
   ],
   "source": [
    "gridsearch_params = [(max_depth, min_child_weight)\n",
    "                    for max_depth in range(9,12)\n",
    "                    for min_child_weight in range(5,8)]\n",
    "\n",
    "min_logloss = float(\"Inf\")\n",
    "best_params = None\n",
    "\n",
    "for max_depth, min_child_weight in gridsearch_params:\n",
    "    print(\"CV with max_depth={}, min_child_weight={}\".format(\n",
    "                             max_depth,\n",
    "                             min_child_weight))\n",
    "\n",
    "    # Update our parameters\n",
    "    params_lgb['max_depth'] = max_depth\n",
    "    params_lgb['min_child_weight'] = min_child_weight\n",
    "\n",
    "    # Run CV\n",
    "    cv_results = lgb.cv(\n",
    "        params_lgb,\n",
    "        lgb.Dataset(x_train_, label=y_train),\n",
    "        num_boost_round=999,\n",
    "        seed=0,\n",
    "        nfold=5,\n",
    "        metrics=['binary_logloss'],\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "\n",
    "    # Update best logloss\n",
    "    mean_logloss = np.min(cv_results['binary_logloss-mean'])\n",
    "    boost_rounds = len(cv_results['binary_logloss-mean'])\n",
    "    print(\"\\tLogloss {} for {} rounds\".format(mean_logloss, boost_rounds))\n",
    "    if mean_logloss < min_logloss:\n",
    "        min_logloss = mean_logloss\n",
    "        best_params = (max_depth, min_child_weight)\n",
    "\n",
    "print(\"Best params: {}, {}, Logloss: {}\".format(best_params[0], best_params[1], min_logloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params_lgb['max_depth'] = 11\n",
    "params_lgb['min_child_weight'] = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with subsample=1.0, colsample_bytree=1.0\n",
      "\tLogloss 0.301524958434 for 35 rounds\n",
      "CV with subsample=1.0, colsample_bytree=0.9\n",
      "\tLogloss 0.30151304919 for 36 rounds\n",
      "CV with subsample=1.0, colsample_bytree=0.8\n",
      "\tLogloss 0.301618596407 for 30 rounds\n",
      "CV with subsample=1.0, colsample_bytree=0.7\n",
      "\tLogloss 0.301977330712 for 33 rounds\n",
      "CV with subsample=0.9, colsample_bytree=1.0\n",
      "\tLogloss 0.301524958434 for 35 rounds\n",
      "CV with subsample=0.9, colsample_bytree=0.9\n",
      "\tLogloss 0.30151304919 for 36 rounds\n",
      "CV with subsample=0.9, colsample_bytree=0.8\n",
      "\tLogloss 0.301618596407 for 30 rounds\n",
      "CV with subsample=0.9, colsample_bytree=0.7\n",
      "\tLogloss 0.301977330712 for 33 rounds\n",
      "CV with subsample=0.8, colsample_bytree=1.0\n",
      "\tLogloss 0.301524958434 for 35 rounds\n",
      "CV with subsample=0.8, colsample_bytree=0.9\n",
      "\tLogloss 0.30151304919 for 36 rounds\n",
      "CV with subsample=0.8, colsample_bytree=0.8\n",
      "\tLogloss 0.301618596407 for 30 rounds\n",
      "CV with subsample=0.8, colsample_bytree=0.7\n",
      "\tLogloss 0.301977330712 for 33 rounds\n",
      "CV with subsample=0.7, colsample_bytree=1.0\n",
      "\tLogloss 0.301524958434 for 35 rounds\n",
      "CV with subsample=0.7, colsample_bytree=0.9\n",
      "\tLogloss 0.30151304919 for 36 rounds\n",
      "CV with subsample=0.7, colsample_bytree=0.8\n",
      "\tLogloss 0.301618596407 for 30 rounds\n",
      "CV with subsample=0.7, colsample_bytree=0.7\n",
      "\tLogloss 0.301977330712 for 33 rounds\n",
      "Best params: 1.0, 0.9, Logloss: 0.30151304919\n"
     ]
    }
   ],
   "source": [
    "gridsearch_params = [\n",
    "    (subsample, colsample)\n",
    "    for subsample in [i/10. for i in range(7,11)]\n",
    "    for colsample in [i/10. for i in range(7,11)]\n",
    "]\n",
    "\n",
    "min_logloss = float(\"Inf\")\n",
    "best_params = None\n",
    "\n",
    "for subsample, colsample in reversed(gridsearch_params):\n",
    "    print(\"CV with subsample={}, colsample_bytree={}\".format(\n",
    "                             subsample,\n",
    "                             colsample))\n",
    "\n",
    "    # We update our parameters\n",
    "    params_lgb['subsample'] = subsample\n",
    "    params_lgb['colsample_bytree'] = colsample\n",
    "\n",
    "    # Run CV\n",
    "    cv_results = lgb.cv(\n",
    "        params_lgb,\n",
    "        lgb.Dataset(x_train_, label=y_train),\n",
    "        num_boost_round=999,\n",
    "        seed=0,\n",
    "        nfold=5,\n",
    "        metrics=['binary_logloss'],\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "\n",
    "    # Update best logloss\n",
    "    mean_logloss = np.min(cv_results['binary_logloss-mean'])\n",
    "    boost_rounds = len(cv_results['binary_logloss-mean'])\n",
    "    print(\"\\tLogloss {} for {} rounds\".format(mean_logloss, boost_rounds))\n",
    "    if mean_logloss < min_logloss:\n",
    "        min_logloss = mean_logloss\n",
    "        best_params = (subsample, colsample)\n",
    "\n",
    "print(\"Best params: {}, {}, Logloss: {}\".format(best_params[0], best_params[1], min_logloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params_lgb['subsample'] = 1.0\n",
    "params_lgb['colsample_bytree'] = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with eta=0.3\n",
      "\tLogloss 0.30151304919 for 36 rounds\n",
      "CV with eta=0.2\n",
      "\tLogloss 0.300409367962 for 61 rounds\n",
      "CV with eta=0.1\n",
      "\tLogloss 0.299742926475 for 166 rounds\n",
      "CV with eta=0.05\n",
      "\tLogloss 0.299374569008 for 242 rounds\n",
      "CV with eta=0.01\n",
      "\tLogloss 0.298904046246 for 1488 rounds\n",
      "Best params: 0.01, Logloss: 0.298904046246\n"
     ]
    }
   ],
   "source": [
    "min_logloss = float(\"Inf\")\n",
    "best_params = None\n",
    "\n",
    "for eta in [.3, .2, .1, .05, .01]:\n",
    "    print(\"CV with eta={}\".format(eta))\n",
    "\n",
    "    # We update our parameters\n",
    "    params_lgb['learning_rate'] = eta\n",
    "    \n",
    "    # Run CV\n",
    "    cv_results = lgb.cv(\n",
    "        params_lgb,\n",
    "        lgb.Dataset(x_train_, label=y_train),\n",
    "        num_boost_round=5000,\n",
    "        seed=0,\n",
    "        nfold=5,\n",
    "        metrics=['binary_logloss'],\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "\n",
    "    # Update best logloss\n",
    "    mean_logloss = np.min(cv_results['binary_logloss-mean'])\n",
    "    boost_rounds = len(cv_results['binary_logloss-mean'])\n",
    "    print(\"\\tLogloss {} for {} rounds\".format(mean_logloss, boost_rounds))\n",
    "    if mean_logloss < min_logloss:\n",
    "        min_logloss = mean_logloss\n",
    "        best_params = eta\n",
    "\n",
    "print(\"Best params: {}, Logloss: {}\".format(best_params, min_logloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params_lgb['learning_rate'] = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_lgb_model = lgb.train(params_lgb,\n",
    "                      lgb.Dataset(x_train_, label=y_train),\n",
    "                      num_boost_round=1488\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_lgb_model.predict(x_test_)\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission['ID_CORRELATIVO'] = x_test_.index\n",
    "submission['ATTRITION'] = y_pred\n",
    "\n",
    "submission.to_csv('./data/submission15_LGB.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.4777066\ttest: 0.4758057\tbest: 0.4758057 (0)\ttotal: 358ms\tremaining: 42.6s\n",
      "1:\tlearn: 0.4079779\ttest: 0.4079152\tbest: 0.4079152 (1)\ttotal: 540ms\tremaining: 31.8s\n",
      "2:\tlearn: 0.3671615\ttest: 0.3666022\tbest: 0.3666022 (2)\ttotal: 740ms\tremaining: 28.9s\n",
      "3:\tlearn: 0.3451698\ttest: 0.3438190\tbest: 0.3438190 (3)\ttotal: 921ms\tremaining: 26.7s\n",
      "4:\tlearn: 0.3355541\ttest: 0.3346152\tbest: 0.3346152 (4)\ttotal: 1.11s\tremaining: 25.7s\n",
      "5:\tlearn: 0.3288819\ttest: 0.3278035\tbest: 0.3278035 (5)\ttotal: 1.3s\tremaining: 24.7s\n",
      "6:\tlearn: 0.3241006\ttest: 0.3238692\tbest: 0.3238692 (6)\ttotal: 1.51s\tremaining: 24.3s\n",
      "7:\tlearn: 0.3229458\ttest: 0.3227808\tbest: 0.3227808 (7)\ttotal: 1.66s\tremaining: 23.2s\n",
      "8:\tlearn: 0.3196518\ttest: 0.3193553\tbest: 0.3193553 (8)\ttotal: 1.85s\tremaining: 22.9s\n",
      "9:\tlearn: 0.3177976\ttest: 0.3175023\tbest: 0.3175023 (9)\ttotal: 2.03s\tremaining: 22.3s\n",
      "10:\tlearn: 0.3165599\ttest: 0.3163700\tbest: 0.3163700 (10)\ttotal: 2.23s\tremaining: 22.1s\n",
      "11:\tlearn: 0.3150738\ttest: 0.3149589\tbest: 0.3149589 (11)\ttotal: 2.41s\tremaining: 21.7s\n",
      "12:\tlearn: 0.3139239\ttest: 0.3139314\tbest: 0.3139314 (12)\ttotal: 2.62s\tremaining: 21.6s\n",
      "13:\tlearn: 0.3131313\ttest: 0.3134995\tbest: 0.3134995 (13)\ttotal: 2.81s\tremaining: 21.2s\n",
      "14:\tlearn: 0.3119516\ttest: 0.3125059\tbest: 0.3125059 (14)\ttotal: 3.01s\tremaining: 21.1s\n",
      "15:\tlearn: 0.3101899\ttest: 0.3115024\tbest: 0.3115024 (15)\ttotal: 3.19s\tremaining: 20.7s\n",
      "16:\tlearn: 0.3093838\ttest: 0.3108537\tbest: 0.3108537 (16)\ttotal: 3.38s\tremaining: 20.5s\n",
      "17:\tlearn: 0.3087348\ttest: 0.3102654\tbest: 0.3102654 (17)\ttotal: 3.57s\tremaining: 20.2s\n",
      "18:\tlearn: 0.3075995\ttest: 0.3096988\tbest: 0.3096988 (18)\ttotal: 3.77s\tremaining: 20.1s\n",
      "19:\tlearn: 0.3070539\ttest: 0.3090061\tbest: 0.3090061 (19)\ttotal: 3.99s\tremaining: 20s\n",
      "20:\tlearn: 0.3063551\ttest: 0.3081974\tbest: 0.3081974 (20)\ttotal: 4.19s\tremaining: 19.8s\n",
      "21:\tlearn: 0.3058943\ttest: 0.3080423\tbest: 0.3080423 (21)\ttotal: 4.39s\tremaining: 19.6s\n",
      "22:\tlearn: 0.3051723\ttest: 0.3075918\tbest: 0.3075918 (22)\ttotal: 4.6s\tremaining: 19.4s\n",
      "23:\tlearn: 0.3046687\ttest: 0.3074609\tbest: 0.3074609 (23)\ttotal: 4.79s\tremaining: 19.2s\n",
      "24:\tlearn: 0.3045009\ttest: 0.3073817\tbest: 0.3073817 (24)\ttotal: 5.01s\tremaining: 19s\n",
      "25:\tlearn: 0.3042408\ttest: 0.3073037\tbest: 0.3073037 (25)\ttotal: 5.2s\tremaining: 18.8s\n",
      "26:\tlearn: 0.3039113\ttest: 0.3073106\tbest: 0.3073037 (25)\ttotal: 5.4s\tremaining: 18.6s\n",
      "27:\tlearn: 0.3030959\ttest: 0.3066001\tbest: 0.3066001 (27)\ttotal: 5.59s\tremaining: 18.4s\n",
      "28:\tlearn: 0.3028178\ttest: 0.3064822\tbest: 0.3064822 (28)\ttotal: 5.79s\tremaining: 18.2s\n",
      "29:\tlearn: 0.3024818\ttest: 0.3060933\tbest: 0.3060933 (29)\ttotal: 5.97s\tremaining: 17.9s\n",
      "30:\tlearn: 0.3014414\ttest: 0.3055853\tbest: 0.3055853 (30)\ttotal: 6.18s\tremaining: 17.7s\n",
      "31:\tlearn: 0.3011456\ttest: 0.3054556\tbest: 0.3054556 (31)\ttotal: 6.36s\tremaining: 17.5s\n",
      "32:\tlearn: 0.3003676\ttest: 0.3048858\tbest: 0.3048858 (32)\ttotal: 6.55s\tremaining: 17.3s\n",
      "33:\tlearn: 0.3000946\ttest: 0.3049466\tbest: 0.3048858 (32)\ttotal: 6.74s\tremaining: 17.1s\n",
      "34:\tlearn: 0.2998216\ttest: 0.3048528\tbest: 0.3048528 (34)\ttotal: 6.95s\tremaining: 16.9s\n",
      "35:\tlearn: 0.2997715\ttest: 0.3048496\tbest: 0.3048496 (35)\ttotal: 7.14s\tremaining: 16.7s\n",
      "36:\tlearn: 0.2993254\ttest: 0.3047898\tbest: 0.3047898 (36)\ttotal: 7.34s\tremaining: 16.5s\n",
      "37:\tlearn: 0.2988523\ttest: 0.3047774\tbest: 0.3047774 (37)\ttotal: 7.52s\tremaining: 16.2s\n",
      "38:\tlearn: 0.2983009\ttest: 0.3048045\tbest: 0.3047774 (37)\ttotal: 7.72s\tremaining: 16s\n",
      "39:\tlearn: 0.2977654\ttest: 0.3044870\tbest: 0.3044870 (39)\ttotal: 7.92s\tremaining: 15.8s\n",
      "40:\tlearn: 0.2972068\ttest: 0.3041649\tbest: 0.3041649 (40)\ttotal: 8.12s\tremaining: 15.6s\n",
      "41:\tlearn: 0.2967382\ttest: 0.3039944\tbest: 0.3039944 (41)\ttotal: 8.31s\tremaining: 15.4s\n",
      "42:\tlearn: 0.2965259\ttest: 0.3038056\tbest: 0.3038056 (42)\ttotal: 8.51s\tremaining: 15.2s\n",
      "43:\tlearn: 0.2963035\ttest: 0.3035920\tbest: 0.3035920 (43)\ttotal: 8.68s\tremaining: 15s\n",
      "44:\tlearn: 0.2961462\ttest: 0.3035615\tbest: 0.3035615 (44)\ttotal: 8.89s\tremaining: 14.8s\n",
      "45:\tlearn: 0.2958253\ttest: 0.3033568\tbest: 0.3033568 (45)\ttotal: 9.07s\tremaining: 14.6s\n",
      "46:\tlearn: 0.2955103\ttest: 0.3034584\tbest: 0.3033568 (45)\ttotal: 9.27s\tremaining: 14.4s\n",
      "47:\tlearn: 0.2949270\ttest: 0.3030970\tbest: 0.3030970 (47)\ttotal: 9.46s\tremaining: 14.2s\n",
      "48:\tlearn: 0.2948280\ttest: 0.3031465\tbest: 0.3030970 (47)\ttotal: 9.67s\tremaining: 14s\n",
      "49:\tlearn: 0.2945795\ttest: 0.3031932\tbest: 0.3030970 (47)\ttotal: 9.86s\tremaining: 13.8s\n",
      "50:\tlearn: 0.2940366\ttest: 0.3033211\tbest: 0.3030970 (47)\ttotal: 10.1s\tremaining: 13.6s\n",
      "51:\tlearn: 0.2934884\ttest: 0.3034717\tbest: 0.3030970 (47)\ttotal: 10.2s\tremaining: 13.4s\n",
      "52:\tlearn: 0.2931962\ttest: 0.3034964\tbest: 0.3030970 (47)\ttotal: 10.4s\tremaining: 13.2s\n",
      "53:\tlearn: 0.2929019\ttest: 0.3032657\tbest: 0.3030970 (47)\ttotal: 10.6s\tremaining: 13s\n",
      "54:\tlearn: 0.2926504\ttest: 0.3032754\tbest: 0.3030970 (47)\ttotal: 10.8s\tremaining: 12.8s\n",
      "55:\tlearn: 0.2923422\ttest: 0.3031850\tbest: 0.3030970 (47)\ttotal: 11s\tremaining: 12.5s\n",
      "56:\tlearn: 0.2918359\ttest: 0.3030816\tbest: 0.3030816 (56)\ttotal: 11.2s\tremaining: 12.4s\n",
      "57:\tlearn: 0.2916217\ttest: 0.3030776\tbest: 0.3030776 (57)\ttotal: 11.4s\tremaining: 12.1s\n",
      "58:\tlearn: 0.2913155\ttest: 0.3029233\tbest: 0.3029233 (58)\ttotal: 11.6s\tremaining: 11.9s\n",
      "59:\tlearn: 0.2909178\ttest: 0.3026173\tbest: 0.3026173 (59)\ttotal: 11.7s\tremaining: 11.7s\n",
      "60:\tlearn: 0.2906070\ttest: 0.3026999\tbest: 0.3026173 (59)\ttotal: 11.9s\tremaining: 11.5s\n",
      "61:\tlearn: 0.2903992\ttest: 0.3026031\tbest: 0.3026031 (61)\ttotal: 12.1s\tremaining: 11.3s\n",
      "62:\tlearn: 0.2900342\ttest: 0.3025826\tbest: 0.3025826 (62)\ttotal: 12.3s\tremaining: 11.1s\n",
      "63:\tlearn: 0.2894901\ttest: 0.3026698\tbest: 0.3025826 (62)\ttotal: 12.5s\tremaining: 10.9s\n",
      "64:\tlearn: 0.2892408\ttest: 0.3026160\tbest: 0.3025826 (62)\ttotal: 12.7s\tremaining: 10.7s\n",
      "65:\tlearn: 0.2890330\ttest: 0.3025946\tbest: 0.3025826 (62)\ttotal: 12.9s\tremaining: 10.5s\n",
      "66:\tlearn: 0.2885450\ttest: 0.3025797\tbest: 0.3025797 (66)\ttotal: 13.1s\tremaining: 10.3s\n",
      "67:\tlearn: 0.2884328\ttest: 0.3026963\tbest: 0.3025797 (66)\ttotal: 13.3s\tremaining: 10.1s\n",
      "68:\tlearn: 0.2882739\ttest: 0.3026998\tbest: 0.3025797 (66)\ttotal: 13.4s\tremaining: 9.94s\n",
      "69:\tlearn: 0.2881819\ttest: 0.3026679\tbest: 0.3025797 (66)\ttotal: 13.6s\tremaining: 9.74s\n",
      "70:\tlearn: 0.2879742\ttest: 0.3027383\tbest: 0.3025797 (66)\ttotal: 13.9s\tremaining: 9.56s\n",
      "71:\tlearn: 0.2876284\ttest: 0.3027433\tbest: 0.3025797 (66)\ttotal: 14s\tremaining: 9.36s\n",
      "72:\tlearn: 0.2872197\ttest: 0.3025858\tbest: 0.3025797 (66)\ttotal: 14.2s\tremaining: 9.16s\n",
      "73:\tlearn: 0.2870110\ttest: 0.3025541\tbest: 0.3025541 (73)\ttotal: 14.4s\tremaining: 8.96s\n",
      "74:\tlearn: 0.2867757\ttest: 0.3027036\tbest: 0.3025541 (73)\ttotal: 14.6s\tremaining: 8.77s\n",
      "75:\tlearn: 0.2866382\ttest: 0.3027555\tbest: 0.3025541 (73)\ttotal: 14.8s\tremaining: 8.56s\n",
      "76:\tlearn: 0.2862126\ttest: 0.3024584\tbest: 0.3024584 (76)\ttotal: 15s\tremaining: 8.37s\n",
      "77:\tlearn: 0.2859863\ttest: 0.3023104\tbest: 0.3023104 (77)\ttotal: 15.2s\tremaining: 8.16s\n",
      "78:\tlearn: 0.2859108\ttest: 0.3024510\tbest: 0.3023104 (77)\ttotal: 15.4s\tremaining: 7.98s\n",
      "79:\tlearn: 0.2856557\ttest: 0.3022827\tbest: 0.3022827 (79)\ttotal: 15.5s\tremaining: 7.77s\n",
      "80:\tlearn: 0.2856005\ttest: 0.3023101\tbest: 0.3022827 (79)\ttotal: 15.7s\tremaining: 7.58s\n",
      "81:\tlearn: 0.2854211\ttest: 0.3023401\tbest: 0.3022827 (79)\ttotal: 15.9s\tremaining: 7.38s\n",
      "82:\tlearn: 0.2852587\ttest: 0.3022836\tbest: 0.3022827 (79)\ttotal: 16.1s\tremaining: 7.19s\n",
      "83:\tlearn: 0.2849302\ttest: 0.3022894\tbest: 0.3022827 (79)\ttotal: 16.3s\tremaining: 6.99s\n",
      "84:\tlearn: 0.2845647\ttest: 0.3022972\tbest: 0.3022827 (79)\ttotal: 16.5s\tremaining: 6.8s\n",
      "85:\tlearn: 0.2843163\ttest: 0.3024342\tbest: 0.3022827 (79)\ttotal: 16.7s\tremaining: 6.61s\n",
      "86:\tlearn: 0.2840895\ttest: 0.3025984\tbest: 0.3022827 (79)\ttotal: 16.9s\tremaining: 6.42s\n",
      "87:\tlearn: 0.2838512\ttest: 0.3024853\tbest: 0.3022827 (79)\ttotal: 17.1s\tremaining: 6.23s\n",
      "88:\tlearn: 0.2836389\ttest: 0.3023501\tbest: 0.3022827 (79)\ttotal: 17.3s\tremaining: 6.04s\n",
      "89:\tlearn: 0.2834234\ttest: 0.3022259\tbest: 0.3022259 (89)\ttotal: 17.5s\tremaining: 5.85s\n",
      "90:\tlearn: 0.2832553\ttest: 0.3024222\tbest: 0.3022259 (89)\ttotal: 17.8s\tremaining: 5.66s\n",
      "91:\tlearn: 0.2830455\ttest: 0.3022847\tbest: 0.3022259 (89)\ttotal: 17.9s\tremaining: 5.46s\n",
      "92:\tlearn: 0.2827297\ttest: 0.3020847\tbest: 0.3020847 (92)\ttotal: 18.1s\tremaining: 5.27s\n",
      "93:\tlearn: 0.2826322\ttest: 0.3021077\tbest: 0.3020847 (92)\ttotal: 18.3s\tremaining: 5.07s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94:\tlearn: 0.2825531\ttest: 0.3020598\tbest: 0.3020598 (94)\ttotal: 18.5s\tremaining: 4.88s\n",
      "95:\tlearn: 0.2823513\ttest: 0.3019790\tbest: 0.3019790 (95)\ttotal: 18.7s\tremaining: 4.69s\n",
      "96:\tlearn: 0.2821651\ttest: 0.3021053\tbest: 0.3019790 (95)\ttotal: 18.9s\tremaining: 4.49s\n",
      "97:\tlearn: 0.2819552\ttest: 0.3019347\tbest: 0.3019347 (97)\ttotal: 19.1s\tremaining: 4.29s\n",
      "98:\tlearn: 0.2817580\ttest: 0.3018878\tbest: 0.3018878 (98)\ttotal: 19.3s\tremaining: 4.1s\n",
      "99:\tlearn: 0.2816279\ttest: 0.3019574\tbest: 0.3018878 (98)\ttotal: 19.5s\tremaining: 3.9s\n",
      "100:\tlearn: 0.2814723\ttest: 0.3018693\tbest: 0.3018693 (100)\ttotal: 19.7s\tremaining: 3.71s\n",
      "101:\tlearn: 0.2813288\ttest: 0.3020008\tbest: 0.3018693 (100)\ttotal: 19.9s\tremaining: 3.52s\n",
      "102:\tlearn: 0.2811826\ttest: 0.3020371\tbest: 0.3018693 (100)\ttotal: 20.1s\tremaining: 3.32s\n",
      "103:\tlearn: 0.2810546\ttest: 0.3020054\tbest: 0.3018693 (100)\ttotal: 20.3s\tremaining: 3.12s\n",
      "104:\tlearn: 0.2806258\ttest: 0.3017774\tbest: 0.3017774 (104)\ttotal: 20.5s\tremaining: 2.93s\n",
      "105:\tlearn: 0.2803392\ttest: 0.3018654\tbest: 0.3017774 (104)\ttotal: 20.7s\tremaining: 2.73s\n",
      "106:\tlearn: 0.2801431\ttest: 0.3019629\tbest: 0.3017774 (104)\ttotal: 20.9s\tremaining: 2.54s\n",
      "107:\tlearn: 0.2799516\ttest: 0.3019680\tbest: 0.3017774 (104)\ttotal: 21.1s\tremaining: 2.34s\n",
      "108:\tlearn: 0.2798084\ttest: 0.3020511\tbest: 0.3017774 (104)\ttotal: 21.3s\tremaining: 2.15s\n",
      "109:\tlearn: 0.2795529\ttest: 0.3022261\tbest: 0.3017774 (104)\ttotal: 21.5s\tremaining: 1.95s\n",
      "110:\tlearn: 0.2793804\ttest: 0.3023404\tbest: 0.3017774 (104)\ttotal: 21.7s\tremaining: 1.76s\n",
      "111:\tlearn: 0.2790960\ttest: 0.3023346\tbest: 0.3017774 (104)\ttotal: 21.9s\tremaining: 1.56s\n",
      "112:\tlearn: 0.2790222\ttest: 0.3022725\tbest: 0.3017774 (104)\ttotal: 22.1s\tremaining: 1.37s\n",
      "113:\tlearn: 0.2787452\ttest: 0.3023111\tbest: 0.3017774 (104)\ttotal: 22.4s\tremaining: 1.18s\n",
      "114:\tlearn: 0.2786708\ttest: 0.3023120\tbest: 0.3017774 (104)\ttotal: 22.6s\tremaining: 983ms\n",
      "115:\tlearn: 0.2785562\ttest: 0.3023827\tbest: 0.3017774 (104)\ttotal: 22.9s\tremaining: 788ms\n",
      "116:\tlearn: 0.2784175\ttest: 0.3022656\tbest: 0.3017774 (104)\ttotal: 23.1s\tremaining: 592ms\n",
      "117:\tlearn: 0.2782663\ttest: 0.3023367\tbest: 0.3017774 (104)\ttotal: 23.3s\tremaining: 395ms\n",
      "118:\tlearn: 0.2781180\ttest: 0.3024432\tbest: 0.3017774 (104)\ttotal: 23.5s\tremaining: 198ms\n",
      "119:\tlearn: 0.2780830\ttest: 0.3023633\tbest: 0.3017774 (104)\ttotal: 23.7s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.3017774336\n",
      "bestIteration = 104\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core._CatBoostBase at 0xfda2e48>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_ = xtrain.fillna(-999)\n",
    "xtest_ = xtest.fillna(-999)\n",
    "\n",
    "cat = CatBoostClassifier(iterations=120, \n",
    "                         learning_rate=0.3, \n",
    "                         depth=6, \n",
    "                         eval_metric='Logloss',\n",
    "                         l2_leaf_reg=3,\n",
    "                         border_count=32,\n",
    "                         od_type='Iter')\n",
    "\n",
    "cat.fit(xtrain_, ytrain, eval_set=(xtest_, ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from catboost import cv, Pool\n",
    "\n",
    "x_train__ = x_train_.fillna(-999)\n",
    "\n",
    "params_cat = {\n",
    "    'depth': 6,\n",
    "    'learning_rate': .3,\n",
    "    'iterations': 1000,\n",
    "    'loss_function': 'Logloss',\n",
    "    'l2_leaf_reg': 3,\n",
    "    'border_count': 32,\n",
    "    'od_type': 'Iter'\n",
    "}\n",
    "\n",
    "#cv(params, Pool(x_train__, y_train),  partition_random_seed=0, fold_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with depth=4\n",
      "\tLogloss 0.303483883959 for 210 rounds\n",
      "CV with depth=5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-b0d35c00fd94>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mPool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mpartition_random_seed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mfold_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     )\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JERSON\\Anaconda2\\lib\\site-packages\\catboost\\core.pyc\u001b[0m in \u001b[0;36mcv\u001b[1;34m(params, pool, fold_count, inverted, partition_random_seed, shuffle)\u001b[0m\n\u001b[0;32m   1674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1675\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mlog_fixup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_cv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfold_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minverted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartition_random_seed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._cv\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._cv\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gridsearch_params = [depth for depth in range(4,12)]\n",
    "\n",
    "min_logloss = float(\"Inf\")\n",
    "best_params = None\n",
    "\n",
    "for depth in gridsearch_params:\n",
    "    print(\"CV with depth={}\".format(depth))\n",
    "\n",
    "    # Update our parameters\n",
    "    params_cat['depth'] = depth\n",
    "\n",
    "    # Run CV\n",
    "    cv_results = cv(\n",
    "        params_cat,\n",
    "        Pool(x_train__, y_train),  \n",
    "        partition_random_seed=0, \n",
    "        fold_count=5\n",
    "    )\n",
    "\n",
    "    # Update best logloss\n",
    "    mean_logloss = np.min(cv_results['Logloss_test_avg'])\n",
    "    boost_rounds = len(cv_results['Logloss_test_avg'])\n",
    "    print(\"\\tLogloss {} for {} rounds\".format(mean_logloss, boost_rounds))\n",
    "    if mean_logloss < min_logloss:\n",
    "        min_logloss = mean_logloss\n",
    "        best_params = depth\n",
    "        best_rounds = boost_rounds\n",
    "\n",
    "print(\"Best params: {}, Logloss: {}, Rounds: {}\".format(best_params, min_logloss, best_rounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_cat['depth'] = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'border_count': 32,\n",
       " 'depth': 7,\n",
       " 'iterations': 1000,\n",
       " 'l2_leaf_reg': 3,\n",
       " 'learning_rate': 0.3,\n",
       " 'loss_function': 'Logloss',\n",
       " 'od_type': 'Iter'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with leaf_reg=10\n",
      "\tLogloss 0.297993372921 for 405 rounds\n",
      "CV with leaf_reg=100\n",
      "\tLogloss 0.296618841003 for 533 rounds\n",
      "CV with leaf_reg=200\n",
      "\tLogloss 0.296863228118 for 533 rounds\n",
      "CV with leaf_reg=500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-123-11261b083aaf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mPool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mpartition_random_seed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mfold_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     )\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JERSON\\Anaconda2\\lib\\site-packages\\catboost\\core.pyc\u001b[0m in \u001b[0;36mcv\u001b[1;34m(params, pool, fold_count, inverted, partition_random_seed, shuffle)\u001b[0m\n\u001b[0;32m   1674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1675\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mlog_fixup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_cv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfold_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minverted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartition_random_seed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._cv\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._cv\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gridsearch_params = [10,100,200,500]\n",
    "\n",
    "min_logloss = float(\"Inf\")\n",
    "best_params = None\n",
    "\n",
    "for leaf_reg in gridsearch_params:\n",
    "    print(\"CV with leaf_reg={}\".format(leaf_reg))\n",
    "\n",
    "    # Update our parameters\n",
    "    params_cat['l2_leaf_reg'] = leaf_reg\n",
    "\n",
    "    # Run CV\n",
    "    cv_results = cv(\n",
    "        params_cat,\n",
    "        Pool(x_train__, y_train),  \n",
    "        partition_random_seed=0, \n",
    "        fold_count=5\n",
    "    )\n",
    "\n",
    "    # Update best logloss\n",
    "    mean_logloss = np.min(cv_results['Logloss_test_avg'])\n",
    "    boost_rounds = len(cv_results['Logloss_test_avg'])\n",
    "    print(\"\\tLogloss {} for {} rounds\".format(mean_logloss, boost_rounds))\n",
    "    if mean_logloss < min_logloss:\n",
    "        min_logloss = mean_logloss\n",
    "        best_params = leaf_reg\n",
    "        best_rounds = boost_rounds\n",
    "\n",
    "print(\"Best params: {}, Logloss: {}, Rounds: {}\".format(best_params, min_logloss, best_rounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_cat['l2_leaf_reg'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with eta=0.05\n",
      "\tLogloss 0.296545588394 for 1020 rounds\n",
      "Best params: 0.05, Logloss: 0.296545588394, Rounds: 1020\n"
     ]
    }
   ],
   "source": [
    "min_logloss = float(\"Inf\")\n",
    "best_params = None\n",
    "\n",
    "params_cat['iterations'] = 5000\n",
    "\n",
    "for eta in [0.05]:\n",
    "    print(\"CV with eta={}\".format(eta))\n",
    "\n",
    "    # We update our parameters\n",
    "    params_cat['learning_rate'] = eta\n",
    "\n",
    "    # Run CV\n",
    "    cv_results = cv(\n",
    "        params_cat,\n",
    "        Pool(x_train__, y_train),  \n",
    "        partition_random_seed=0, \n",
    "        fold_count=5\n",
    "    )\n",
    "\n",
    "    # Update best logloss\n",
    "    mean_logloss = np.min(cv_results['Logloss_test_avg'])\n",
    "    boost_rounds = len(cv_results['Logloss_test_avg'])\n",
    "    print(\"\\tLogloss {} for {} rounds\".format(mean_logloss, boost_rounds))\n",
    "    if mean_logloss < min_logloss:\n",
    "        min_logloss = mean_logloss\n",
    "        best_params = eta\n",
    "        best_rounds = boost_rounds\n",
    "\n",
    "print(\"Best params: {}, Logloss: {}, Rounds: {}\".format(best_params, min_logloss, best_rounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_cat['iterations'] = 533\n",
    "params_cat['learning_rate'] = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6111281\ttotal: 224ms\tremaining: 1m 59s\n",
      "1:\tlearn: 0.5465069\ttotal: 456ms\tremaining: 2m\n",
      "2:\tlearn: 0.5090004\ttotal: 577ms\tremaining: 1m 41s\n",
      "3:\tlearn: 0.4662903\ttotal: 820ms\tremaining: 1m 48s\n",
      "4:\tlearn: 0.4340754\ttotal: 1.04s\tremaining: 1m 50s\n",
      "5:\tlearn: 0.4128742\ttotal: 1.28s\tremaining: 1m 52s\n",
      "6:\tlearn: 0.3941823\ttotal: 1.52s\tremaining: 1m 53s\n",
      "7:\tlearn: 0.3831947\ttotal: 1.76s\tremaining: 1m 55s\n",
      "8:\tlearn: 0.3723913\ttotal: 2.01s\tremaining: 1m 57s\n",
      "9:\tlearn: 0.3628061\ttotal: 2.27s\tremaining: 1m 58s\n",
      "10:\tlearn: 0.3553447\ttotal: 2.52s\tremaining: 1m 59s\n",
      "11:\tlearn: 0.3507091\ttotal: 2.78s\tremaining: 2m\n",
      "12:\tlearn: 0.3453069\ttotal: 3.03s\tremaining: 2m 1s\n",
      "13:\tlearn: 0.3413203\ttotal: 3.29s\tremaining: 2m 1s\n",
      "14:\tlearn: 0.3376049\ttotal: 3.54s\tremaining: 2m 2s\n",
      "15:\tlearn: 0.3357743\ttotal: 3.78s\tremaining: 2m 2s\n",
      "16:\tlearn: 0.3333883\ttotal: 4.04s\tremaining: 2m 2s\n",
      "17:\tlearn: 0.3304876\ttotal: 4.29s\tremaining: 2m 2s\n",
      "18:\tlearn: 0.3289273\ttotal: 4.54s\tremaining: 2m 2s\n",
      "19:\tlearn: 0.3270711\ttotal: 4.82s\tremaining: 2m 3s\n",
      "20:\tlearn: 0.3258669\ttotal: 5.07s\tremaining: 2m 3s\n",
      "21:\tlearn: 0.3249098\ttotal: 5.37s\tremaining: 2m 4s\n",
      "22:\tlearn: 0.3232015\ttotal: 5.62s\tremaining: 2m 4s\n",
      "23:\tlearn: 0.3217718\ttotal: 5.89s\tremaining: 2m 4s\n",
      "24:\tlearn: 0.3207556\ttotal: 6.14s\tremaining: 2m 4s\n",
      "25:\tlearn: 0.3203782\ttotal: 6.4s\tremaining: 2m 4s\n",
      "26:\tlearn: 0.3195432\ttotal: 6.65s\tremaining: 2m 4s\n",
      "27:\tlearn: 0.3190605\ttotal: 6.91s\tremaining: 2m 4s\n",
      "28:\tlearn: 0.3184373\ttotal: 7.17s\tremaining: 2m 4s\n",
      "29:\tlearn: 0.3176958\ttotal: 7.44s\tremaining: 2m 4s\n",
      "30:\tlearn: 0.3167875\ttotal: 7.66s\tremaining: 2m 4s\n",
      "31:\tlearn: 0.3158652\ttotal: 7.93s\tremaining: 2m 4s\n",
      "32:\tlearn: 0.3151067\ttotal: 8.18s\tremaining: 2m 3s\n",
      "33:\tlearn: 0.3148966\ttotal: 8.43s\tremaining: 2m 3s\n",
      "34:\tlearn: 0.3148645\ttotal: 8.6s\tremaining: 2m 2s\n",
      "35:\tlearn: 0.3143128\ttotal: 8.84s\tremaining: 2m 2s\n",
      "36:\tlearn: 0.3140818\ttotal: 9.08s\tremaining: 2m 1s\n",
      "37:\tlearn: 0.3135314\ttotal: 9.33s\tremaining: 2m 1s\n",
      "38:\tlearn: 0.3128601\ttotal: 9.59s\tremaining: 2m 1s\n",
      "39:\tlearn: 0.3122977\ttotal: 9.84s\tremaining: 2m 1s\n",
      "40:\tlearn: 0.3117412\ttotal: 10.1s\tremaining: 2m 1s\n",
      "41:\tlearn: 0.3112992\ttotal: 10.3s\tremaining: 2m\n",
      "42:\tlearn: 0.3109187\ttotal: 10.6s\tremaining: 2m\n",
      "43:\tlearn: 0.3106307\ttotal: 10.8s\tremaining: 2m\n",
      "44:\tlearn: 0.3103828\ttotal: 11.1s\tremaining: 2m\n",
      "45:\tlearn: 0.3102089\ttotal: 11.4s\tremaining: 2m\n",
      "46:\tlearn: 0.3099800\ttotal: 11.6s\tremaining: 2m\n",
      "47:\tlearn: 0.3096966\ttotal: 11.9s\tremaining: 1m 59s\n",
      "48:\tlearn: 0.3092936\ttotal: 12.1s\tremaining: 1m 59s\n",
      "49:\tlearn: 0.3092366\ttotal: 12.4s\tremaining: 1m 59s\n",
      "50:\tlearn: 0.3088861\ttotal: 12.6s\tremaining: 1m 59s\n",
      "51:\tlearn: 0.3085878\ttotal: 12.8s\tremaining: 1m 58s\n",
      "52:\tlearn: 0.3081571\ttotal: 13.1s\tremaining: 1m 58s\n",
      "53:\tlearn: 0.3079118\ttotal: 13.3s\tremaining: 1m 58s\n",
      "54:\tlearn: 0.3076422\ttotal: 13.6s\tremaining: 1m 57s\n",
      "55:\tlearn: 0.3072238\ttotal: 13.8s\tremaining: 1m 57s\n",
      "56:\tlearn: 0.3071373\ttotal: 14s\tremaining: 1m 57s\n",
      "57:\tlearn: 0.3067836\ttotal: 14.3s\tremaining: 1m 57s\n",
      "58:\tlearn: 0.3066024\ttotal: 14.6s\tremaining: 1m 57s\n",
      "59:\tlearn: 0.3060536\ttotal: 14.8s\tremaining: 1m 56s\n",
      "60:\tlearn: 0.3058461\ttotal: 15s\tremaining: 1m 56s\n",
      "61:\tlearn: 0.3057250\ttotal: 15.3s\tremaining: 1m 56s\n",
      "62:\tlearn: 0.3056446\ttotal: 15.5s\tremaining: 1m 55s\n",
      "63:\tlearn: 0.3055040\ttotal: 15.8s\tremaining: 1m 55s\n",
      "64:\tlearn: 0.3054191\ttotal: 16.1s\tremaining: 1m 55s\n",
      "65:\tlearn: 0.3053286\ttotal: 16.3s\tremaining: 1m 55s\n",
      "66:\tlearn: 0.3050660\ttotal: 16.5s\tremaining: 1m 55s\n",
      "67:\tlearn: 0.3049111\ttotal: 16.8s\tremaining: 1m 55s\n",
      "68:\tlearn: 0.3048668\ttotal: 17.1s\tremaining: 1m 54s\n",
      "69:\tlearn: 0.3048100\ttotal: 17.3s\tremaining: 1m 54s\n",
      "70:\tlearn: 0.3044030\ttotal: 17.6s\tremaining: 1m 54s\n",
      "71:\tlearn: 0.3042755\ttotal: 17.8s\tremaining: 1m 54s\n",
      "72:\tlearn: 0.3040098\ttotal: 18.1s\tremaining: 1m 53s\n",
      "73:\tlearn: 0.3034154\ttotal: 18.3s\tremaining: 1m 53s\n",
      "74:\tlearn: 0.3030313\ttotal: 18.6s\tremaining: 1m 53s\n",
      "75:\tlearn: 0.3027505\ttotal: 18.8s\tremaining: 1m 53s\n",
      "76:\tlearn: 0.3022424\ttotal: 19.1s\tremaining: 1m 53s\n",
      "77:\tlearn: 0.3020719\ttotal: 19.3s\tremaining: 1m 52s\n",
      "78:\tlearn: 0.3019220\ttotal: 19.6s\tremaining: 1m 52s\n",
      "79:\tlearn: 0.3017046\ttotal: 19.9s\tremaining: 1m 52s\n",
      "80:\tlearn: 0.3016002\ttotal: 20.1s\tremaining: 1m 52s\n",
      "81:\tlearn: 0.3015345\ttotal: 20.3s\tremaining: 1m 51s\n",
      "82:\tlearn: 0.3014140\ttotal: 20.6s\tremaining: 1m 51s\n",
      "83:\tlearn: 0.3013081\ttotal: 20.8s\tremaining: 1m 51s\n",
      "84:\tlearn: 0.3011417\ttotal: 21.1s\tremaining: 1m 50s\n",
      "85:\tlearn: 0.3009471\ttotal: 21.3s\tremaining: 1m 50s\n",
      "86:\tlearn: 0.3009190\ttotal: 21.5s\tremaining: 1m 50s\n",
      "87:\tlearn: 0.3005029\ttotal: 21.8s\tremaining: 1m 50s\n",
      "88:\tlearn: 0.3004471\ttotal: 22.1s\tremaining: 1m 50s\n",
      "89:\tlearn: 0.3002875\ttotal: 22.3s\tremaining: 1m 49s\n",
      "90:\tlearn: 0.3002218\ttotal: 22.5s\tremaining: 1m 49s\n",
      "91:\tlearn: 0.3001529\ttotal: 22.8s\tremaining: 1m 49s\n",
      "92:\tlearn: 0.2998571\ttotal: 23s\tremaining: 1m 49s\n",
      "93:\tlearn: 0.2997460\ttotal: 23.3s\tremaining: 1m 48s\n",
      "94:\tlearn: 0.2996754\ttotal: 23.5s\tremaining: 1m 48s\n",
      "95:\tlearn: 0.2995909\ttotal: 23.8s\tremaining: 1m 48s\n",
      "96:\tlearn: 0.2994922\ttotal: 24.1s\tremaining: 1m 48s\n",
      "97:\tlearn: 0.2993494\ttotal: 24.3s\tremaining: 1m 47s\n",
      "98:\tlearn: 0.2992727\ttotal: 24.6s\tremaining: 1m 47s\n",
      "99:\tlearn: 0.2989764\ttotal: 24.8s\tremaining: 1m 47s\n",
      "100:\tlearn: 0.2988401\ttotal: 25s\tremaining: 1m 47s\n",
      "101:\tlearn: 0.2986192\ttotal: 25.3s\tremaining: 1m 46s\n",
      "102:\tlearn: 0.2985664\ttotal: 25.6s\tremaining: 1m 46s\n",
      "103:\tlearn: 0.2983652\ttotal: 25.8s\tremaining: 1m 46s\n",
      "104:\tlearn: 0.2982846\ttotal: 26.1s\tremaining: 1m 46s\n",
      "105:\tlearn: 0.2981819\ttotal: 26.3s\tremaining: 1m 45s\n",
      "106:\tlearn: 0.2980028\ttotal: 26.6s\tremaining: 1m 45s\n",
      "107:\tlearn: 0.2979548\ttotal: 26.8s\tremaining: 1m 45s\n",
      "108:\tlearn: 0.2978787\ttotal: 27.1s\tremaining: 1m 45s\n",
      "109:\tlearn: 0.2977535\ttotal: 27.3s\tremaining: 1m 44s\n",
      "110:\tlearn: 0.2975025\ttotal: 27.6s\tremaining: 1m 44s\n",
      "111:\tlearn: 0.2972665\ttotal: 27.8s\tremaining: 1m 44s\n",
      "112:\tlearn: 0.2970913\ttotal: 28.1s\tremaining: 1m 44s\n",
      "113:\tlearn: 0.2967544\ttotal: 28.3s\tremaining: 1m 44s\n",
      "114:\tlearn: 0.2967323\ttotal: 28.5s\tremaining: 1m 43s\n",
      "115:\tlearn: 0.2966335\ttotal: 28.7s\tremaining: 1m 43s\n",
      "116:\tlearn: 0.2964664\ttotal: 29s\tremaining: 1m 43s\n",
      "117:\tlearn: 0.2963617\ttotal: 29.2s\tremaining: 1m 42s\n",
      "118:\tlearn: 0.2962294\ttotal: 29.5s\tremaining: 1m 42s\n",
      "119:\tlearn: 0.2961619\ttotal: 29.7s\tremaining: 1m 42s\n",
      "120:\tlearn: 0.2959274\ttotal: 30s\tremaining: 1m 42s\n",
      "121:\tlearn: 0.2958185\ttotal: 30.2s\tremaining: 1m 41s\n",
      "122:\tlearn: 0.2957007\ttotal: 30.5s\tremaining: 1m 41s\n",
      "123:\tlearn: 0.2956365\ttotal: 30.7s\tremaining: 1m 41s\n",
      "124:\tlearn: 0.2955627\ttotal: 31s\tremaining: 1m 41s\n",
      "125:\tlearn: 0.2953340\ttotal: 31.3s\tremaining: 1m 40s\n",
      "126:\tlearn: 0.2952847\ttotal: 31.5s\tremaining: 1m 40s\n",
      "127:\tlearn: 0.2951861\ttotal: 31.7s\tremaining: 1m 40s\n",
      "128:\tlearn: 0.2951571\ttotal: 32s\tremaining: 1m 40s\n",
      "129:\tlearn: 0.2948717\ttotal: 32.3s\tremaining: 1m 40s\n",
      "130:\tlearn: 0.2947590\ttotal: 32.5s\tremaining: 1m 39s\n",
      "131:\tlearn: 0.2947225\ttotal: 32.8s\tremaining: 1m 39s\n",
      "132:\tlearn: 0.2944277\ttotal: 33.1s\tremaining: 1m 39s\n",
      "133:\tlearn: 0.2943614\ttotal: 33.3s\tremaining: 1m 39s\n",
      "134:\tlearn: 0.2942992\ttotal: 33.6s\tremaining: 1m 39s\n",
      "135:\tlearn: 0.2941586\ttotal: 33.9s\tremaining: 1m 38s\n",
      "136:\tlearn: 0.2941002\ttotal: 34.1s\tremaining: 1m 38s\n",
      "137:\tlearn: 0.2940160\ttotal: 34.4s\tremaining: 1m 38s\n",
      "138:\tlearn: 0.2939393\ttotal: 34.6s\tremaining: 1m 38s\n",
      "139:\tlearn: 0.2938037\ttotal: 34.9s\tremaining: 1m 38s\n",
      "140:\tlearn: 0.2936603\ttotal: 35.2s\tremaining: 1m 37s\n",
      "141:\tlearn: 0.2936221\ttotal: 35.4s\tremaining: 1m 37s\n",
      "142:\tlearn: 0.2935514\ttotal: 35.7s\tremaining: 1m 37s\n",
      "143:\tlearn: 0.2934483\ttotal: 35.9s\tremaining: 1m 37s\n",
      "144:\tlearn: 0.2933865\ttotal: 36.2s\tremaining: 1m 36s\n",
      "145:\tlearn: 0.2932842\ttotal: 36.5s\tremaining: 1m 36s\n",
      "146:\tlearn: 0.2932337\ttotal: 36.7s\tremaining: 1m 36s\n",
      "147:\tlearn: 0.2931670\ttotal: 37s\tremaining: 1m 36s\n",
      "148:\tlearn: 0.2930097\ttotal: 37.3s\tremaining: 1m 36s\n",
      "149:\tlearn: 0.2929160\ttotal: 37.5s\tremaining: 1m 35s\n",
      "150:\tlearn: 0.2928560\ttotal: 37.8s\tremaining: 1m 35s\n",
      "151:\tlearn: 0.2928292\ttotal: 38.1s\tremaining: 1m 35s\n",
      "152:\tlearn: 0.2926757\ttotal: 38.3s\tremaining: 1m 35s\n",
      "153:\tlearn: 0.2924216\ttotal: 38.6s\tremaining: 1m 35s\n",
      "154:\tlearn: 0.2923317\ttotal: 38.9s\tremaining: 1m 34s\n",
      "155:\tlearn: 0.2922548\ttotal: 39.2s\tremaining: 1m 34s\n",
      "156:\tlearn: 0.2922157\ttotal: 39.4s\tremaining: 1m 34s\n",
      "157:\tlearn: 0.2921852\ttotal: 39.7s\tremaining: 1m 34s\n",
      "158:\tlearn: 0.2921236\ttotal: 40s\tremaining: 1m 34s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159:\tlearn: 0.2920381\ttotal: 40.3s\tremaining: 1m 33s\n",
      "160:\tlearn: 0.2919825\ttotal: 40.5s\tremaining: 1m 33s\n",
      "161:\tlearn: 0.2918649\ttotal: 40.8s\tremaining: 1m 33s\n",
      "162:\tlearn: 0.2918418\ttotal: 41s\tremaining: 1m 33s\n",
      "163:\tlearn: 0.2917334\ttotal: 41.3s\tremaining: 1m 32s\n",
      "164:\tlearn: 0.2916582\ttotal: 41.5s\tremaining: 1m 32s\n",
      "165:\tlearn: 0.2915894\ttotal: 41.8s\tremaining: 1m 32s\n",
      "166:\tlearn: 0.2915015\ttotal: 42s\tremaining: 1m 32s\n",
      "167:\tlearn: 0.2914062\ttotal: 42.3s\tremaining: 1m 31s\n",
      "168:\tlearn: 0.2913199\ttotal: 42.5s\tremaining: 1m 31s\n",
      "169:\tlearn: 0.2912536\ttotal: 42.8s\tremaining: 1m 31s\n",
      "170:\tlearn: 0.2911828\ttotal: 43.1s\tremaining: 1m 31s\n",
      "171:\tlearn: 0.2909550\ttotal: 43.4s\tremaining: 1m 31s\n",
      "172:\tlearn: 0.2909049\ttotal: 43.6s\tremaining: 1m 30s\n",
      "173:\tlearn: 0.2907941\ttotal: 43.9s\tremaining: 1m 30s\n",
      "174:\tlearn: 0.2906935\ttotal: 44.2s\tremaining: 1m 30s\n",
      "175:\tlearn: 0.2906359\ttotal: 44.4s\tremaining: 1m 30s\n",
      "176:\tlearn: 0.2905220\ttotal: 44.6s\tremaining: 1m 29s\n",
      "177:\tlearn: 0.2904600\ttotal: 44.9s\tremaining: 1m 29s\n",
      "178:\tlearn: 0.2902880\ttotal: 45.2s\tremaining: 1m 29s\n",
      "179:\tlearn: 0.2901960\ttotal: 45.4s\tremaining: 1m 29s\n",
      "180:\tlearn: 0.2900990\ttotal: 45.7s\tremaining: 1m 28s\n",
      "181:\tlearn: 0.2900331\ttotal: 46s\tremaining: 1m 28s\n",
      "182:\tlearn: 0.2899508\ttotal: 46.2s\tremaining: 1m 28s\n",
      "183:\tlearn: 0.2898489\ttotal: 46.5s\tremaining: 1m 28s\n",
      "184:\tlearn: 0.2897909\ttotal: 46.7s\tremaining: 1m 27s\n",
      "185:\tlearn: 0.2897408\ttotal: 47s\tremaining: 1m 27s\n",
      "186:\tlearn: 0.2896475\ttotal: 47.3s\tremaining: 1m 27s\n",
      "187:\tlearn: 0.2895901\ttotal: 47.5s\tremaining: 1m 27s\n",
      "188:\tlearn: 0.2894639\ttotal: 47.8s\tremaining: 1m 26s\n",
      "189:\tlearn: 0.2893961\ttotal: 48s\tremaining: 1m 26s\n",
      "190:\tlearn: 0.2892912\ttotal: 48.3s\tremaining: 1m 26s\n",
      "191:\tlearn: 0.2891191\ttotal: 48.5s\tremaining: 1m 26s\n",
      "192:\tlearn: 0.2890962\ttotal: 48.8s\tremaining: 1m 25s\n",
      "193:\tlearn: 0.2890250\ttotal: 49s\tremaining: 1m 25s\n",
      "194:\tlearn: 0.2889579\ttotal: 49.3s\tremaining: 1m 25s\n",
      "195:\tlearn: 0.2889016\ttotal: 49.5s\tremaining: 1m 25s\n",
      "196:\tlearn: 0.2888286\ttotal: 49.8s\tremaining: 1m 24s\n",
      "197:\tlearn: 0.2887468\ttotal: 50s\tremaining: 1m 24s\n",
      "198:\tlearn: 0.2886888\ttotal: 50.3s\tremaining: 1m 24s\n",
      "199:\tlearn: 0.2886077\ttotal: 50.5s\tremaining: 1m 24s\n",
      "200:\tlearn: 0.2885813\ttotal: 50.8s\tremaining: 1m 23s\n",
      "201:\tlearn: 0.2885047\ttotal: 51s\tremaining: 1m 23s\n",
      "202:\tlearn: 0.2884298\ttotal: 51.3s\tremaining: 1m 23s\n",
      "203:\tlearn: 0.2883582\ttotal: 51.6s\tremaining: 1m 23s\n",
      "204:\tlearn: 0.2882603\ttotal: 51.8s\tremaining: 1m 22s\n",
      "205:\tlearn: 0.2881184\ttotal: 52.1s\tremaining: 1m 22s\n",
      "206:\tlearn: 0.2880773\ttotal: 52.4s\tremaining: 1m 22s\n",
      "207:\tlearn: 0.2880197\ttotal: 52.7s\tremaining: 1m 22s\n",
      "208:\tlearn: 0.2880036\ttotal: 52.9s\tremaining: 1m 22s\n",
      "209:\tlearn: 0.2879835\ttotal: 53.2s\tremaining: 1m 21s\n",
      "210:\tlearn: 0.2879091\ttotal: 53.5s\tremaining: 1m 21s\n",
      "211:\tlearn: 0.2878488\ttotal: 53.8s\tremaining: 1m 21s\n",
      "212:\tlearn: 0.2878098\ttotal: 54.1s\tremaining: 1m 21s\n",
      "213:\tlearn: 0.2877255\ttotal: 54.3s\tremaining: 1m 20s\n",
      "214:\tlearn: 0.2876785\ttotal: 54.6s\tremaining: 1m 20s\n",
      "215:\tlearn: 0.2876273\ttotal: 54.9s\tremaining: 1m 20s\n",
      "216:\tlearn: 0.2875576\ttotal: 55.1s\tremaining: 1m 20s\n",
      "217:\tlearn: 0.2875295\ttotal: 55.4s\tremaining: 1m 20s\n",
      "218:\tlearn: 0.2874923\ttotal: 55.7s\tremaining: 1m 19s\n",
      "219:\tlearn: 0.2874284\ttotal: 56s\tremaining: 1m 19s\n",
      "220:\tlearn: 0.2873365\ttotal: 56.2s\tremaining: 1m 19s\n",
      "221:\tlearn: 0.2872652\ttotal: 56.5s\tremaining: 1m 19s\n",
      "222:\tlearn: 0.2871740\ttotal: 56.8s\tremaining: 1m 18s\n",
      "223:\tlearn: 0.2871387\ttotal: 57s\tremaining: 1m 18s\n",
      "224:\tlearn: 0.2871247\ttotal: 57.3s\tremaining: 1m 18s\n",
      "225:\tlearn: 0.2869054\ttotal: 57.6s\tremaining: 1m 18s\n",
      "226:\tlearn: 0.2868286\ttotal: 57.8s\tremaining: 1m 17s\n",
      "227:\tlearn: 0.2867725\ttotal: 58.1s\tremaining: 1m 17s\n",
      "228:\tlearn: 0.2867392\ttotal: 58.4s\tremaining: 1m 17s\n",
      "229:\tlearn: 0.2866678\ttotal: 58.7s\tremaining: 1m 17s\n",
      "230:\tlearn: 0.2866295\ttotal: 59s\tremaining: 1m 17s\n",
      "231:\tlearn: 0.2865886\ttotal: 59.3s\tremaining: 1m 16s\n",
      "232:\tlearn: 0.2865720\ttotal: 59.5s\tremaining: 1m 16s\n",
      "233:\tlearn: 0.2864981\ttotal: 59.9s\tremaining: 1m 16s\n",
      "234:\tlearn: 0.2864549\ttotal: 1m\tremaining: 1m 16s\n",
      "235:\tlearn: 0.2863103\ttotal: 1m\tremaining: 1m 16s\n",
      "236:\tlearn: 0.2862093\ttotal: 1m\tremaining: 1m 15s\n",
      "237:\tlearn: 0.2861944\ttotal: 1m 1s\tremaining: 1m 15s\n",
      "238:\tlearn: 0.2860743\ttotal: 1m 1s\tremaining: 1m 15s\n",
      "239:\tlearn: 0.2860330\ttotal: 1m 1s\tremaining: 1m 15s\n",
      "240:\tlearn: 0.2859073\ttotal: 1m 1s\tremaining: 1m 15s\n",
      "241:\tlearn: 0.2858679\ttotal: 1m 2s\tremaining: 1m 14s\n",
      "242:\tlearn: 0.2858056\ttotal: 1m 2s\tremaining: 1m 14s\n",
      "243:\tlearn: 0.2857479\ttotal: 1m 2s\tremaining: 1m 14s\n",
      "244:\tlearn: 0.2856583\ttotal: 1m 3s\tremaining: 1m 14s\n",
      "245:\tlearn: 0.2856135\ttotal: 1m 3s\tremaining: 1m 13s\n",
      "246:\tlearn: 0.2855631\ttotal: 1m 3s\tremaining: 1m 13s\n",
      "247:\tlearn: 0.2855450\ttotal: 1m 3s\tremaining: 1m 13s\n",
      "248:\tlearn: 0.2854752\ttotal: 1m 4s\tremaining: 1m 13s\n",
      "249:\tlearn: 0.2854290\ttotal: 1m 4s\tremaining: 1m 13s\n",
      "250:\tlearn: 0.2853902\ttotal: 1m 4s\tremaining: 1m 12s\n",
      "251:\tlearn: 0.2853096\ttotal: 1m 5s\tremaining: 1m 12s\n",
      "252:\tlearn: 0.2852495\ttotal: 1m 5s\tremaining: 1m 12s\n",
      "253:\tlearn: 0.2851753\ttotal: 1m 5s\tremaining: 1m 12s\n",
      "254:\tlearn: 0.2851057\ttotal: 1m 5s\tremaining: 1m 11s\n",
      "255:\tlearn: 0.2850324\ttotal: 1m 6s\tremaining: 1m 11s\n",
      "256:\tlearn: 0.2849418\ttotal: 1m 6s\tremaining: 1m 11s\n",
      "257:\tlearn: 0.2849012\ttotal: 1m 6s\tremaining: 1m 11s\n",
      "258:\tlearn: 0.2848475\ttotal: 1m 7s\tremaining: 1m 10s\n",
      "259:\tlearn: 0.2847470\ttotal: 1m 7s\tremaining: 1m 10s\n",
      "260:\tlearn: 0.2846860\ttotal: 1m 7s\tremaining: 1m 10s\n",
      "261:\tlearn: 0.2845941\ttotal: 1m 7s\tremaining: 1m 10s\n",
      "262:\tlearn: 0.2844809\ttotal: 1m 8s\tremaining: 1m 9s\n",
      "263:\tlearn: 0.2843808\ttotal: 1m 8s\tremaining: 1m 9s\n",
      "264:\tlearn: 0.2843066\ttotal: 1m 8s\tremaining: 1m 9s\n",
      "265:\tlearn: 0.2842491\ttotal: 1m 8s\tremaining: 1m 9s\n",
      "266:\tlearn: 0.2841976\ttotal: 1m 9s\tremaining: 1m 8s\n",
      "267:\tlearn: 0.2841839\ttotal: 1m 9s\tremaining: 1m 8s\n",
      "268:\tlearn: 0.2841398\ttotal: 1m 9s\tremaining: 1m 8s\n",
      "269:\tlearn: 0.2841235\ttotal: 1m 10s\tremaining: 1m 8s\n",
      "270:\tlearn: 0.2840589\ttotal: 1m 10s\tremaining: 1m 8s\n",
      "271:\tlearn: 0.2840482\ttotal: 1m 10s\tremaining: 1m 7s\n",
      "272:\tlearn: 0.2839899\ttotal: 1m 10s\tremaining: 1m 7s\n",
      "273:\tlearn: 0.2839413\ttotal: 1m 11s\tremaining: 1m 7s\n",
      "274:\tlearn: 0.2838943\ttotal: 1m 11s\tremaining: 1m 7s\n",
      "275:\tlearn: 0.2838642\ttotal: 1m 11s\tremaining: 1m 6s\n",
      "276:\tlearn: 0.2837688\ttotal: 1m 12s\tremaining: 1m 6s\n",
      "277:\tlearn: 0.2837487\ttotal: 1m 12s\tremaining: 1m 6s\n",
      "278:\tlearn: 0.2836822\ttotal: 1m 12s\tremaining: 1m 6s\n",
      "279:\tlearn: 0.2836261\ttotal: 1m 13s\tremaining: 1m 6s\n",
      "280:\tlearn: 0.2834957\ttotal: 1m 13s\tremaining: 1m 5s\n",
      "281:\tlearn: 0.2834578\ttotal: 1m 13s\tremaining: 1m 5s\n",
      "282:\tlearn: 0.2833879\ttotal: 1m 13s\tremaining: 1m 5s\n",
      "283:\tlearn: 0.2833173\ttotal: 1m 14s\tremaining: 1m 5s\n",
      "284:\tlearn: 0.2832982\ttotal: 1m 14s\tremaining: 1m 4s\n",
      "285:\tlearn: 0.2832536\ttotal: 1m 14s\tremaining: 1m 4s\n",
      "286:\tlearn: 0.2832144\ttotal: 1m 15s\tremaining: 1m 4s\n",
      "287:\tlearn: 0.2831852\ttotal: 1m 15s\tremaining: 1m 4s\n",
      "288:\tlearn: 0.2831332\ttotal: 1m 15s\tremaining: 1m 3s\n",
      "289:\tlearn: 0.2830696\ttotal: 1m 15s\tremaining: 1m 3s\n",
      "290:\tlearn: 0.2830367\ttotal: 1m 16s\tremaining: 1m 3s\n",
      "291:\tlearn: 0.2830195\ttotal: 1m 16s\tremaining: 1m 3s\n",
      "292:\tlearn: 0.2829822\ttotal: 1m 16s\tremaining: 1m 2s\n",
      "293:\tlearn: 0.2829234\ttotal: 1m 16s\tremaining: 1m 2s\n",
      "294:\tlearn: 0.2828954\ttotal: 1m 17s\tremaining: 1m 2s\n",
      "295:\tlearn: 0.2828580\ttotal: 1m 17s\tremaining: 1m 2s\n",
      "296:\tlearn: 0.2827819\ttotal: 1m 17s\tremaining: 1m 1s\n",
      "297:\tlearn: 0.2826658\ttotal: 1m 18s\tremaining: 1m 1s\n",
      "298:\tlearn: 0.2825321\ttotal: 1m 18s\tremaining: 1m 1s\n",
      "299:\tlearn: 0.2824871\ttotal: 1m 18s\tremaining: 1m 1s\n",
      "300:\tlearn: 0.2824270\ttotal: 1m 18s\tremaining: 1m\n",
      "301:\tlearn: 0.2823031\ttotal: 1m 19s\tremaining: 1m\n",
      "302:\tlearn: 0.2822450\ttotal: 1m 19s\tremaining: 1m\n",
      "303:\tlearn: 0.2821603\ttotal: 1m 19s\tremaining: 60s\n",
      "304:\tlearn: 0.2820882\ttotal: 1m 19s\tremaining: 59.7s\n",
      "305:\tlearn: 0.2819919\ttotal: 1m 20s\tremaining: 59.5s\n",
      "306:\tlearn: 0.2819778\ttotal: 1m 20s\tremaining: 59.2s\n",
      "307:\tlearn: 0.2818561\ttotal: 1m 20s\tremaining: 59s\n",
      "308:\tlearn: 0.2817624\ttotal: 1m 21s\tremaining: 58.7s\n",
      "309:\tlearn: 0.2816842\ttotal: 1m 21s\tremaining: 58.5s\n",
      "310:\tlearn: 0.2816602\ttotal: 1m 21s\tremaining: 58.2s\n",
      "311:\tlearn: 0.2816063\ttotal: 1m 21s\tremaining: 57.9s\n",
      "312:\tlearn: 0.2815138\ttotal: 1m 22s\tremaining: 57.7s\n",
      "313:\tlearn: 0.2814657\ttotal: 1m 22s\tremaining: 57.5s\n",
      "314:\tlearn: 0.2814078\ttotal: 1m 22s\tremaining: 57.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315:\tlearn: 0.2813414\ttotal: 1m 22s\tremaining: 56.9s\n",
      "316:\tlearn: 0.2812428\ttotal: 1m 23s\tremaining: 56.7s\n",
      "317:\tlearn: 0.2812041\ttotal: 1m 23s\tremaining: 56.4s\n",
      "318:\tlearn: 0.2811285\ttotal: 1m 23s\tremaining: 56.2s\n",
      "319:\tlearn: 0.2810411\ttotal: 1m 24s\tremaining: 55.9s\n",
      "320:\tlearn: 0.2809743\ttotal: 1m 24s\tremaining: 55.7s\n",
      "321:\tlearn: 0.2808922\ttotal: 1m 24s\tremaining: 55.4s\n",
      "322:\tlearn: 0.2807952\ttotal: 1m 24s\tremaining: 55.2s\n",
      "323:\tlearn: 0.2807237\ttotal: 1m 25s\tremaining: 54.9s\n",
      "324:\tlearn: 0.2806704\ttotal: 1m 25s\tremaining: 54.7s\n",
      "325:\tlearn: 0.2806008\ttotal: 1m 25s\tremaining: 54.4s\n",
      "326:\tlearn: 0.2805077\ttotal: 1m 25s\tremaining: 54.1s\n",
      "327:\tlearn: 0.2804466\ttotal: 1m 26s\tremaining: 53.9s\n",
      "328:\tlearn: 0.2803986\ttotal: 1m 26s\tremaining: 53.6s\n",
      "329:\tlearn: 0.2803382\ttotal: 1m 26s\tremaining: 53.4s\n",
      "330:\tlearn: 0.2802612\ttotal: 1m 27s\tremaining: 53.2s\n",
      "331:\tlearn: 0.2801919\ttotal: 1m 27s\tremaining: 52.9s\n",
      "332:\tlearn: 0.2801523\ttotal: 1m 27s\tremaining: 52.7s\n",
      "333:\tlearn: 0.2800554\ttotal: 1m 27s\tremaining: 52.4s\n",
      "334:\tlearn: 0.2800004\ttotal: 1m 28s\tremaining: 52.1s\n",
      "335:\tlearn: 0.2799754\ttotal: 1m 28s\tremaining: 51.9s\n",
      "336:\tlearn: 0.2799463\ttotal: 1m 28s\tremaining: 51.7s\n",
      "337:\tlearn: 0.2798468\ttotal: 1m 29s\tremaining: 51.4s\n",
      "338:\tlearn: 0.2797998\ttotal: 1m 29s\tremaining: 51.1s\n",
      "339:\tlearn: 0.2797724\ttotal: 1m 29s\tremaining: 50.9s\n",
      "340:\tlearn: 0.2797072\ttotal: 1m 29s\tremaining: 50.6s\n",
      "341:\tlearn: 0.2796062\ttotal: 1m 30s\tremaining: 50.4s\n",
      "342:\tlearn: 0.2795560\ttotal: 1m 30s\tremaining: 50.1s\n",
      "343:\tlearn: 0.2795123\ttotal: 1m 30s\tremaining: 49.9s\n",
      "344:\tlearn: 0.2794771\ttotal: 1m 31s\tremaining: 49.7s\n",
      "345:\tlearn: 0.2794213\ttotal: 1m 31s\tremaining: 49.4s\n",
      "346:\tlearn: 0.2793590\ttotal: 1m 31s\tremaining: 49.1s\n",
      "347:\tlearn: 0.2793325\ttotal: 1m 31s\tremaining: 48.9s\n",
      "348:\tlearn: 0.2792917\ttotal: 1m 32s\tremaining: 48.7s\n",
      "349:\tlearn: 0.2792258\ttotal: 1m 32s\tremaining: 48.4s\n",
      "350:\tlearn: 0.2790949\ttotal: 1m 32s\tremaining: 48.1s\n",
      "351:\tlearn: 0.2790274\ttotal: 1m 33s\tremaining: 47.9s\n",
      "352:\tlearn: 0.2789217\ttotal: 1m 33s\tremaining: 47.6s\n",
      "353:\tlearn: 0.2788727\ttotal: 1m 33s\tremaining: 47.3s\n",
      "354:\tlearn: 0.2788325\ttotal: 1m 33s\tremaining: 47.1s\n",
      "355:\tlearn: 0.2787385\ttotal: 1m 34s\tremaining: 46.8s\n",
      "356:\tlearn: 0.2786866\ttotal: 1m 34s\tremaining: 46.6s\n",
      "357:\tlearn: 0.2786722\ttotal: 1m 34s\tremaining: 46.3s\n",
      "358:\tlearn: 0.2786468\ttotal: 1m 34s\tremaining: 46s\n",
      "359:\tlearn: 0.2785990\ttotal: 1m 35s\tremaining: 45.8s\n",
      "360:\tlearn: 0.2785538\ttotal: 1m 35s\tremaining: 45.5s\n",
      "361:\tlearn: 0.2784536\ttotal: 1m 35s\tremaining: 45.2s\n",
      "362:\tlearn: 0.2783871\ttotal: 1m 36s\tremaining: 45s\n",
      "363:\tlearn: 0.2783588\ttotal: 1m 36s\tremaining: 44.7s\n",
      "364:\tlearn: 0.2783422\ttotal: 1m 36s\tremaining: 44.4s\n",
      "365:\tlearn: 0.2782880\ttotal: 1m 36s\tremaining: 44.2s\n",
      "366:\tlearn: 0.2782756\ttotal: 1m 37s\tremaining: 43.9s\n",
      "367:\tlearn: 0.2782546\ttotal: 1m 37s\tremaining: 43.6s\n",
      "368:\tlearn: 0.2782113\ttotal: 1m 37s\tremaining: 43.4s\n",
      "369:\tlearn: 0.2781562\ttotal: 1m 37s\tremaining: 43.1s\n",
      "370:\tlearn: 0.2781292\ttotal: 1m 38s\tremaining: 42.8s\n",
      "371:\tlearn: 0.2780698\ttotal: 1m 38s\tremaining: 42.6s\n",
      "372:\tlearn: 0.2780470\ttotal: 1m 38s\tremaining: 42.3s\n",
      "373:\tlearn: 0.2779921\ttotal: 1m 38s\tremaining: 42.1s\n",
      "374:\tlearn: 0.2779028\ttotal: 1m 39s\tremaining: 41.8s\n",
      "375:\tlearn: 0.2778303\ttotal: 1m 39s\tremaining: 41.6s\n",
      "376:\tlearn: 0.2777559\ttotal: 1m 39s\tremaining: 41.3s\n",
      "377:\tlearn: 0.2777119\ttotal: 1m 40s\tremaining: 41s\n",
      "378:\tlearn: 0.2776641\ttotal: 1m 40s\tremaining: 40.8s\n",
      "379:\tlearn: 0.2775994\ttotal: 1m 40s\tremaining: 40.5s\n",
      "380:\tlearn: 0.2775631\ttotal: 1m 40s\tremaining: 40.3s\n",
      "381:\tlearn: 0.2775034\ttotal: 1m 41s\tremaining: 40s\n",
      "382:\tlearn: 0.2774607\ttotal: 1m 41s\tremaining: 39.7s\n",
      "383:\tlearn: 0.2774305\ttotal: 1m 41s\tremaining: 39.5s\n",
      "384:\tlearn: 0.2773956\ttotal: 1m 41s\tremaining: 39.2s\n",
      "385:\tlearn: 0.2773356\ttotal: 1m 42s\tremaining: 38.9s\n",
      "386:\tlearn: 0.2772898\ttotal: 1m 42s\tremaining: 38.7s\n",
      "387:\tlearn: 0.2772187\ttotal: 1m 42s\tremaining: 38.4s\n",
      "388:\tlearn: 0.2771606\ttotal: 1m 43s\tremaining: 38.1s\n",
      "389:\tlearn: 0.2771120\ttotal: 1m 43s\tremaining: 37.9s\n",
      "390:\tlearn: 0.2770343\ttotal: 1m 43s\tremaining: 37.6s\n",
      "391:\tlearn: 0.2769372\ttotal: 1m 43s\tremaining: 37.4s\n",
      "392:\tlearn: 0.2768995\ttotal: 1m 44s\tremaining: 37.1s\n",
      "393:\tlearn: 0.2768097\ttotal: 1m 44s\tremaining: 36.9s\n",
      "394:\tlearn: 0.2767669\ttotal: 1m 44s\tremaining: 36.6s\n",
      "395:\tlearn: 0.2766960\ttotal: 1m 45s\tremaining: 36.4s\n",
      "396:\tlearn: 0.2766318\ttotal: 1m 45s\tremaining: 36.1s\n",
      "397:\tlearn: 0.2766215\ttotal: 1m 45s\tremaining: 35.8s\n",
      "398:\tlearn: 0.2765811\ttotal: 1m 45s\tremaining: 35.6s\n",
      "399:\tlearn: 0.2765167\ttotal: 1m 46s\tremaining: 35.3s\n",
      "400:\tlearn: 0.2764920\ttotal: 1m 46s\tremaining: 35s\n",
      "401:\tlearn: 0.2763963\ttotal: 1m 46s\tremaining: 34.8s\n",
      "402:\tlearn: 0.2763422\ttotal: 1m 47s\tremaining: 34.5s\n",
      "403:\tlearn: 0.2763046\ttotal: 1m 47s\tremaining: 34.3s\n",
      "404:\tlearn: 0.2762790\ttotal: 1m 47s\tremaining: 34s\n",
      "405:\tlearn: 0.2762245\ttotal: 1m 47s\tremaining: 33.7s\n",
      "406:\tlearn: 0.2761740\ttotal: 1m 48s\tremaining: 33.5s\n",
      "407:\tlearn: 0.2761405\ttotal: 1m 48s\tremaining: 33.2s\n",
      "408:\tlearn: 0.2760458\ttotal: 1m 48s\tremaining: 32.9s\n",
      "409:\tlearn: 0.2760071\ttotal: 1m 48s\tremaining: 32.7s\n",
      "410:\tlearn: 0.2759697\ttotal: 1m 49s\tremaining: 32.4s\n",
      "411:\tlearn: 0.2759527\ttotal: 1m 49s\tremaining: 32.1s\n",
      "412:\tlearn: 0.2759212\ttotal: 1m 49s\tremaining: 31.9s\n",
      "413:\tlearn: 0.2758649\ttotal: 1m 49s\tremaining: 31.6s\n",
      "414:\tlearn: 0.2758140\ttotal: 1m 50s\tremaining: 31.3s\n",
      "415:\tlearn: 0.2757392\ttotal: 1m 50s\tremaining: 31.1s\n",
      "416:\tlearn: 0.2756983\ttotal: 1m 50s\tremaining: 30.8s\n",
      "417:\tlearn: 0.2756656\ttotal: 1m 50s\tremaining: 30.5s\n",
      "418:\tlearn: 0.2756191\ttotal: 1m 51s\tremaining: 30.3s\n",
      "419:\tlearn: 0.2755464\ttotal: 1m 51s\tremaining: 30s\n",
      "420:\tlearn: 0.2754622\ttotal: 1m 51s\tremaining: 29.7s\n",
      "421:\tlearn: 0.2754277\ttotal: 1m 52s\tremaining: 29.5s\n",
      "422:\tlearn: 0.2753826\ttotal: 1m 52s\tremaining: 29.2s\n",
      "423:\tlearn: 0.2753444\ttotal: 1m 52s\tremaining: 28.9s\n",
      "424:\tlearn: 0.2753016\ttotal: 1m 52s\tremaining: 28.7s\n",
      "425:\tlearn: 0.2751845\ttotal: 1m 53s\tremaining: 28.4s\n",
      "426:\tlearn: 0.2751166\ttotal: 1m 53s\tremaining: 28.1s\n",
      "427:\tlearn: 0.2750695\ttotal: 1m 53s\tremaining: 27.9s\n",
      "428:\tlearn: 0.2750254\ttotal: 1m 53s\tremaining: 27.6s\n",
      "429:\tlearn: 0.2749741\ttotal: 1m 54s\tremaining: 27.4s\n",
      "430:\tlearn: 0.2749486\ttotal: 1m 54s\tremaining: 27.1s\n",
      "431:\tlearn: 0.2748958\ttotal: 1m 54s\tremaining: 26.8s\n",
      "432:\tlearn: 0.2748476\ttotal: 1m 54s\tremaining: 26.6s\n",
      "433:\tlearn: 0.2747707\ttotal: 1m 55s\tremaining: 26.3s\n",
      "434:\tlearn: 0.2747207\ttotal: 1m 55s\tremaining: 26s\n",
      "435:\tlearn: 0.2746915\ttotal: 1m 55s\tremaining: 25.8s\n",
      "436:\tlearn: 0.2746398\ttotal: 1m 56s\tremaining: 25.5s\n",
      "437:\tlearn: 0.2746063\ttotal: 1m 56s\tremaining: 25.2s\n",
      "438:\tlearn: 0.2745063\ttotal: 1m 56s\tremaining: 25s\n",
      "439:\tlearn: 0.2744329\ttotal: 1m 56s\tremaining: 24.7s\n",
      "440:\tlearn: 0.2743691\ttotal: 1m 57s\tremaining: 24.4s\n",
      "441:\tlearn: 0.2742746\ttotal: 1m 57s\tremaining: 24.2s\n",
      "442:\tlearn: 0.2742161\ttotal: 1m 57s\tremaining: 23.9s\n",
      "443:\tlearn: 0.2741855\ttotal: 1m 57s\tremaining: 23.6s\n",
      "444:\tlearn: 0.2741343\ttotal: 1m 58s\tremaining: 23.4s\n",
      "445:\tlearn: 0.2740915\ttotal: 1m 58s\tremaining: 23.1s\n",
      "446:\tlearn: 0.2739966\ttotal: 1m 58s\tremaining: 22.8s\n",
      "447:\tlearn: 0.2739031\ttotal: 1m 59s\tremaining: 22.6s\n",
      "448:\tlearn: 0.2738548\ttotal: 1m 59s\tremaining: 22.3s\n",
      "449:\tlearn: 0.2737920\ttotal: 1m 59s\tremaining: 22s\n",
      "450:\tlearn: 0.2737420\ttotal: 1m 59s\tremaining: 21.8s\n",
      "451:\tlearn: 0.2736817\ttotal: 2m\tremaining: 21.5s\n",
      "452:\tlearn: 0.2736035\ttotal: 2m\tremaining: 21.3s\n",
      "453:\tlearn: 0.2735527\ttotal: 2m\tremaining: 21s\n",
      "454:\tlearn: 0.2735134\ttotal: 2m\tremaining: 20.7s\n",
      "455:\tlearn: 0.2734569\ttotal: 2m 1s\tremaining: 20.5s\n",
      "456:\tlearn: 0.2734177\ttotal: 2m 1s\tremaining: 20.2s\n",
      "457:\tlearn: 0.2733741\ttotal: 2m 1s\tremaining: 19.9s\n",
      "458:\tlearn: 0.2733483\ttotal: 2m 1s\tremaining: 19.7s\n",
      "459:\tlearn: 0.2732936\ttotal: 2m 2s\tremaining: 19.4s\n",
      "460:\tlearn: 0.2732484\ttotal: 2m 2s\tremaining: 19.1s\n",
      "461:\tlearn: 0.2732075\ttotal: 2m 2s\tremaining: 18.9s\n",
      "462:\tlearn: 0.2731152\ttotal: 2m 3s\tremaining: 18.6s\n",
      "463:\tlearn: 0.2730329\ttotal: 2m 3s\tremaining: 18.3s\n",
      "464:\tlearn: 0.2730149\ttotal: 2m 3s\tremaining: 18.1s\n",
      "465:\tlearn: 0.2729645\ttotal: 2m 3s\tremaining: 17.8s\n",
      "466:\tlearn: 0.2729218\ttotal: 2m 4s\tremaining: 17.5s\n",
      "467:\tlearn: 0.2728848\ttotal: 2m 4s\tremaining: 17.3s\n",
      "468:\tlearn: 0.2728558\ttotal: 2m 4s\tremaining: 17s\n",
      "469:\tlearn: 0.2728215\ttotal: 2m 4s\tremaining: 16.7s\n",
      "470:\tlearn: 0.2727397\ttotal: 2m 5s\tremaining: 16.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "471:\tlearn: 0.2727012\ttotal: 2m 5s\tremaining: 16.2s\n",
      "472:\tlearn: 0.2726438\ttotal: 2m 5s\tremaining: 15.9s\n",
      "473:\tlearn: 0.2725981\ttotal: 2m 5s\tremaining: 15.7s\n",
      "474:\tlearn: 0.2725434\ttotal: 2m 6s\tremaining: 15.4s\n",
      "475:\tlearn: 0.2725035\ttotal: 2m 6s\tremaining: 15.1s\n",
      "476:\tlearn: 0.2724273\ttotal: 2m 6s\tremaining: 14.9s\n",
      "477:\tlearn: 0.2723953\ttotal: 2m 7s\tremaining: 14.6s\n",
      "478:\tlearn: 0.2723467\ttotal: 2m 7s\tremaining: 14.4s\n",
      "479:\tlearn: 0.2723286\ttotal: 2m 7s\tremaining: 14.1s\n",
      "480:\tlearn: 0.2722736\ttotal: 2m 7s\tremaining: 13.8s\n",
      "481:\tlearn: 0.2722390\ttotal: 2m 8s\tremaining: 13.6s\n",
      "482:\tlearn: 0.2721467\ttotal: 2m 8s\tremaining: 13.3s\n",
      "483:\tlearn: 0.2720939\ttotal: 2m 8s\tremaining: 13s\n",
      "484:\tlearn: 0.2720831\ttotal: 2m 8s\tremaining: 12.8s\n",
      "485:\tlearn: 0.2720432\ttotal: 2m 9s\tremaining: 12.5s\n",
      "486:\tlearn: 0.2720041\ttotal: 2m 9s\tremaining: 12.2s\n",
      "487:\tlearn: 0.2719589\ttotal: 2m 9s\tremaining: 12s\n",
      "488:\tlearn: 0.2719442\ttotal: 2m 9s\tremaining: 11.7s\n",
      "489:\tlearn: 0.2719059\ttotal: 2m 10s\tremaining: 11.4s\n",
      "490:\tlearn: 0.2718552\ttotal: 2m 10s\tremaining: 11.2s\n",
      "491:\tlearn: 0.2717947\ttotal: 2m 10s\tremaining: 10.9s\n",
      "492:\tlearn: 0.2717652\ttotal: 2m 11s\tremaining: 10.6s\n",
      "493:\tlearn: 0.2717358\ttotal: 2m 11s\tremaining: 10.4s\n",
      "494:\tlearn: 0.2717046\ttotal: 2m 11s\tremaining: 10.1s\n",
      "495:\tlearn: 0.2716674\ttotal: 2m 11s\tremaining: 9.83s\n",
      "496:\tlearn: 0.2716195\ttotal: 2m 12s\tremaining: 9.56s\n",
      "497:\tlearn: 0.2715841\ttotal: 2m 12s\tremaining: 9.3s\n",
      "498:\tlearn: 0.2715199\ttotal: 2m 12s\tremaining: 9.03s\n",
      "499:\tlearn: 0.2714723\ttotal: 2m 12s\tremaining: 8.77s\n",
      "500:\tlearn: 0.2714245\ttotal: 2m 13s\tremaining: 8.5s\n",
      "501:\tlearn: 0.2713604\ttotal: 2m 13s\tremaining: 8.23s\n",
      "502:\tlearn: 0.2713474\ttotal: 2m 13s\tremaining: 7.97s\n",
      "503:\tlearn: 0.2713117\ttotal: 2m 13s\tremaining: 7.7s\n",
      "504:\tlearn: 0.2712504\ttotal: 2m 14s\tremaining: 7.44s\n",
      "505:\tlearn: 0.2712294\ttotal: 2m 14s\tremaining: 7.18s\n",
      "506:\tlearn: 0.2711951\ttotal: 2m 14s\tremaining: 6.91s\n",
      "507:\tlearn: 0.2711359\ttotal: 2m 15s\tremaining: 6.65s\n",
      "508:\tlearn: 0.2710758\ttotal: 2m 15s\tremaining: 6.38s\n",
      "509:\tlearn: 0.2710222\ttotal: 2m 15s\tremaining: 6.12s\n",
      "510:\tlearn: 0.2709782\ttotal: 2m 15s\tremaining: 5.85s\n",
      "511:\tlearn: 0.2709273\ttotal: 2m 16s\tremaining: 5.59s\n",
      "512:\tlearn: 0.2708487\ttotal: 2m 16s\tremaining: 5.32s\n",
      "513:\tlearn: 0.2708103\ttotal: 2m 16s\tremaining: 5.06s\n",
      "514:\tlearn: 0.2707380\ttotal: 2m 17s\tremaining: 4.79s\n",
      "515:\tlearn: 0.2706997\ttotal: 2m 17s\tremaining: 4.53s\n",
      "516:\tlearn: 0.2706707\ttotal: 2m 17s\tremaining: 4.26s\n",
      "517:\tlearn: 0.2706060\ttotal: 2m 18s\tremaining: 4s\n",
      "518:\tlearn: 0.2705783\ttotal: 2m 18s\tremaining: 3.73s\n",
      "519:\tlearn: 0.2705522\ttotal: 2m 18s\tremaining: 3.46s\n",
      "520:\tlearn: 0.2704957\ttotal: 2m 18s\tremaining: 3.2s\n",
      "521:\tlearn: 0.2704513\ttotal: 2m 19s\tremaining: 2.93s\n",
      "522:\tlearn: 0.2703748\ttotal: 2m 19s\tremaining: 2.67s\n",
      "523:\tlearn: 0.2703500\ttotal: 2m 19s\tremaining: 2.4s\n",
      "524:\tlearn: 0.2703050\ttotal: 2m 19s\tremaining: 2.13s\n",
      "525:\tlearn: 0.2702433\ttotal: 2m 20s\tremaining: 1.87s\n",
      "526:\tlearn: 0.2701931\ttotal: 2m 20s\tremaining: 1.6s\n",
      "527:\tlearn: 0.2701356\ttotal: 2m 20s\tremaining: 1.33s\n",
      "528:\tlearn: 0.2700861\ttotal: 2m 21s\tremaining: 1.07s\n",
      "529:\tlearn: 0.2700408\ttotal: 2m 21s\tremaining: 800ms\n",
      "530:\tlearn: 0.2700057\ttotal: 2m 21s\tremaining: 533ms\n",
      "531:\tlearn: 0.2699866\ttotal: 2m 21s\tremaining: 267ms\n",
      "532:\tlearn: 0.2699393\ttotal: 2m 22s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core._CatBoostBase at 0xfdc7240>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat = CatBoostClassifier(**params_cat)\n",
    "\n",
    "cat.fit(x_train__, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_test__ = x_test_.fillna(-999)\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission['ID_CORRELATIVO'] = x_test__.index\n",
    "submission['ATTRITION'] = cat.predict_proba(x_test__)[:,1]\n",
    "submission.to_csv('./data/submission15_CAT.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelado requerimientos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_req = df_train_req.join(y_train).drop('ATTRITION', axis=1)\n",
    "y_req = df_train_req.join(y_train)['ATTRITION']\n",
    "x_req.fillna(-999, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_req, x_test_req, y_train_req, y_test_req = train_test_split(x_req, y_req, test_size=.10, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6044600\ttest: 0.6031466\tbest: 0.6031466 (0)\ttotal: 113ms\tremaining: 11.2s\n",
      "1:\tlearn: 0.5360142\ttest: 0.5335257\tbest: 0.5335257 (1)\ttotal: 154ms\tremaining: 7.53s\n",
      "2:\tlearn: 0.4833144\ttest: 0.4797636\tbest: 0.4797636 (2)\ttotal: 211ms\tremaining: 6.82s\n",
      "3:\tlearn: 0.4425946\ttest: 0.4381694\tbest: 0.4381694 (3)\ttotal: 287ms\tremaining: 6.89s\n",
      "4:\tlearn: 0.4113655\ttest: 0.4060800\tbest: 0.4060800 (4)\ttotal: 328ms\tremaining: 6.22s\n",
      "5:\tlearn: 0.3870200\ttest: 0.3810510\tbest: 0.3810510 (5)\ttotal: 455ms\tremaining: 7.12s\n",
      "6:\tlearn: 0.3683349\ttest: 0.3616980\tbest: 0.3616980 (6)\ttotal: 545ms\tremaining: 7.24s\n",
      "7:\tlearn: 0.3538443\ttest: 0.3465183\tbest: 0.3465183 (7)\ttotal: 621ms\tremaining: 7.14s\n",
      "8:\tlearn: 0.3425319\ttest: 0.3346711\tbest: 0.3346711 (8)\ttotal: 746ms\tremaining: 7.54s\n",
      "9:\tlearn: 0.3337013\ttest: 0.3253680\tbest: 0.3253680 (9)\ttotal: 811ms\tremaining: 7.3s\n",
      "10:\tlearn: 0.3265408\ttest: 0.3178328\tbest: 0.3178328 (10)\ttotal: 898ms\tremaining: 7.26s\n",
      "11:\tlearn: 0.3211377\ttest: 0.3120240\tbest: 0.3120240 (11)\ttotal: 940ms\tremaining: 6.89s\n",
      "12:\tlearn: 0.3168793\ttest: 0.3074006\tbest: 0.3074006 (12)\ttotal: 1s\tremaining: 6.73s\n",
      "13:\tlearn: 0.3135073\ttest: 0.3036986\tbest: 0.3036986 (13)\ttotal: 1.06s\tremaining: 6.53s\n",
      "14:\tlearn: 0.3108425\ttest: 0.3007379\tbest: 0.3007379 (14)\ttotal: 1.1s\tremaining: 6.25s\n",
      "15:\tlearn: 0.3087274\ttest: 0.2983564\tbest: 0.2983564 (15)\ttotal: 1.15s\tremaining: 6.02s\n",
      "16:\tlearn: 0.3069971\ttest: 0.2964117\tbest: 0.2964117 (16)\ttotal: 1.23s\tremaining: 5.99s\n",
      "17:\tlearn: 0.3055970\ttest: 0.2948086\tbest: 0.2948086 (17)\ttotal: 1.36s\tremaining: 6.21s\n",
      "18:\tlearn: 0.3045175\ttest: 0.2935270\tbest: 0.2935270 (18)\ttotal: 1.45s\tremaining: 6.2s\n",
      "19:\tlearn: 0.3035652\ttest: 0.2924651\tbest: 0.2924651 (19)\ttotal: 1.54s\tremaining: 6.16s\n",
      "20:\tlearn: 0.3026691\ttest: 0.2915082\tbest: 0.2915082 (20)\ttotal: 1.63s\tremaining: 6.15s\n",
      "21:\tlearn: 0.3020542\ttest: 0.2907527\tbest: 0.2907527 (21)\ttotal: 1.68s\tremaining: 5.97s\n",
      "22:\tlearn: 0.3014800\ttest: 0.2901059\tbest: 0.2901059 (22)\ttotal: 1.75s\tremaining: 5.84s\n",
      "23:\tlearn: 0.3009963\ttest: 0.2895892\tbest: 0.2895892 (23)\ttotal: 1.86s\tremaining: 5.88s\n",
      "24:\tlearn: 0.3007164\ttest: 0.2891881\tbest: 0.2891881 (24)\ttotal: 1.94s\tremaining: 5.81s\n",
      "25:\tlearn: 0.3003997\ttest: 0.2888390\tbest: 0.2888390 (25)\ttotal: 2.04s\tremaining: 5.82s\n",
      "26:\tlearn: 0.3002221\ttest: 0.2885744\tbest: 0.2885744 (26)\ttotal: 2.13s\tremaining: 5.76s\n",
      "27:\tlearn: 0.2999729\ttest: 0.2883119\tbest: 0.2883119 (27)\ttotal: 2.24s\tremaining: 5.76s\n",
      "28:\tlearn: 0.2998590\ttest: 0.2881292\tbest: 0.2881292 (28)\ttotal: 2.28s\tremaining: 5.59s\n",
      "29:\tlearn: 0.2996869\ttest: 0.2879386\tbest: 0.2879386 (29)\ttotal: 2.37s\tremaining: 5.52s\n",
      "30:\tlearn: 0.2995809\ttest: 0.2877867\tbest: 0.2877867 (30)\ttotal: 2.44s\tremaining: 5.42s\n",
      "31:\tlearn: 0.2994967\ttest: 0.2876583\tbest: 0.2876583 (31)\ttotal: 2.49s\tremaining: 5.29s\n",
      "32:\tlearn: 0.2993930\ttest: 0.2875513\tbest: 0.2875513 (32)\ttotal: 2.61s\tremaining: 5.3s\n",
      "33:\tlearn: 0.2993402\ttest: 0.2874597\tbest: 0.2874597 (33)\ttotal: 2.67s\tremaining: 5.17s\n",
      "34:\tlearn: 0.2993095\ttest: 0.2873935\tbest: 0.2873935 (34)\ttotal: 2.71s\tremaining: 5.03s\n",
      "35:\tlearn: 0.2992847\ttest: 0.2873367\tbest: 0.2873367 (35)\ttotal: 2.75s\tremaining: 4.89s\n",
      "36:\tlearn: 0.2991330\ttest: 0.2872043\tbest: 0.2872043 (36)\ttotal: 2.87s\tremaining: 4.89s\n",
      "37:\tlearn: 0.2990649\ttest: 0.2871168\tbest: 0.2871168 (37)\ttotal: 2.94s\tremaining: 4.8s\n",
      "38:\tlearn: 0.2989767\ttest: 0.2870185\tbest: 0.2870185 (38)\ttotal: 3.02s\tremaining: 4.72s\n",
      "39:\tlearn: 0.2988655\ttest: 0.2869326\tbest: 0.2869326 (39)\ttotal: 3.14s\tremaining: 4.71s\n",
      "40:\tlearn: 0.2988280\ttest: 0.2869043\tbest: 0.2869043 (40)\ttotal: 3.21s\tremaining: 4.62s\n",
      "41:\tlearn: 0.2987653\ttest: 0.2868726\tbest: 0.2868726 (41)\ttotal: 3.3s\tremaining: 4.55s\n",
      "42:\tlearn: 0.2987342\ttest: 0.2868340\tbest: 0.2868340 (42)\ttotal: 3.36s\tremaining: 4.45s\n",
      "43:\tlearn: 0.2986831\ttest: 0.2867934\tbest: 0.2867934 (43)\ttotal: 3.48s\tremaining: 4.43s\n",
      "44:\tlearn: 0.2986401\ttest: 0.2867725\tbest: 0.2867725 (44)\ttotal: 3.58s\tremaining: 4.37s\n",
      "45:\tlearn: 0.2984960\ttest: 0.2866086\tbest: 0.2866086 (45)\ttotal: 3.73s\tremaining: 4.38s\n",
      "46:\tlearn: 0.2984678\ttest: 0.2865880\tbest: 0.2865880 (46)\ttotal: 3.82s\tremaining: 4.31s\n",
      "47:\tlearn: 0.2984174\ttest: 0.2865563\tbest: 0.2865563 (47)\ttotal: 3.91s\tremaining: 4.24s\n",
      "48:\tlearn: 0.2983790\ttest: 0.2865104\tbest: 0.2865104 (48)\ttotal: 4.03s\tremaining: 4.19s\n",
      "49:\tlearn: 0.2983004\ttest: 0.2864583\tbest: 0.2864583 (49)\ttotal: 4.14s\tremaining: 4.14s\n",
      "50:\tlearn: 0.2982773\ttest: 0.2864504\tbest: 0.2864504 (50)\ttotal: 4.24s\tremaining: 4.08s\n",
      "51:\tlearn: 0.2982282\ttest: 0.2863891\tbest: 0.2863891 (51)\ttotal: 4.33s\tremaining: 4s\n",
      "52:\tlearn: 0.2982175\ttest: 0.2863897\tbest: 0.2863891 (51)\ttotal: 4.38s\tremaining: 3.88s\n",
      "53:\tlearn: 0.2982154\ttest: 0.2863855\tbest: 0.2863855 (53)\ttotal: 4.43s\tremaining: 3.78s\n",
      "54:\tlearn: 0.2981542\ttest: 0.2863684\tbest: 0.2863684 (54)\ttotal: 4.56s\tremaining: 3.73s\n",
      "55:\tlearn: 0.2980913\ttest: 0.2863178\tbest: 0.2863178 (55)\ttotal: 4.68s\tremaining: 3.68s\n",
      "56:\tlearn: 0.2980344\ttest: 0.2863206\tbest: 0.2863178 (55)\ttotal: 4.79s\tremaining: 3.61s\n",
      "57:\tlearn: 0.2980343\ttest: 0.2863191\tbest: 0.2863178 (55)\ttotal: 4.85s\tremaining: 3.51s\n",
      "58:\tlearn: 0.2980342\ttest: 0.2863171\tbest: 0.2863171 (58)\ttotal: 4.9s\tremaining: 3.41s\n",
      "59:\tlearn: 0.2980296\ttest: 0.2863124\tbest: 0.2863124 (59)\ttotal: 4.99s\tremaining: 3.33s\n",
      "60:\tlearn: 0.2980295\ttest: 0.2863115\tbest: 0.2863115 (60)\ttotal: 5.05s\tremaining: 3.23s\n",
      "61:\tlearn: 0.2980215\ttest: 0.2862934\tbest: 0.2862934 (61)\ttotal: 5.13s\tremaining: 3.14s\n",
      "62:\tlearn: 0.2979809\ttest: 0.2862655\tbest: 0.2862655 (62)\ttotal: 5.24s\tremaining: 3.08s\n",
      "63:\tlearn: 0.2979110\ttest: 0.2862398\tbest: 0.2862398 (63)\ttotal: 5.36s\tremaining: 3.02s\n",
      "64:\tlearn: 0.2979023\ttest: 0.2862384\tbest: 0.2862384 (64)\ttotal: 5.49s\tremaining: 2.96s\n",
      "65:\tlearn: 0.2978627\ttest: 0.2862413\tbest: 0.2862384 (64)\ttotal: 5.6s\tremaining: 2.88s\n",
      "66:\tlearn: 0.2978601\ttest: 0.2862382\tbest: 0.2862382 (66)\ttotal: 5.65s\tremaining: 2.78s\n",
      "67:\tlearn: 0.2978600\ttest: 0.2862377\tbest: 0.2862377 (67)\ttotal: 5.7s\tremaining: 2.68s\n",
      "68:\tlearn: 0.2978522\ttest: 0.2862283\tbest: 0.2862283 (68)\ttotal: 5.8s\tremaining: 2.61s\n",
      "69:\tlearn: 0.2978323\ttest: 0.2862416\tbest: 0.2862283 (68)\ttotal: 5.97s\tremaining: 2.56s\n",
      "70:\tlearn: 0.2978314\ttest: 0.2862411\tbest: 0.2862283 (68)\ttotal: 6.05s\tremaining: 2.47s\n",
      "71:\tlearn: 0.2978036\ttest: 0.2862380\tbest: 0.2862283 (68)\ttotal: 6.18s\tremaining: 2.4s\n",
      "72:\tlearn: 0.2977970\ttest: 0.2862299\tbest: 0.2862283 (68)\ttotal: 6.25s\tremaining: 2.31s\n",
      "73:\tlearn: 0.2977605\ttest: 0.2862185\tbest: 0.2862185 (73)\ttotal: 6.37s\tremaining: 2.24s\n",
      "74:\tlearn: 0.2977605\ttest: 0.2862185\tbest: 0.2862185 (74)\ttotal: 6.42s\tremaining: 2.14s\n",
      "75:\tlearn: 0.2977145\ttest: 0.2861931\tbest: 0.2861931 (75)\ttotal: 6.55s\tremaining: 2.07s\n",
      "76:\tlearn: 0.2976996\ttest: 0.2861991\tbest: 0.2861931 (75)\ttotal: 6.67s\tremaining: 1.99s\n",
      "77:\tlearn: 0.2976633\ttest: 0.2861673\tbest: 0.2861673 (77)\ttotal: 6.78s\tremaining: 1.91s\n",
      "78:\tlearn: 0.2976272\ttest: 0.2861437\tbest: 0.2861437 (78)\ttotal: 6.89s\tremaining: 1.83s\n",
      "79:\tlearn: 0.2976235\ttest: 0.2861500\tbest: 0.2861437 (78)\ttotal: 6.95s\tremaining: 1.74s\n",
      "80:\tlearn: 0.2976203\ttest: 0.2861564\tbest: 0.2861437 (78)\ttotal: 7.03s\tremaining: 1.65s\n",
      "81:\tlearn: 0.2975897\ttest: 0.2861639\tbest: 0.2861437 (78)\ttotal: 7.14s\tremaining: 1.57s\n",
      "82:\tlearn: 0.2975544\ttest: 0.2861293\tbest: 0.2861293 (82)\ttotal: 7.26s\tremaining: 1.49s\n",
      "83:\tlearn: 0.2975207\ttest: 0.2861495\tbest: 0.2861293 (82)\ttotal: 7.35s\tremaining: 1.4s\n",
      "84:\tlearn: 0.2975207\ttest: 0.2861499\tbest: 0.2861293 (82)\ttotal: 7.4s\tremaining: 1.31s\n",
      "85:\tlearn: 0.2975151\ttest: 0.2861460\tbest: 0.2861293 (82)\ttotal: 7.49s\tremaining: 1.22s\n",
      "86:\tlearn: 0.2974535\ttest: 0.2861362\tbest: 0.2861293 (82)\ttotal: 7.64s\tremaining: 1.14s\n",
      "87:\tlearn: 0.2974535\ttest: 0.2861367\tbest: 0.2861293 (82)\ttotal: 7.68s\tremaining: 1.05s\n",
      "88:\tlearn: 0.2974535\ttest: 0.2861371\tbest: 0.2861293 (82)\ttotal: 7.72s\tremaining: 954ms\n",
      "89:\tlearn: 0.2974517\ttest: 0.2861427\tbest: 0.2861293 (82)\ttotal: 7.77s\tremaining: 864ms\n",
      "90:\tlearn: 0.2973502\ttest: 0.2860664\tbest: 0.2860664 (90)\ttotal: 7.9s\tremaining: 782ms\n",
      "91:\tlearn: 0.2973276\ttest: 0.2860237\tbest: 0.2860237 (91)\ttotal: 7.99s\tremaining: 694ms\n",
      "92:\tlearn: 0.2973236\ttest: 0.2860256\tbest: 0.2860237 (91)\ttotal: 8.05s\tremaining: 606ms\n",
      "93:\tlearn: 0.2972953\ttest: 0.2860152\tbest: 0.2860152 (93)\ttotal: 8.18s\tremaining: 522ms\n",
      "94:\tlearn: 0.2972537\ttest: 0.2859765\tbest: 0.2859765 (94)\ttotal: 8.33s\tremaining: 439ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95:\tlearn: 0.2972434\ttest: 0.2859774\tbest: 0.2859765 (94)\ttotal: 8.42s\tremaining: 351ms\n",
      "96:\tlearn: 0.2971881\ttest: 0.2859623\tbest: 0.2859623 (96)\ttotal: 8.54s\tremaining: 264ms\n",
      "97:\tlearn: 0.2971674\ttest: 0.2859644\tbest: 0.2859623 (96)\ttotal: 8.66s\tremaining: 177ms\n",
      "98:\tlearn: 0.2971250\ttest: 0.2859578\tbest: 0.2859578 (98)\ttotal: 8.8s\tremaining: 88.8ms\n",
      "99:\tlearn: 0.2970371\ttest: 0.2859411\tbest: 0.2859411 (99)\ttotal: 8.99s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.2859410566\n",
      "bestIteration = 99\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core._CatBoostBase at 0xb492b70>"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_feat_index = np.where(x_req.dtypes != np.int64)[0]\n",
    "\n",
    "cat = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=6)\n",
    "cat.fit(x_train_req, y_train_req, cat_features=cat_feat_index, eval_set=(x_test_req, y_test_req))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'Logloss_test_avg': [0.6044590655482499,\n",
       "              0.5358769341370746,\n",
       "              0.4831631361206874,\n",
       "              0.44266836665059667,\n",
       "              0.41130337126945904,\n",
       "              0.387199392487251,\n",
       "              0.36854173334588264,\n",
       "              0.35395704295215674,\n",
       "              0.34267024847766736,\n",
       "              0.33383455044077237,\n",
       "              0.3268189790347938,\n",
       "              0.32134512608824817,\n",
       "              0.3170148265141174,\n",
       "              0.31358654291561605,\n",
       "              0.3108858404954843,\n",
       "              0.3087493540895614,\n",
       "              0.30703190890868537,\n",
       "              0.3056071108638281,\n",
       "              0.3044784752974292,\n",
       "              0.30361880765614996,\n",
       "              0.30287648987621496,\n",
       "              0.30227719167405254,\n",
       "              0.30180587338820675,\n",
       "              0.30143048209180734,\n",
       "              0.3011080926921161,\n",
       "              0.30076702870558575,\n",
       "              0.30057768507970434,\n",
       "              0.3003539898528995,\n",
       "              0.30013740848490567,\n",
       "              0.29998441518110547,\n",
       "              0.2998357550579544,\n",
       "              0.29970791028018445,\n",
       "              0.299592455581155,\n",
       "              0.2994793051072507,\n",
       "              0.29943097687793213,\n",
       "              0.2993677936185415,\n",
       "              0.29928015734905683,\n",
       "              0.299222242561448,\n",
       "              0.2991752985581511,\n",
       "              0.29911290636458154,\n",
       "              0.2990719905096405,\n",
       "              0.2990395415988582,\n",
       "              0.29901092380712674,\n",
       "              0.2989728229349152,\n",
       "              0.2989349204481081,\n",
       "              0.2989098055941685,\n",
       "              0.29890893327692036,\n",
       "              0.2988984007468958,\n",
       "              0.2988744165186078,\n",
       "              0.2988390216339997,\n",
       "              0.2988169570520611,\n",
       "              0.29880086173182807,\n",
       "              0.2987872386782083,\n",
       "              0.29877375401527106,\n",
       "              0.2987654070090594,\n",
       "              0.29874081971405325,\n",
       "              0.29871530773785976,\n",
       "              0.2986833038924919,\n",
       "              0.298670772203811,\n",
       "              0.2986581003919335,\n",
       "              0.29864106987252914,\n",
       "              0.29862472525842704,\n",
       "              0.29861391421792927,\n",
       "              0.29859464883276593,\n",
       "              0.29858701169356383,\n",
       "              0.29856894153608016,\n",
       "              0.2985328715862442,\n",
       "              0.2985217985184493,\n",
       "              0.2985097585418705,\n",
       "              0.29851009167886383,\n",
       "              0.2985048794697234,\n",
       "              0.2984942992399261,\n",
       "              0.29849027983616944,\n",
       "              0.2984832916130662,\n",
       "              0.2984790267218383,\n",
       "              0.29847781047271227,\n",
       "              0.29847812593202044,\n",
       "              0.29846932244832625,\n",
       "              0.2984606805922132,\n",
       "              0.29845906394431315,\n",
       "              0.29845616719079077,\n",
       "              0.2984518377971164,\n",
       "              0.2984432088146489,\n",
       "              0.2984427970927467,\n",
       "              0.29843574453316546,\n",
       "              0.2984345194878634,\n",
       "              0.29842057932057375,\n",
       "              0.29841145820236437,\n",
       "              0.2984066556134876,\n",
       "              0.29840614559528306,\n",
       "              0.29839704857969346,\n",
       "              0.2983998655234941,\n",
       "              0.29837979591324004,\n",
       "              0.2983611838902634,\n",
       "              0.2983543006820771,\n",
       "              0.2983559682767874,\n",
       "              0.2983673464267911,\n",
       "              0.29836618662111847,\n",
       "              0.2983631710069754,\n",
       "              0.2983540282459697],\n",
       "             'Logloss_test_stddev': [0.0005302050655482113,\n",
       "              0.0008027996968810062,\n",
       "              0.0012501554322457066,\n",
       "              0.0017044927282799417,\n",
       "              0.002188541030553298,\n",
       "              0.002599548840434625,\n",
       "              0.002996971416666937,\n",
       "              0.003450168330827939,\n",
       "              0.0037769490260006955,\n",
       "              0.004077913727747972,\n",
       "              0.004455317491192571,\n",
       "              0.004662540774977728,\n",
       "              0.004903317151723187,\n",
       "              0.005160726269618181,\n",
       "              0.005367765111183356,\n",
       "              0.005542277959299176,\n",
       "              0.005646204949992743,\n",
       "              0.0057061550909081335,\n",
       "              0.005875578186342371,\n",
       "              0.005994428729222684,\n",
       "              0.006105136833629706,\n",
       "              0.0061573828365720585,\n",
       "              0.006229532655314151,\n",
       "              0.006320123620611304,\n",
       "              0.0063942935233799285,\n",
       "              0.0063963155760151675,\n",
       "              0.006448959221363585,\n",
       "              0.006471156497506218,\n",
       "              0.006534513126844361,\n",
       "              0.006576343917004817,\n",
       "              0.006663152594243766,\n",
       "              0.006709368772963875,\n",
       "              0.006751011208441008,\n",
       "              0.0067784201900404125,\n",
       "              0.006799666167304942,\n",
       "              0.006800166340075271,\n",
       "              0.006800697483757739,\n",
       "              0.006812882113251861,\n",
       "              0.006828734962474072,\n",
       "              0.006872514494898817,\n",
       "              0.006891391340597478,\n",
       "              0.00691316021415298,\n",
       "              0.00692535131358381,\n",
       "              0.006913193907657615,\n",
       "              0.006904946091558276,\n",
       "              0.00691017798098321,\n",
       "              0.0069100101210384885,\n",
       "              0.006925915178919674,\n",
       "              0.006910704898092721,\n",
       "              0.006943256513525515,\n",
       "              0.006961458576571121,\n",
       "              0.006971609634633492,\n",
       "              0.0069685642588975475,\n",
       "              0.006980903319880433,\n",
       "              0.0069823281692871116,\n",
       "              0.006975882316231234,\n",
       "              0.006961055816053447,\n",
       "              0.006978349951134445,\n",
       "              0.006967356886416826,\n",
       "              0.006971947309992971,\n",
       "              0.006996925897789951,\n",
       "              0.007014967460346978,\n",
       "              0.00701900066334403,\n",
       "              0.007014313934950003,\n",
       "              0.007023263847073047,\n",
       "              0.007013158153734274,\n",
       "              0.007014593149820373,\n",
       "              0.00703194629128407,\n",
       "              0.007025643716956628,\n",
       "              0.007027269085064396,\n",
       "              0.0070234634108756215,\n",
       "              0.0070324896662203595,\n",
       "              0.007028067829995632,\n",
       "              0.007037599431983604,\n",
       "              0.007030762906557176,\n",
       "              0.007028925151159392,\n",
       "              0.007028733034567221,\n",
       "              0.007023509645566279,\n",
       "              0.007022845493153102,\n",
       "              0.007027219922946196,\n",
       "              0.007030131137317569,\n",
       "              0.0070184799541852,\n",
       "              0.007020027810440625,\n",
       "              0.007018103663314115,\n",
       "              0.007011439353988954,\n",
       "              0.007012343930922773,\n",
       "              0.00700901381688833,\n",
       "              0.0070094065305061634,\n",
       "              0.007012071758683345,\n",
       "              0.007001812469527092,\n",
       "              0.006996817985712948,\n",
       "              0.006993653989749598,\n",
       "              0.006957780854313694,\n",
       "              0.006960594360837947,\n",
       "              0.006960775853615596,\n",
       "              0.006964897669496203,\n",
       "              0.006970205352013158,\n",
       "              0.006968844226525232,\n",
       "              0.006972525163113022,\n",
       "              0.00696780388967226],\n",
       "             'Logloss_train_avg': [0.604349241240369,\n",
       "              0.5358020680729753,\n",
       "              0.48308152364118895,\n",
       "              0.442505170697938,\n",
       "              0.41118078130039615,\n",
       "              0.38706234785023924,\n",
       "              0.3684075733331451,\n",
       "              0.3538389461994355,\n",
       "              0.3425480848450321,\n",
       "              0.3337253433122166,\n",
       "              0.3267062994382293,\n",
       "              0.32123297867833395,\n",
       "              0.3168497759150529,\n",
       "              0.3134374414191308,\n",
       "              0.31071284430257456,\n",
       "              0.3085836915088448,\n",
       "              0.3068802691347593,\n",
       "              0.3054687724346419,\n",
       "              0.3043236314358002,\n",
       "              0.30345949081643253,\n",
       "              0.3026856582606139,\n",
       "              0.30209735176719654,\n",
       "              0.3016176640844645,\n",
       "              0.3012288557197367,\n",
       "              0.30089851009448554,\n",
       "              0.3005606073386863,\n",
       "              0.30037558461362146,\n",
       "              0.30013161672204347,\n",
       "              0.2999152353139227,\n",
       "              0.299767407505126,\n",
       "              0.29960606015227553,\n",
       "              0.2994759691813611,\n",
       "              0.299360471515352,\n",
       "              0.29923537718865256,\n",
       "              0.29917809228779213,\n",
       "              0.2991115423921114,\n",
       "              0.2990184998926565,\n",
       "              0.2989514440273137,\n",
       "              0.2988971874122312,\n",
       "              0.29882894974751234,\n",
       "              0.29876022685209264,\n",
       "              0.2987174622178017,\n",
       "              0.2986430754730842,\n",
       "              0.2985839106363633,\n",
       "              0.2985418770217043,\n",
       "              0.29849572444632566,\n",
       "              0.2984644697004989,\n",
       "              0.29843465196324404,\n",
       "              0.2983912709862846,\n",
       "              0.2983403226007285,\n",
       "              0.2982975347419277,\n",
       "              0.2982590585513014,\n",
       "              0.29821645763103916,\n",
       "              0.29817197617816704,\n",
       "              0.2981661695034217,\n",
       "              0.2981294237591265,\n",
       "              0.2980921593130735,\n",
       "              0.29803033073816076,\n",
       "              0.297996321786519,\n",
       "              0.2979641733403778,\n",
       "              0.2979243046818036,\n",
       "              0.29790430489010467,\n",
       "              0.297884682004878,\n",
       "              0.2978466731203276,\n",
       "              0.2978171220967707,\n",
       "              0.2977806326404369,\n",
       "              0.29772757559456753,\n",
       "              0.29769696381128585,\n",
       "              0.2976634359207823,\n",
       "              0.29764186952156085,\n",
       "              0.29762776141031155,\n",
       "              0.2976162248108983,\n",
       "              0.2976038325977609,\n",
       "              0.29758762882284934,\n",
       "              0.2975657568347533,\n",
       "              0.29754613325884105,\n",
       "              0.29754520185728184,\n",
       "              0.2975235176310056,\n",
       "              0.29749196793330085,\n",
       "              0.297479992499622,\n",
       "              0.29746845905440955,\n",
       "              0.2974460272530872,\n",
       "              0.29740648594493413,\n",
       "              0.29738492897031255,\n",
       "              0.2973430975479801,\n",
       "              0.297333837208435,\n",
       "              0.2973013688598046,\n",
       "              0.29724211480450136,\n",
       "              0.2972149063609052,\n",
       "              0.29717731358303195,\n",
       "              0.29715235473378304,\n",
       "              0.29712897271253047,\n",
       "              0.2970511432437738,\n",
       "              0.2970278144896764,\n",
       "              0.2969969173264362,\n",
       "              0.29697453172191796,\n",
       "              0.2969475382394431,\n",
       "              0.29689732747420583,\n",
       "              0.29686837472135463,\n",
       "              0.29684146533857925],\n",
       "             'Logloss_train_stddev': [0.00047276868938167223,\n",
       "              0.0009225340458520675,\n",
       "              0.0011406337808597723,\n",
       "              0.001313434496156618,\n",
       "              0.001364360010532723,\n",
       "              0.0014503794403463752,\n",
       "              0.0015164442307847342,\n",
       "              0.0015125768088979068,\n",
       "              0.0015536110308879882,\n",
       "              0.0015856111663879882,\n",
       "              0.0015069361916836136,\n",
       "              0.0015514022610086796,\n",
       "              0.0015704435727601801,\n",
       "              0.001530913419012218,\n",
       "              0.0014964369254185307,\n",
       "              0.0015058687665562148,\n",
       "              0.0015572819528380307,\n",
       "              0.001648642749380095,\n",
       "              0.0016254077445704454,\n",
       "              0.0016179583531942478,\n",
       "              0.0016081231131138633,\n",
       "              0.001650887310460214,\n",
       "              0.0016646980572122647,\n",
       "              0.0016407922768598285,\n",
       "              0.001631845772384906,\n",
       "              0.0016957270844960037,\n",
       "              0.0017003203520567058,\n",
       "              0.0017077615869507173,\n",
       "              0.0016887168632956282,\n",
       "              0.0016790709948551889,\n",
       "              0.00162205429579181,\n",
       "              0.0016100114988281944,\n",
       "              0.0016075056318674357,\n",
       "              0.0016219735985163188,\n",
       "              0.0016295563650066225,\n",
       "              0.0016575076383165753,\n",
       "              0.001683405210225322,\n",
       "              0.0016755815234488353,\n",
       "              0.0016688862469853114,\n",
       "              0.0016535460591816694,\n",
       "              0.0016017766861104383,\n",
       "              0.0015962267889620821,\n",
       "              0.0016216522704537095,\n",
       "              0.001663393246122245,\n",
       "              0.0016853812139615008,\n",
       "              0.0016736603899131907,\n",
       "              0.0016584007049762171,\n",
       "              0.001644956939285656,\n",
       "              0.00167876837960247,\n",
       "              0.0016435748154535704,\n",
       "              0.0016096277131620947,\n",
       "              0.0016158663974876087,\n",
       "              0.0016250696321150977,\n",
       "              0.0016319677509172487,\n",
       "              0.0016345203392000242,\n",
       "              0.0016437328500113516,\n",
       "              0.0016565267818363808,\n",
       "              0.0016632103295203277,\n",
       "              0.0016737608940007559,\n",
       "              0.00166017027542984,\n",
       "              0.0016372936432030145,\n",
       "              0.001622736688144356,\n",
       "              0.0016174803285589196,\n",
       "              0.0016329836740067172,\n",
       "              0.0016196583042911482,\n",
       "              0.0016271696366330132,\n",
       "              0.0016245381252933922,\n",
       "              0.0016065636836740938,\n",
       "              0.001623325144019267,\n",
       "              0.0016120264054728874,\n",
       "              0.001618180899642977,\n",
       "              0.0016105802564980237,\n",
       "              0.0016218540856086556,\n",
       "              0.0015980740834773883,\n",
       "              0.0016055140905891158,\n",
       "              0.0016016556643391436,\n",
       "              0.0016011503513134158,\n",
       "              0.0015869636888822273,\n",
       "              0.001589734839845426,\n",
       "              0.0015770962884687726,\n",
       "              0.0015718892798656805,\n",
       "              0.0015784658733142398,\n",
       "              0.0015943917447062608,\n",
       "              0.0016036650026530089,\n",
       "              0.001605389376816444,\n",
       "              0.001602139208433136,\n",
       "              0.0016057456557305614,\n",
       "              0.001564697824581389,\n",
       "              0.0015632914294796427,\n",
       "              0.001585022709300779,\n",
       "              0.0015934704532821057,\n",
       "              0.0015817111626003012,\n",
       "              0.0016187153035830666,\n",
       "              0.0016116141686608545,\n",
       "              0.0016223876691456346,\n",
       "              0.0016046580791354676,\n",
       "              0.0016106491326443682,\n",
       "              0.0016497659486925014,\n",
       "              0.0016501846866844876,\n",
       "              0.0016530147144865177]})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from catboost import cv, Pool\n",
    "\n",
    "params ={\n",
    "    'depth': 6,\n",
    "    'learning_rate': .1,\n",
    "    'iterations': 100,\n",
    "    'loss_function': 'Logloss'\n",
    "}\n",
    "\n",
    "cv(params, Pool(x_train_req, y_train_req, cat_features=cat_feat_index),  partition_random_seed=1234, fold_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ATT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_CORRELATIVO</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47411</th>\n",
       "      <td>0.291465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39861</th>\n",
       "      <td>0.269891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38898</th>\n",
       "      <td>0.028191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50927</th>\n",
       "      <td>0.019253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32969</th>\n",
       "      <td>0.470753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     ATT\n",
       "ID_CORRELATIVO          \n",
       "47411           0.291465\n",
       "39861           0.269891\n",
       "38898           0.028191\n",
       "50927           0.019253\n",
       "32969           0.470753"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_req['ATTRITION_REQ'] = cat.predict_proba(df_test_req)[:, 1]\n",
    "\n",
    "df_test_req_ = df_test_req.groupby(by=df_test_req.index)['ATTRITION_REQ'].mean()\n",
    "\n",
    "new_submission = pd.DataFrame(index=x_test_.index)\n",
    "new_submission['ATT'] = y_pred\n",
    "new_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test_req_ = new_submission.join(df_test_req_)\n",
    "df_test_req_['ATTRITION'] = df_test_req_.apply(lambda x: np.mean(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test_req_.reset_index(inplace=True)\n",
    "df_test_req_[['ID_CORRELATIVO', 'ATTRITION']].to_csv('./data/submission11.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
